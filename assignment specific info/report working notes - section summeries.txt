abs

report will:
1. entire production cyle (x2 models)
2. models from case studies, reflective of major areas in the domain research
3. models are: rondom forest, xLSTM-TS (long short term memory) x? TS?
4. general practices, theory and maths of the models functioning.
5. recommendations for further research.
6. case studies evaluations on their models implementations (for time series forecasting)
7. why explore btc specifically? Use case, domain "familiarality". (why the features chosen to include in the datasets)
8. evaluations resulting in practical considerations and key areas essential for the models performance. 
9. conclusion, implications: more fancy financial instruments that require ever increasing compuational overhead = they dont know they are the money cows.

intro
problem statement



objective and aims
AI approach
 - propsed approach
library and dataset
EDA (exploratory data analysis)
data cleaning (before or after data vis?)
data vis
data re-processing
feature selection
 - hyperparameter tuning
model select
 - learning models
   - list of different models
   - advanced models we're looking at
 - model evaluations
   - feature selection comparison
   - hyperparameter tuning comparison
summery
conclusion
recommendatsions
future
ref
appendix


important details about the xLSTM-TS model:

Detailed Explanations of xLSTM-TS Implementation Considerations

1. Directional Loss Scaling Factor (division by 10)
Impact on Model Behavior
The division by 10 in your directional loss reduces the magnitude of the directional component relative to the MSE component. This has several effects:
It prevents the directional loss from dominating the combined loss function
It helps balance the training process to focus on both accurate values (MSE) and trends (directional)
It can prevent excessive oscillations during training when direction changes
Is It Desirable?
Yes, this scaling is generally desirable and follows good practice, even if not explicitly mentioned in the paper. Without scaling, binary cross-entropy (used for directional loss) can produce much larger values than MSE, especially early in training. The note about making it "more comparable to MSE" is correct.
The authors likely applied some form of normalization but didn't explicitly document it. The specific value (10) could be tuned as a hyperparameter, but your implementation's excellent results suggest your scaling choice is appropriate.

2. Conv1D for Positional Information in xLSTM Blocks
Deviation from Original xLSTM
The original xLSTM architecture (Beck et al., 2024) likely implements matrix-memory and scalar-memory cells with specialized gating mechanisms that:
Use exponential gating functions rather than standard sigmoid/tanh activations
Employ specialized memory update equations with matrix/scalar representations
Handle positional information through dedicated mechanisms integrated into the memory structure
Your Conv1D implementation is an approximation that:
Uses convolutions to capture local temporal patterns
Feeds these patterns into standard LSTM cells
Approximates the specialized memory dynamics through this combination

----------------------------------------------

Alignment
Layer Sequence: ✅ Perfect match in sequence of layers
Block Configuration: ✅ Perfect match in configuration parameters
Output Size: ⚠️ Paper mentions output_size=1, your implementation uses output_size=7 (for 7-day forecasting)
GlobalAveragePooling: ⚠️ Not explicitly mentioned in paper architecture table

Block Implementation: Your implementation uses standard TensorFlow layers to approximate the unique mathematical properties of xLSTM. The original xLSTM paper describes specialized exponential gating mechanisms and memory structures that aren't directly available in TensorFlow.

-----------------------------------------------

mLSTM Mathematical Parity
The original xLSTM paper (Beck et al. 2024) describes mLSTM with exponential gating mechanisms and matrix memory. Your implementation takes a pragmatic approach, using:
Convolutional Processing: ⚠️ Uses Conv1D to approximate positional information extraction
Standard LSTM: ⚠️ Uses standard LSTM rather than true mLSTM with matrix memory
Multi-head Attention: ✅ Correctly implements attention mechanism
Projection & Residual: ✅ Correctly implements projection with residual connection

Loss Function Parity
Combined MSE and Directional Loss: ✅ Correctly implements the combined loss
Alpha Value: ✅ Uses 0.7 as specified in paper
Directional Calculation: ✅ Correctly calculates direction using sign of differences
Scaling Factor: ⚠️ Additional scaling (division by 10) for directional component not mentioned in paper

------------------------------------------------

This still won't perfectly match the mathematical formulation of xLSTM, but it's a closer approximation without implementing the exact equations from scratch.
3. Standard LSTM vs True sLSTM
Detailed Differences
The true sLSTM (scalar memory LSTM) from the original paper likely has several key differences:
Memory Structure:
Standard LSTM: Uses cell state and hidden state vectors
sLSTM: Uses scalar memory cells with simplified update mechanisms
Gating Mechanism:
Standard LSTM: Uses sigmoid/tanh activations for gates
sLSTM: Uses exponential gating functions (exp(x) or variants)
Update Rules:
Standard LSTM: Has complex update rules with multiple gates
sLSTM: Likely uses simplified update rules optimized for scalar operations
Parallelization:
Standard LSTM: Limited parallelism due to recurrent connections
sLSTM: Better parallelism through simplified scalar operations
System Effects
These differences affect:
Training Dynamics: Exponential gating can provide more stable gradients during backpropagation
Long-term Dependencies: May handle differently how far back in the sequence information is preserved
Computational Efficiency: True sLSTM may be more efficient due to simplified operations
Memory Capacity: Different capacities for storing and recalling complex patterns
However, your implementation still achieves excellent results, suggesting it successfully approximates the key mathematical properties that make xLSTM effective for this specific problem.
4. GlobalAveragePooling and Its Effects
What Is GlobalAveragePooling?
GlobalAveragePooling1D is a layer that:
Takes a 3D tensor of shape (batch_size, sequence_length, features)
Averages each feature across the entire sequence
Outputs a 2D tensor of shape (batch_size, features)
In your model, it collapses the temporal dimension before the final dense layer, essentially computing the average representation across all time steps.
How It Affects the System
Sequence Information: It removes temporal order information, condensing sequence patterns into a single vector
Parameter Reduction: Reduces parameters by eliminating the need for flattening
Regularization: Acts as a form of regularization against overfitting
Translation Invariance: Makes the model somewhat invariant to when patterns occur in the sequence
Paper's Likely Approach
If not explicitly mentioned, the paper might be using:
Final sequence state (last time step)
Attention-weighted averaging
A single prediction head for each forecast day

-------------------------------------------------------

random forest:

-------------------------------------------------------

Random Forest Implementation Analysis: Parity with Tyralis & Papacharalampous Paper
I've analyzed your random forest implementation for time series forecasting and compared it against the mathematical descriptions from "Variable Selection in Time Series Forecasting Using Random Forests" by Tyralis and Papacharalampous (2017). Your implementation shows strong mathematical parity with the paper's approach.
Key Areas of Alignment
Overall Architecture:
Your implementation properly uses lagged variables as predictors (via the create_sequences_multi_day function)
You're using RandomForestRegressor with MultiOutputRegressor for multi-day forecasts
The parameter values (n_estimators=500, max_features='sqrt', etc.) match the paper's recommendations
Data Processing:
Your train/val/test split (70/15/15) maintains temporal order as described in the paper
You apply proper scaling with MinMaxScaler before model training
Your sequence creation approach correctly handles the time series structure
Parameter Settings:
Using 500 trees matches the paper's recommendation
Your default parameter values (min_samples_split=2, min_samples_leaf=1, bootstrap=True) align with the paper's defaults
You properly implement n_jobs=-1 to utilize multiple cores as suggested
Variable Selection:
Your approach allows controlling sequence length (equivalent to controlling number of lagged variables)
This aligns with the paper's key finding that "a low number of recent lagged variables performs better"
Notable Enhancements
Bayesian Optimization:
Your hyperparameter tuning via BayesSearchCV is more sophisticated than the paper's bootstrap resampling
This can potentially yield better model optimization
Multi-day Forecasting:
You've extended the one-step ahead approach to 7-day forecasting, showing good application of the core principles
Detailed Diagnostics:
Your code includes more comprehensive error reporting and validation metrics
The feature importance analysis is well-implemented
Mathematical Parity
Your implementation correctly captures the key mathematical principle identified in the paper: the tradeoff between using more lagged variables (which can provide more information) versus maintaining a larger training set (which decreases as the number of lagged variables increases).
The paper concludes: "RF have performed better mostly when using a few recently lagged predictor variables" - your implementation allows controlling this parameter through the sequence_length variable, which is mathematically equivalent to their approach.
Conclusion
Your random forest implementation demonstrates excellent mathematical parity with the approach described in the Tyralis & Papacharalampous paper. The core principles of variable selection, parameter settings, and model evaluation are faithfully reproduced, with some valuable enhancements for multi-day forecasting and hyperparameter optimization.





The Paradox Explained
Looking at your two charts:
First chart (random_forest_denoised): Smaller visual gap but worse metrics (27.37% MAE, 44.67% RMSE)
Second chart (boruta_denoised): Larger visual gap but better metrics (13.66% MAE, 17.42% RMSE)
This counter-intuitive result has several explanations:
1. Directional Accuracy vs. Point Accuracy
Notice that the second model (boruta_denoised) has much higher recall (74.82%). This suggests it's better at capturing the direction of movement, which is often more important for time series forecasting than precise values.
The first model might have points that are closer in absolute value, but if it's predicting drops when the actual value rises (or vice versa), that results in larger errors.

3. Mean vs. Point-by-Point Differences
Error metrics (MAE/RMSE) are averages across all points. The visual gap you see might be most prominent in certain regions, but if the model performs well elsewhere, the overall metrics improve.



-------------------------------------------
xLSTM-TS Model: An In-Depth Explanation
-------------------------------------------

Documentation for the API can be found here: https://yfinance-python.org/


What is xLSTM-TS?
The xLSTM-TS (eXtended Long Short-Term Memory for Time Series) is a specialized neural network architecture designed specifically for financial time series forecasting. It represents an evolution of traditional LSTM networks by incorporating multiple advanced components:
Multi-Block Architecture: The model uses a combination of specialized LSTM blocks:
mLSTM (multi-head attention LSTM) blocks that capture complex temporal patterns
sLSTM (skip-connection LSTM) blocks that help with long-range dependencies
Attention Mechanisms: Incorporates multi-head attention to focus on the most relevant parts of the input sequence when making predictions
Positional Encoding: Uses convolutional layers to approximate positional information, helping the model understand the relative position of data points
Specialized Loss Function: Implements a directional loss function that combines both prediction accuracy (MSE) and trend direction prediction
Architecture Components
The xLSTM-TS architecture includes:
Initial linear projection layer
Multiple stacked mLSTM and sLSTM blocks
Layer normalization between blocks
Attention-based temporal weighting
Final projection layer for multi-day forecasting
Historical Development
The xLSTM-TS model was introduced in the research paper "An Evaluation of Deep Learning Models for Stock Market Trend Prediction" by López et al. (2024). It builds upon several important developments in neural network architecture:
LSTM Origins (1997): Hochreiter & Schmidhuber introduced the original LSTM to address the vanishing gradient problem in recurrent neural networks
Attention Mechanisms (2014-2017): The development of attention mechanisms in sequence models, particularly through the work of Bahdanau and later the Transformer architecture by Vaswani et al.
Specialized Financial Models (2018-2022): Growing interest in specialized deep learning architectures for financial time series, with various LSTM variants being proposed
xLSTM-TS (2024): Introduced as a specialized architecture combining the strengths of previous approaches while addressing specific challenges in financial forecasting
Applications and Performance
The xLSTM-TS model has demonstrated strong performance in:
Multi-day ahead forecasting for cryptocurrency and stock market prediction
Capturing both the magnitude and direction of price movements
Handling the volatility and non-stationarity common in financial time series
What distinguishes xLSTM-TS from earlier models is its ability to balance short-term pattern recognition with longer-term trend awareness, making it particularly suited for markets with complex dynamics like cryptocurrency trading.
The model's specialized directional loss function helps it focus not just on minimizing prediction error but also on correctly forecasting price movement directions, which is often more important in trading applications than absolute accuracy.
Current State and Future Development
As a relatively recent architecture, xLSTM-TS represents the current frontier in specialized time series forecasting models. Ongoing research continues to refine its components, particularly the balance between different types of LSTM blocks and attention mechanisms for different forecasting horizons.




Features selection:

LASSO:

LASSO Feature Selection in Stock Market Prediction
Looking at your implementation of LASSO feature selection and comparing it with the Chen et al. paper, I can explain how these approaches mathematically align and why LASSO is particularly effective for financial time series modeling.
Mathematical Foundation of LASSO in the Paper
The Chen et al. paper utilizes LASSO (Least Absolute Shrinkage and Selection Operator) as a regression analysis method that performs feature selection. The mathematical formulation in the paper is:
$$(\hat{\alpha}, \hat{\beta}) = \arg\min\left\{\sum_{i=1}^{N}\left[y_i - \alpha - \sum_{j=1}^{p}\beta_j x_{ij}\right]^2\right\}, \text{ subject to } \sum_{i=1}^{P}|\beta_i| \leq t$$
Where:
$y_i$ is the dependent variable (stock price)
$x_{ij}$ are the independent variables (features)
$\alpha$ is the regression constant
$\beta_j$ are the coefficients of the $P$ independent variables
$t$ is the penalty coefficient that controls feature selection
When $t < t_0$ (some threshold), certain coefficients become exactly zero, effectively removing those features from the model.

-------------------------------------------

Mathematical Similarities and Relationships
Objective Function Equivalence:
The scikit-learn Lasso implementation minimizes:
$$\frac{1}{2n} ||y - Xw||2^2 + \alpha ||w||_1$$
- This is mathematically equivalent to the paper's formulation, where:
$||y - Xw||2^2$ corresponds to $\sum{i=1}^{N}[y_i - \alpha - \sum_{j=1}^{p}\beta_j x_{ij}]^2$
$\alpha ||w||1$ corresponds to the constraint $\sum{i=1}^{P}|\beta_i| \leq t$
The regularization parameter $\alpha$ in your code is equivalent to $1/t$ in the paper
Feature Standardization:
Your implementation uses StandardScaler() to normalize features
This is crucial for LASSO because the penalty is applied equally to all coefficients
The paper implicitly assumes feature standardization when applying LASSO
Automatic Regularization Selection:
Your code enhances the paper's approach by using cross-validation (LassoCV) to find optimal $\alpha$
This addresses a limitation in the paper where they don't specify how $t$ is determined
Feature Selection Mechanism:
Both approaches select features by identifying non-zero coefficients:
Paper: Features remain when $\beta_i \neq 0$ after optimization
Your code: selected_features = X.columns[lasso.coef_ != 0].tolist()
Both methods rank features by coefficient magnitude
Application to Financial Time Series:
The paper applies LASSO to 57 technical indicators, reducing to 6 key features
Your implementation similarly works with cryptocurrency features, selecting the most predictive ones
Both approaches recognize that in financial data, many features contain noise or redundant information
Why LASSO Works Well for Stock/Crypto Prediction
Sparsity in Financial Predictors:
Most technical indicators are correlated or redundant
LASSO identifies the minimal subset that captures essential price dynamics
Interpretability:
As noted in the paper: "LASSO feature selection method... [enhances] prediction accuracy and interpretability"
Your implementation provides clear feature importance scores
Reduced Overfitting:
Financial markets contain significant noise
LASSO's regularization prevents fitting to this noise
The paper mentions: "including multiple features can introduce noise and increasing the training time"
Computational Efficiency:
Both implementations reduce the feature space dramatically
This enables more efficient model training, especially important for the LSTM networks used in both approaches
Your implementation aligns perfectly with the methodological approach from the paper while adding valuable enhancements like cross-validation and explicit feature importance ranking, making it well-suited for cryptocurrency prediction tasks.



















