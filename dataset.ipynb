{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if pip install isnt working:\n",
    "# !python --version\n",
    "\n",
    "# !curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
    "# !python get-pip.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements:\n",
    "#!python -m \n",
    "# pip install pandas yfinance requests numpy matplotlib PyWavelets seaborn scikit-learn scipy\n",
    "# import the necessary libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pywt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need a dataset for this project, I was unable to find a complete dataset from a reputable source that suited my specific use case. So decided to look into collecting my own data and compiling one myself. \n",
    "# I'll be using the yfinance library to get the stock market data (in future I would like to include more data sources such as those from Tiingo).\n",
    "# I'll start by making a directory for the datasets.csv that we will need to generate.\n",
    "\n",
    "project_root = Path(os.getcwd()) #find file path of the working directory for notebook scripts.\n",
    "output_dir = project_root / \"dataset\"\n",
    "#check to see if the directory exists, make it if it doesn't\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_path = output_dir / \"2015-2025_dataset.csv\"\n",
    "\n",
    "# defining the date range for the dataset. \n",
    "# we'll be using start and end dates multiples times so its best to define them here only once, its likely we'll need to generate seperate time frames for the test/training split.\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2025-02-01\"\n",
    "\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D') # Format: YYYY-MM-DD, \"freq\" is the frequency of dates in this case: ='D' means daily.\n",
    "\n",
    "df = pd.DataFrame(index=date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block is Largely AI assisted with Claude sonnet 3.5: Finding the right syntax and logic for the yahoo finance API, to pull the historical trading data needed. Converted the currencies to USD and average them together for the first two columns of this dataset, was achieved wtih relative ease thanks to AI assistance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stock_data(symbol: str, currency: str, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    df = yf.download(symbol, start=start_date, end=end_date)\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(index=date_range)\n",
    "        \n",
    "    # Create a new DataFrame with just Close and Volume\n",
    "    result = pd.DataFrame(index=df.index)\n",
    "    result['Close'] = df['Close']\n",
    "    result['Volume'] = df['Volume']\n",
    "    \n",
    "    # Get currency conversion rate if needed\n",
    "    if currency:\n",
    "        fx_data = yf.download(currency, start=start_date, end=end_date)\n",
    "        if not fx_data.empty:\n",
    "            fx_rate = fx_data['Close']\n",
    "            \n",
    "            # Ensure both dataframes have datetime index\n",
    "            result.index = pd.to_datetime(result.index)\n",
    "            fx_rate.index = pd.to_datetime(fx_rate.index)\n",
    "            \n",
    "            # Find common dates between stock and forex data\n",
    "            common_dates = result.index.intersection(fx_rate.index)            \n",
    "            # Keep only dates where we have both stock and forex data\n",
    "            result = result.loc[common_dates]\n",
    "            fx_rate = fx_rate.loc[common_dates]\n",
    "            \n",
    "            # Convert only Close prices to USD using element-wise multiplication\n",
    "            result['Close'] = result['Close'].values * fx_rate.values\n",
    "        else:\n",
    "            return pd.DataFrame(index=date_range)\n",
    "    \n",
    "    # Handle volume based on the index, since asian markets often have lower nominal volumes\n",
    "    if symbol in ['^N225', '^HSI']:  \n",
    "        result['Volume'] = result['Volume'] / 1_000  # Convert to thousands\n",
    "    else:\n",
    "        result['Volume'] = result['Volume'] / 1_000_000  # Convert to millions\n",
    "                \n",
    "    # Add sanity checks for extreme values\n",
    "    if result['Close'].max() > 50000 or result['Close'].min() < 1:\n",
    "        return pd.DataFrame(index=date_range)\n",
    "        \n",
    "    if result['Volume'].min() > 0 and result['Volume'].max() / result['Volume'].min() > 1000:\n",
    "        return pd.DataFrame(index=date_range)\n",
    "        \n",
    "    # Rename columns with symbol prefix\n",
    "    result = result.rename(columns={\n",
    "        'Close': f'{symbol}_Close_USD',\n",
    "        'Volume': f'{symbol}_Volume'\n",
    "    })\n",
    "    \n",
    "    # Reindex to full date range without filling\n",
    "    result = result.reindex(date_range)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need to define a few functions that will need to be run and combined together before we can generate the dataset.\n",
    "# Lets start with the global stock market data.\n",
    "# Cluade AI did the currency conversions\n",
    "\n",
    "def get_market_stock_data(start_date, end_date):\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    result_df = pd.DataFrame(index=date_range)\n",
    "    \n",
    "    # Define indices with their currencies\n",
    "    indices = {\n",
    "        'GDAXI': {'symbol': '^GDAXI', 'currency': 'EURUSD=X'},    # Germany DAX\n",
    "        'IXIC': {'symbol': '^IXIC', 'currency': None},            # NASDAQ (already in USD)\n",
    "        'DJI': {'symbol': '^DJI', 'currency': None},              # Dow Jones (already in USD)\n",
    "        'N225': {'symbol': '^N225', 'currency': 'JPYUSD=X'},      # Nikkei\n",
    "        'STOXX50E': {'symbol': '^STOXX', 'currency': 'EURUSD=X'}, # Euro STOXX 50\n",
    "        'HSI': {'symbol': '^HSI', 'currency': 'HKDUSD=X'},        # Hang Seng\n",
    "        'FTSE': {'symbol': '^FTSE', 'currency': 'GBPUSD=X'}       # FTSE 100\n",
    "    }\n",
    "    \n",
    "    # Fetch data for all indices\n",
    "    combined_df = pd.DataFrame(index=date_range)\n",
    "    \n",
    "    for name, info in indices.items():\n",
    "        index_data = fetch_stock_data(info['symbol'], info['currency'], start_date, end_date)\n",
    "        if not index_data.empty and len(index_data.columns) > 0:\n",
    "            combined_df = pd.concat([combined_df, index_data], axis=1)\n",
    "    \n",
    "    # Calculate global averages\n",
    "    close_cols = [col for col in combined_df.columns if str(col).endswith('_Close_USD')]\n",
    "    volume_cols = [col for col in combined_df.columns if str(col).endswith('_Volume_M')]\n",
    "    \n",
    "    if close_cols and volume_cols:\n",
    "        result_df = pd.DataFrame(index=date_range)\n",
    "        result_df['Global averaged stocks(USD)'] = combined_df[close_cols].mean(axis=1, skipna=True)\n",
    "        result_df['Global averaged stocks (volume)'] = combined_df[volume_cols].mean(axis=1, skipna=True)\n",
    "        \n",
    "        return result_df\n",
    "    return pd.DataFrame(index=date_range, columns=['Global averaged stocks(USD)', 'Global averaged stocks (volume)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_currency_metrics(start_date, end_date):\n",
    "    \n",
    "    # Fetch the US Dollar index (DXY) and the gold futures data from Yahoo Finance.\n",
    "    \n",
    "    # Get DXY (US Dollar Index)\n",
    "    dxy = yf.download(\"DX-Y.NYB\", start=start_date, end=end_date)\n",
    "    df['Currency US Dollar Index'] = dxy['Close']\n",
    "    \n",
    "    # Get Gold Futures\n",
    "    gold = yf.download(\"GC=F\", start=start_date, end=end_date)\n",
    "    df['Currency Gold Futures'] = gold['Close']\n",
    "    \n",
    "    # Get Bitcoin price data\n",
    "    btc = yf.download(\"BTC-USD\", start=start_date, end=end_date)\n",
    "    df['BTC/USD'] = btc['Close']\n",
    "    \n",
    "    # Get Bitcoin price data\n",
    "    btc = yf.download(\"BTC-USD\", start=start_date, end=end_date)\n",
    "    df['BTC Volume'] = btc['Volume']\n",
    "    \n",
    "    # Forward fill missing values (for weekends/holidays)\n",
    "    df['Currency US Dollar Index'] = df['Currency US Dollar Index']\n",
    "    df['Currency Gold Futures'] = df['Currency Gold Futures']\n",
    "    df['BTC/USD'] = df['BTC/USD']\n",
    "    \n",
    "    # Calculate Gold/BTC Ratio where BTC price is not zero or null\n",
    "    df['Gold/BTC Ratio'] = df['Currency Gold Futures'].div(df['BTC/USD'].replace(0, float('nan')))\n",
    "    df['Gold/BTC Ratio'] = df['Gold/BTC Ratio']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blockchain_metric(metric_name, start_date, end_date):\n",
    "    \n",
    "    # Fetch single blockchain metric one by one, from the Blockchain.info API.\n",
    " \n",
    "    # Convert dates to timestamps\n",
    "    start_ts = int(datetime.strptime(start_date, \"%Y-%m-%d\").timestamp())\n",
    "    end_ts = int(datetime.strptime(end_date, \"%Y-%m-%d\").timestamp())\n",
    "    \n",
    "    # Fetch data from API with updated URL structure\n",
    "    url = f\"{\"https://api.blockchain.info\"}/{metric_name}\"\n",
    "    params = {\n",
    "        \"timespan\": \"all\",\n",
    "        \"start\": start_ts,\n",
    "        \"end\": end_ts,\n",
    "        \"format\": \"json\",\n",
    "        \"sampled\": \"true\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        return pd.Series(index=pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "        \n",
    "    data = response.json()\n",
    "    \n",
    "    # Check if the response has the expected structure\n",
    "    if not isinstance(data, dict) or 'values' not in data:\n",
    "        return pd.Series(index=pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "    \n",
    "    # Process the values\n",
    "    values = []\n",
    "    timestamps = []\n",
    "    for entry in data['values']:\n",
    "        if isinstance(entry, (list, tuple)) and len(entry) >= 2:\n",
    "            timestamps.append(entry[0])\n",
    "            values.append(float(entry[1]))\n",
    "        elif isinstance(entry, dict) and 'x' in entry and 'y' in entry:\n",
    "            timestamps.append(entry['x'])\n",
    "            values.append(float(entry['y']))\n",
    "    \n",
    "    if not values:\n",
    "        return pd.Series(index=pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "    \n",
    "    # Create DataFrame and handle data types\n",
    "    df = pd.DataFrame({'timestamp': timestamps, 'value': values})\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s').dt.normalize()\n",
    "    df = df.drop_duplicates('timestamp', keep='last')\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    # Handle potential overflow for large numbers\n",
    "    df['value'] = df['value'].astype('float64')\n",
    "    \n",
    "    # Reindex to ensure consistent date range\n",
    "    return df['value'].reindex(date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onchain_metrics(start_date, end_date):\n",
    "    \n",
    "    # Fetch on-chain metrics from Blockchain.info API.\n",
    "    \n",
    "    # Define metrics and their API endpoints with updated paths\n",
    "    metrics = {\n",
    "        'Onchain Active Addresses': 'charts/n-unique-addresses',\n",
    "        'Onchain Transaction Count': 'charts/n-transactions',\n",
    "        'Onchain Hash Rate (GH/s)': 'charts/hash-rate',\n",
    "        'Onchain Mining Difficulty': 'charts/difficulty',\n",
    "        'Onchain Transaction Fees (BTC)': 'charts/transaction-fees',\n",
    "        'Onchain Median Confirmation Time (min)': 'charts/median-confirmation-time'\n",
    "    }\n",
    "    \n",
    "    # Fetch each metric\n",
    "    for col_name, metric_name in metrics.items():\n",
    "        series = get_blockchain_metric(metric_name, start_date, end_date)\n",
    "        df[col_name] = series\n",
    "        \n",
    "        # Handle missing values for each metric appropriately\n",
    "        if col_name in ['Onchain Mining Difficulty', 'Onchain Hash Rate (GH/s)']:\n",
    "            # These metrics should be forward filled as they change infrequently\n",
    "            df[col_name] = df[col_name]#.fillna(method='ffill')\n",
    "        elif col_name in ['Onchain Transaction Count', 'Onchain Active Addresses']:\n",
    "            # These metrics should use rolling average for gaps\n",
    "            df[col_name] = df[col_name]#.fillna(df[col_name].rolling(window=7, min_periods=1).mean())\n",
    "        else:\n",
    "            # For other metrics, use forward fill with a 7-day limit\n",
    "            df[col_name] = df[col_name]#.fillna(method='ffill', limit=7)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_volatility_indices(start_date, end_date):    \n",
    "    # Create an empty DataFrame to store all volatility indices\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Get CBOE SKEW Index from Yahoo Finance\n",
    "    skew = yf.download(\"^SKEW\", start=start_date, end=end_date)\n",
    "    df['Volatility_CBOE SKEW Index'] = skew['Close']\n",
    "    \n",
    "    # Get VIX\n",
    "    vix = yf.download(\"^VIX\", start=start_date, end=end_date)\n",
    "    df['Volatility_CBOE Volatility Index (VIX)'] = vix['Close']\n",
    "    \n",
    "    # Get Oil VIX\n",
    "    ovx = yf.download(\"^OVX\", start=start_date, end=end_date)\n",
    "    df['Volatility_Crude Oil Volatility Index (OVX)'] = ovx['Close']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@100trillionUSD/modeling-bitcoins-value-with-scarcity-91fa0fc03e25\n",
    "\n",
    "https://medium.com/@100trillionUSD/bitcoin-stock-to-flow-cross-asset-model-50d260feed12\n",
    "\n",
    "https://newhedge.io/bitcoin/stock-to-flow\n",
    "\n",
    "Here I would like to include and calculate the \"Stock to Flow\" model intially conceptualized by \"PlanB\".\n",
    "\n",
    "Cluade AI helped with the S2F model calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stock_to_flow(start_date, end_date):\n",
    "    \n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    s2f_df = pd.DataFrame(index=date_range)\n",
    "    \n",
    "    # API parameters\n",
    "    params = {\n",
    "        \"timespan\": \"all\",\n",
    "        \"start\": int(pd.Timestamp(start_date).timestamp()),\n",
    "        \"end\": int(pd.Timestamp(end_date).timestamp()),\n",
    "        \"format\": \"json\",\n",
    "        \"sampled\": \"false\"\n",
    "    }\n",
    "    \n",
    "    # Get total supply\n",
    "    response = requests.get(\"https://api.blockchain.info/charts/total-bitcoins\", params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()['values']\n",
    "        df = pd.DataFrame(data, columns=['x', 'y'])\n",
    "        df['timestamp'] = pd.to_datetime(df['x'], unit='s').dt.normalize()\n",
    "        stock = df.groupby('timestamp')['y'].mean()\n",
    "        stock = stock.reindex(date_range).interpolate(method='linear')\n",
    "        \n",
    "        # Calculate flow based on Bitcoin halving schedule\n",
    "        s2f_df['timestamp'] = date_range\n",
    "        s2f_df['block height'] = ((s2f_df['timestamp'] - pd.Timestamp('2009-01-03')) / pd.Timedelta(minutes=10)).astype(int) # \"genesis block\" date (January 3, 2009) the first BTC block to be mined.\n",
    "        \n",
    "        # Calculate daily block rewards based on halving schedule\n",
    "        def get_block_reward(block_height):\n",
    "            halvings = block_height // 210000 # Roughly every 4 years there is a BTC \"halving event\" (when the mining rewards are halved) this is every 210,000 blocks.\n",
    "            return 50 / (2 ** halvings)\n",
    "        \n",
    "        s2f_df['daily production'] = s2f_df['block height'].apply(get_block_reward) * 144  # Timing by 144 gives us the total daily Bitcoin production (24 hours * 60 minutes) / 10 minutes = 144 blocks per day, \".apply(get_block_reward)\" calculates the reward for each block height.\n",
    "        \n",
    "        # Calculate S2F ratio (stock divided by yearly flow)\n",
    "        s2f_df['s2f ratio'] = stock / (s2f_df['daily production'] * 365)\n",
    "        \n",
    "        # Calculate expected price using S2F model\n",
    "        # Using the formula: Price = exp(-1.84) * S2F^3.36\n",
    "        s2f_df['S2F Model'] = np.exp(-1.84) * (s2f_df['s2f ratio'] ** 3.36)\n",
    "        \n",
    "        # Convert to USD and handle any extreme values\n",
    "        s2f_df['S2F Model'] = s2f_df['S2F Model']\n",
    "        \n",
    "        return s2f_df[['S2F Model']]\n",
    "    \n",
    "    return pd.DataFrame(index=date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all data components\n",
    "components = [\n",
    "    ('Stockmarket', get_market_stock_data(start_date, end_date)),\n",
    "    ('Currency Metrics', get_currency_metrics(start_date, end_date)),\n",
    "    ('On-chain Metrics', get_onchain_metrics(start_date, end_date)),\n",
    "    ('Volatility Indices', get_volatility_indices(start_date, end_date)),\n",
    "    ('S2F Model', calculate_stock_to_flow(start_date, end_date))\n",
    "    ]\n",
    "\n",
    "# Combine all components\n",
    "for name, component_df in components:\n",
    "    if component_df is not None and not component_df.empty:\n",
    "        for column in component_df.columns:\n",
    "            df[column] = component_df[column]\n",
    "\n",
    "# Reorder columns to group related metrics together\n",
    "column_order = [\n",
    "    'Global averaged stocks(USD)',\n",
    "    'Global averaged stocks (volume)',\n",
    "    'Currency US Dollar Index',\n",
    "    'Currency Gold Futures',\n",
    "    'Volatility_CBOE SKEW Index',\n",
    "    'Volatility_CBOE Volatility Index (VIX)',\n",
    "    'Volatility_Crude Oil Volatility Index (OVX)',\n",
    "    'Gold/BTC Ratio',\n",
    "    'BTC/USD',\n",
    "    'BTC Volume',\n",
    "    'S2F Model',\n",
    "    'Onchain Active Addresses',\n",
    "    'Onchain Transaction Count',\n",
    "    'Onchain Hash Rate (GH/s)',\n",
    "    'Onchain Mining Difficulty',\n",
    "    'Onchain Transaction Fees (BTC)',\n",
    "    'Onchain Median Confirmation Time (min)'\n",
    "]\n",
    "\n",
    "# Reorder the columns\n",
    "df = df[column_order]\n",
    "\n",
    "# Save the dataset\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "df.to_csv(output_path)\n",
    "print(f\"Dataset saved to {output_path}\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, provided that the code all runs correctly. We should have a dataset that is largly complete, except for missing entries in the weekends for stockmarket data and every other day in the blckchain metrics for BTC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved dataset\n",
    "df = pd.read_csv(output_path, index_col=0, parse_dates=True)\n",
    "\n",
    "# Interpolate each column based on its data type\n",
    "for column in df.columns:\n",
    "    # For all other metrics (prices, volumes, etc), use linear interpolation\n",
    "    df[column] = df[column].interpolate(method='linear', limit=5)\n",
    "\n",
    "# We will need to remove the first row of data as it contains null entries and there is no way to interpolate it.\n",
    "df = df.iloc[1:]  \n",
    "\n",
    "# Save the interpolated dataset with a new name\n",
    "interpolated_path = output_dir / \"2015-2025_dataset_interpolated.csv\"\n",
    "df.to_csv(interpolated_path)\n",
    "print(f\"\\nInterpolated dataset saved to {interpolated_path}\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_dir / \"2015-2025_dataset_interpolated.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Create a copy of the dataframe and normalize all columns\n",
    "df_normalized = pd.DataFrame(\n",
    "    scaler.fit_transform(df),\n",
    "    columns=df.columns,\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "# Save as new\n",
    "df_normalized.to_csv(output_dir / \"2015-2025_dataset_normalized.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this block creates a simple polt of each graph, allowing us to check and compare with online sources such as tradingview for accuracy. If there is anything major missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_dir / \"2015-2025_dataset_normalized.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Create subplots for each column\n",
    "n_cols = len(df.columns)\n",
    "fig, axes = plt.subplots(n_cols, 1, figsize=(15, 7*n_cols))\n",
    "\n",
    "# Plot each column\n",
    "for i, column in enumerate(df.columns):\n",
    "    # Create the plot on the corresponding subplot\n",
    "    axes[i].plot(df.index, df[column])\n",
    "    \n",
    "    # Customize each subplot\n",
    "    axes[i].set_title(f'{column}', fontsize=14)\n",
    "    axes[i].set_xlabel('Date', fontsize=12)\n",
    "    axes[i].set_ylabel('Value', fontsize=12)\n",
    "    axes[i].grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show all plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://dx.doi.org/10.3390/info12100388 - how to pre-process! [decription of how to do \"wavelet decomposition\" and/or \"wavelet\" denoising]\n",
    "\n",
    "Instead of handling outliers in a more traditional approach, since financial data is real world data and I am not versed enough in finance and economics to understand fully what kind of data could or should classify as \"outliers\" with much confidence. I instead would prefer to try \"denoising\" from the literacture I've found on the similar projects.\n",
    "\n",
    "\"Wavelet transforms analyse stock market trends over different periods and often show superior performance. Peng et al. (2021) demonstrated that combining multiresolution wavelet reconstruction with deep learning significantly improves medium-term stock prediction accuracy, achieving a 75% hit rate for US stocks. Another study introduced the Adaptive Multi-Scale Wavelet Neural Network (AMSW-NN), which performs well but depends on dataset quality (Ouyang et al., 2021).\" - https://arxiv.org/html/2408.12408v1#S3.SS3 TL;DR \"multiresolution wavelet reconstruction\"  is very good and preferable over \"adaptive Multi-Scale Wavelet Neural Network (AMSW-NN)\" due to its increased dependance on quality data. - \"multiresolution wavelet\" method explained in greater detail here: Peng et al. (2021) [https://www.mdpi.com/2078-2489/12/10/388] - only had a 0.63% improvement with much greater complexity, Best to keep things simple for both my sanity in programming and the \"computational efficienty\" of Pan Tang, Cheng Tang and Keren Wang's [https://doi.org/10.1002/for.3071] apporach:\n",
    "\n",
    "\n",
    "\"LSTM (long short-term memory), we propose a hybrid model of wavelet transform (WT) and multi-input LSTM\"\n",
    "\n",
    "LSTM + WT = flexible model.\n",
    "\n",
    "\"level 1 decomposition with db4 mother wavelet to eliminate noise. originall used in image processing. it is more widely\n",
    "used in stock price forecasting (Aussem, 1998; Alru-maih & Al-Fawzan, 2002; Caetano & Yoneyama, 2007; Huang, 2011)\"\n",
    "\n",
    "y[n] = Σ x[k]g[2n-k]  # Low-pass filter\n",
    "y[n] = Σ x[k]h[2n-k]  # High-pass filter\n",
    "\n",
    "\"db4\" stands for \"Daubechies-4\" wavelet\n",
    "It's called a \"mother\" wavelet because it's the original pattern that gets scaled and shifted\n",
    "The \"4\" represents the number of vanishing moments (a measure of complexity)\n",
    "\n",
    "Tang's Approach (2024):\n",
    "Simple level 1 decomposition with db4\n",
    "Complete zeroing of high-frequency coefficients\n",
    "Claimed Results:\n",
    "> Test accuracy increased from 51.72% - 57.76% to 64.66% - 72.19% after applying their denoising method\n",
    "> Focused on LSTM model performance improvement\n",
    "López Gil/Peng's Approach (2021, 2024):\n",
    "Multi-level (3-level) decomposition with db4\n",
    "Adaptive thresholding at each level\n",
    "López Gil's Results:\n",
    "> Achieved 72.82% test accuracy and 73.16% F1 score on the EWZ daily dataset\n",
    "Peng's Original Results:\n",
    "> Achieved 75% hit rate for US stocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude AI helped a lot here:\n",
    "def wavelet_denoising(df, wavelet='db4'):\n",
    "    df_denoised = df.copy()\n",
    "    \n",
    "    for column in df.columns:\n",
    "        # 1. Level 1 decomposition with db4\n",
    "        coeffs = pywt.wavedec(df[column].values, wavelet, level=1)\n",
    "        \n",
    "        # 2. Zero out high-frequency coefficients (detail coefficients)\n",
    "        coeffs_modified = list(coeffs)\n",
    "        coeffs_modified[1] = np.zeros_like(coeffs[1])\n",
    "        \n",
    "        # 3. Reconstruct\n",
    "        denoised_data = pywt.waverec(coeffs_modified, wavelet)\n",
    "        \n",
    "        # Handle length if needed\n",
    "        if len(denoised_data) > len(df):\n",
    "            denoised_data = denoised_data[:len(df)]\n",
    "            \n",
    "        df_denoised[column] = denoised_data\n",
    "    \n",
    "    return df_denoised\n",
    "\n",
    "def plot_denoising_results(original_data, denoised_data, column_name):\n",
    "    noise = original_data[column_name] - denoised_data[column_name]\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Original and denoised data\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(original_data.index, original_data[column_name], \n",
    "             label='Original', alpha=0.5)\n",
    "    plt.plot(denoised_data.index, denoised_data[column_name], \n",
    "             label='Denoised', alpha=0.8)\n",
    "    plt.title(f'Denoising Results for {column_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Removed noise\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(original_data.index, noise, label='Removed Noise', \n",
    "             alpha=0.5, color='red')\n",
    "    plt.title('Removed Noise Component')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "df = pd.read_csv(output_dir / \"2015-2025_dataset_normalized.csv\", \n",
    "                    index_col=0, parse_dates=True)\n",
    "\n",
    "df_denoised = wavelet_denoising(df)\n",
    "\n",
    "for column in df.columns:\n",
    "    plot_denoising_results(df, df_denoised, column)\n",
    "\n",
    "df_denoised.to_csv(output_dir / \"2015-2025_dataset_denoised.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for feature selection: [https://arxiv.org/pdf/2303.02223v2] (Chen's) - uses \"Random Forest importance scores, Mutual Information, Correlation analysis\" method for feature selection, with good success with \"multi-input LSTM\" (which I intent to use as my model of choice.)\n",
    "\n",
    "[https://doi.org/10.1002/for.3071] - Teng's method\n",
    "\n",
    "[https://www.mdpi.com/1999-4893/10/4/114] - tyralis's method\n",
    "\n",
    "tyralis: Uses Random Forest with rolling windows, Calculates permutation importance (how much performance drops when feature is shuffled, Selects features scoring above (mean + std) threshold.\n",
    "\n",
    "Tang: Fixed threshold with RF importance.\n",
    "\n",
    "Chen: Adaptive weighting based on market volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main block: # Claude AI helped a lot here:\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
    "    return df\n",
    "\n",
    "def save_selected_features(df, selected_features, target, method_name):\n",
    "    final_features = selected_features + [target]\n",
    "    df_selected = df[final_features]\n",
    "    output_path = output_dir / f\"2015-2025_dataset_selected_features_{method_name}.csv\"\n",
    "    df_selected.to_csv(output_path)\n",
    "    return df_selected.shape\n",
    "\n",
    "def plot_feature_importance(scores_df, method_name):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    scores_df.sort_values('Score', ascending=True).plot(kind='barh')\n",
    "    plt.title(f'Feature Importance Scores - {method_name}')\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show(output_dir / f'feature_importance_{method_name}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tyralis's method, Claude AI helped a lot here:\n",
    "def tyralis_selection(df, target='BTC/USD', forecast_horizon=1):\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    \n",
    "    def calculate_permutation_importance(X_train, y_train, X_val, y_val):\n",
    "        rf = RandomForestRegressor(n_estimators=500,  \n",
    "                                 max_features='sqrt',  \n",
    "                                 random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        # Calculate permutation importance\n",
    "        importances = []\n",
    "        baseline_score = rf.score(X_val, y_val)\n",
    "        \n",
    "        for col in X_train.columns:\n",
    "            X_val_permuted = X_val.copy()\n",
    "            X_val_permuted[col] = np.random.permutation(X_val_permuted[col])\n",
    "            permuted_score = rf.score(X_val_permuted, y_val)\n",
    "            importance = baseline_score - permuted_score\n",
    "            importances.append(importance)\n",
    "            \n",
    "        return pd.Series(importances, index=X_train.columns)\n",
    "    \n",
    "    # Rolling origin evaluation\n",
    "    importance_scores = []\n",
    "    window_size = len(df) // 3  # Minimum training size\n",
    "    \n",
    "    for i in range(window_size, len(df) - forecast_horizon):\n",
    "        # Training and validation sets\n",
    "        X_train = X.iloc[i-window_size:i]\n",
    "        y_train = y.iloc[i-window_size:i]\n",
    "        X_val = X.iloc[i:i+forecast_horizon]\n",
    "        y_val = y.iloc[i:i+forecast_horizon]\n",
    "        \n",
    "        # Calculate importance for this window\n",
    "        window_importance = calculate_permutation_importance(\n",
    "            X_train, y_train, X_val, y_val\n",
    "        )\n",
    "        importance_scores.append(window_importance)\n",
    "    \n",
    "    # Average importance scores across all windows\n",
    "    final_importance = pd.DataFrame(importance_scores).mean()\n",
    "    \n",
    "    # Select features\n",
    "    threshold = final_importance.mean() + final_importance.std()\n",
    "    selected_features = final_importance[final_importance > threshold]\n",
    "    \n",
    "    return selected_features.index.tolist(), pd.DataFrame({\n",
    "        'Feature': final_importance.index,\n",
    "        'Score': final_importance.values\n",
    "    }).sort_values('Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teng's method, Claude AI helped a lot here:\n",
    "def tang_selection(df, target='BTC/USD', threshold=0.1):\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    \n",
    "    # Random Forest importance\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    # Calculate importance scores\n",
    "    importance_scores = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Score': rf.feature_importances_\n",
    "    }).sort_values('Score', ascending=False)\n",
    "\n",
    "    selected_features = importance_scores[importance_scores['Score'] > threshold]\n",
    "    \n",
    "    return selected_features['Feature'].tolist(), importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chen's method, Claude AI helped a lot here:\n",
    "def chen_selection(df, target='BTC/USD', volatility_window=30):\n",
    "# adaptive weighting based on market volatility\n",
    "\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    \n",
    "    # Calculate market volatility\n",
    "    returns = df[target].pct_change()\n",
    "    volatility = returns.rolling(window=volatility_window).std().iloc[-1]\n",
    "    \n",
    "    # Calculate Information Value\n",
    "    def calculate_iv(feature):\n",
    "        bins = pd.qcut(feature, q=10, duplicates='drop')\n",
    "        iv = mutual_info_regression(feature.values.reshape(-1, 1), y)[0]\n",
    "        return iv\n",
    "    \n",
    "    iv_scores = pd.Series({col: calculate_iv(X[col]) for col in X.columns})\n",
    "    \n",
    "    # Calculate Stability Index\n",
    "    def calculate_psi(feature):\n",
    "        mid_point = len(feature) // 2\n",
    "        hist1, _ = np.histogram(feature[:mid_point], bins=10)\n",
    "        hist2, _ = np.histogram(feature[mid_point:], bins=10)\n",
    "        hist1 = hist1/sum(hist1)\n",
    "        hist2 = hist2/sum(hist2)\n",
    "        hist1 = np.clip(hist1, 0.0001, None)\n",
    "        hist2 = np.clip(hist2, 0.0001, None)\n",
    "        return np.sum((hist1 - hist2) * np.log(hist1/hist2))\n",
    "    \n",
    "    psi_scores = pd.Series({col: calculate_psi(X[col]) for col in X.columns})\n",
    "    \n",
    "    # Random Forest importance\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    rf_scores = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "    \n",
    "    # Normalize scores\n",
    "    iv_norm = (iv_scores - iv_scores.min()) / (iv_scores.max() - iv_scores.min())\n",
    "    psi_norm = 1 - (psi_scores - psi_scores.min()) / (psi_scores.max() - psi_scores.min())\n",
    "    rf_norm = (rf_scores - rf_scores.min()) / (rf_scores.max() - rf_scores.min())\n",
    "    \n",
    "    # Adaptive weights based on volatility\n",
    "    if volatility > 0.02:  # High volatility\n",
    "        weights = {'iv': 0.4, 'psi': 0.4, 'rf': 0.2}\n",
    "    else:  # Normal conditions\n",
    "        weights = {'iv': 0.3, 'psi': 0.2, 'rf': 0.5}\n",
    "    \n",
    "    # Calculate final scores\n",
    "    final_scores = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Score': weights['iv'] * iv_norm + weights['psi'] * psi_norm + weights['rf'] * rf_norm\n",
    "    }).sort_values('Score', ascending=False)\n",
    "    \n",
    "    # Select features above 75th percentile\n",
    "    threshold = final_scores['Score'].quantile(0.75)\n",
    "    selected_features = final_scores[final_scores['Score'] > threshold]\n",
    "    \n",
    "    return selected_features['Feature'].tolist(), final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude AI helped a lot here:\n",
    "def run_feature_selection(method_choice='chen'):\n",
    "    df = pd.read_csv(output_dir / \"2015-2025_dataset_denoised.csv\", \n",
    "                    index_col=0, parse_dates=True)\n",
    "    target = 'BTC/USD'\n",
    "    \n",
    "    methods = {\n",
    "        'tyralis': tyralis_selection,\n",
    "        'tang': tang_selection,\n",
    "        'chen': chen_selection\n",
    "    }\n",
    "    \n",
    "    selected_features, importance_scores = methods[method_choice](df, target)\n",
    "    \n",
    "    shape = save_selected_features(df, selected_features, target, method_choice)\n",
    "    \n",
    "    plot_feature_importance(importance_scores, method_choice)\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features\")\n",
    "    print(\"Features:\", ', '.join(selected_features))\n",
    "    print(f\"Output shape: {shape}\")\n",
    "\n",
    "run_feature_selection('chen')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
