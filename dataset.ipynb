{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if pip install isnt working:\n",
    "# !python --version\n",
    "\n",
    "# !curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
    "# !python get-pip.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add citations to functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: yfinance in c:\\python312\\lib\\site-packages (0.2.54)\n",
      "Requirement already satisfied: requests in c:\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: PyWavelets in c:\\python312\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: seaborn in c:\\python312\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\python312\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\python312\\lib\\site-packages (1.15.2)\n",
      "Requirement already satisfied: statsmodels in c:\\python312\\lib\\site-packages (0.14.4)\n",
      "Requirement already satisfied: tensorflow in c:\\python312\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: tqdm in c:\\python312\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: ipywidgets in c:\\python312\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: boruta in c:\\python312\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\python312\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\python312\\lib\\site-packages (from yfinance) (4.3.6)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\python312\\lib\\site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\python312\\lib\\site-packages (from yfinance) (3.17.9)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\python312\\lib\\site-packages (from yfinance) (4.13.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests) (2024.7.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python312\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python312\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python312\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\python312\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python312\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\python312\\lib\\site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\python312\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\python312\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\python312\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\python312\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\python312\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\python312\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\python312\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\python312\\lib\\site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python312\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\python312\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\python312\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\python312\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\python312\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\python312\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\python312\\lib\\site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\python312\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\python312\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\nina\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\nina\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (9.0.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\nina\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\python312\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\python312\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\python312\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\nina\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\nina\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\nina\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\nina\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\nina\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\nina\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\nina\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: rich in c:\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\nina\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\python312\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\nina\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\nina\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\nina\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# requirements:\n",
    "#!python -m \n",
    "! pip install pandas yfinance requests numpy matplotlib PyWavelets seaborn scikit-learn scipy statsmodels tensorflow tqdm ipywidgets boruta\n",
    "# import the necessary libraries.\n",
    "import requests\n",
    "import pywt\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import deque\n",
    "\n",
    "from statsmodels.robust import mad\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.notebook import tqdm  # For Jupyter Notebooks\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Multiply, LSTM, Activation, LayerNormalization, MultiHeadAttention, Dropout, Bidirectional, Add, BatchNormalization, Layer, Conv1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need a dataset for this project, I was unable to find a complete dataset from a reputable source that suited my specific use case. So decided to look into collecting my own data and compiling one myself. \n",
    "# I'll be using the yfinance library to get the stock market data (in future I would like to include more data sources such as those from Tiingo).\n",
    "# I'll start by making a directory for the datasets.csv that we will need to generate.\n",
    "\n",
    "def vscode_progress(iterable, length=None, desc=''):\n",
    "    length = length or len(iterable)\n",
    "    for i, item in enumerate(iterable):\n",
    "        sys.stdout.write(f'\\r{desc} {i+1}/{length} ({((i+1)/length)*100:.1f}%)')\n",
    "        sys.stdout.flush()\n",
    "        yield item\n",
    "    print()\n",
    "    \n",
    "project_root = Path(os.getcwd()) #find file path of the working directory for notebook scripts.\n",
    "output_dir = project_root / \"dataset\"\n",
    "#check to see if the directory exists, make it if it doesn't\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_path = output_dir / \"2015-2025_dataset.csv\"\n",
    "\n",
    "# defining the date range for the dataset. \n",
    "# we'll be using start and end dates multiples times so its best to define them here only once, its likely we'll need to generate seperate time frames for the test/training split.\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2025-02-01\"\n",
    "\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D') # Format: YYYY-MM-DD, \"freq\" is the frequency of dates in this case: ='D' means daily.\n",
    "\n",
    "df = pd.DataFrame(index=date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will need to define a few functions that are needed to be run before I can generate the completed dataset require the for the AI model to work. The first block here is Largely AI assisted with Claude sonnet 3.5: Using the yahoo finance API, this block pulls the historical trading data needed for each individual major global stock exhange for volume and closed price, then converted the currencies to USD using forex data (also from the yfinance API) and returns the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stock_data(symbol: str, currency: str, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    df = yf.download(symbol, start=start_date, end=end_date)\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(index=date_range)\n",
    "        \n",
    "    # Create a new DataFrame with just Close and Volume\n",
    "    result = pd.DataFrame(index=df.index)\n",
    "    result['Close'] = df['Close']\n",
    "    result['Volume'] = df['Volume']\n",
    "    \n",
    "    # Get currency conversion rate if needed\n",
    "    if currency:\n",
    "        fx_data = yf.download(currency, start=start_date, end=end_date)\n",
    "        if not fx_data.empty:\n",
    "            fx_rate = fx_data['Close']\n",
    "            \n",
    "            # Ensure both dataframes have datetime index\n",
    "            result.index = pd.to_datetime(result.index)\n",
    "            fx_rate.index = pd.to_datetime(fx_rate.index)\n",
    "            \n",
    "            # Find common dates between stock and forex data\n",
    "            common_dates = result.index.intersection(fx_rate.index)            \n",
    "            # Keep only dates where we have both stock and forex data\n",
    "            result = result.loc[common_dates]\n",
    "            fx_rate = fx_rate.loc[common_dates]\n",
    "            \n",
    "            # Convert only Close prices to USD using element-wise multiplication\n",
    "            result['Close'] = result['Close'].values * fx_rate.values\n",
    "        else:\n",
    "            return pd.DataFrame(index=date_range)\n",
    "    \n",
    "    # Handle volume based on the index\n",
    "    if symbol in ['^N225', '^HSI']:  # Asian markets often have lower nominal volumes\n",
    "        result['Volume'] = result['Volume'] / 1_000  # Convert to thousands\n",
    "    else:\n",
    "        result['Volume'] = result['Volume'] / 1_000_000  # Convert to millions\n",
    "                \n",
    "    # Add sanity checks for extreme values\n",
    "    if result['Close'].max() > 50000 or result['Close'].min() < 1:\n",
    "        return pd.DataFrame(index=date_range)\n",
    "        \n",
    "    if result['Volume'].min() == 0 or result['Volume'].max() / result['Volume'].min() > 1000:\n",
    "        return pd.DataFrame(index=date_range)\n",
    "        \n",
    "    # Rename columns with symbol prefix\n",
    "    result = result.rename(columns={\n",
    "        'Close': f'{symbol}_Close_USD',\n",
    "        'Volume': f'{symbol}_Volume_M'  # M for millions or thousands for Asian markets\n",
    "    })\n",
    "    \n",
    "    # Reindex to full date range without filling\n",
    "    result = result.reindex(date_range)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block calls the previous blocks function iteratively for each of the 7 stock markets I have decided to include in the data collection. After aquiring the entire daily closed USD price and volume data for each exchange they are averaged together into two combined columns for the previously specified time frame.\n",
    "\n",
    "- \"(Tang et al.) demonstrates strong correlations between global market indices and crypto markets\" \n",
    "- \"The inclusion of Asian markets (Nikkei, Hang Seng) is particularly relevant as studies have shown significant Bitcoin trading volume from these regions\" \n",
    "- \"The SKEW index; research shows its effectiveness in predicting \"black swan\" events in crypto markets, OVX (Oil Volatility) \"Enhancing Bitcoin Price Prediction with Deep Learning\" shows volatility indices are key predictors\"\n",
    "\n",
    "- \"\"Cryptocurrency Valuation: An Explainable AI Approach\" validates the use of on-chain metrics as fundamental indicators\" - \"Hash rate and mining difficulty are particularly important as they reflect network security and mining economics\"\n",
    "- \"Transaction metrics provide insight into network usage and adoption\"\n",
    "\n",
    "- \"Deep Learning for Financial Applications: A Survey\" supports the inclusion of traditional safe-haven assets like gold, The DXY (Dollar Index) inclusion is supported by research showing strong inverse correlations with Bitcoin during certain market conditions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_market_stock_data(start_date, end_date):\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    result_df = pd.DataFrame(index=date_range)\n",
    "    \n",
    "    # Define indices with their currencies\n",
    "    indices = {\n",
    "        'GDAXI': {'symbol': '^GDAXI', 'currency': 'EURUSD=X'},    # Germany DAX\n",
    "        'IXIC': {'symbol': '^IXIC', 'currency': None},            # NASDAQ (already in USD)\n",
    "        'DJI': {'symbol': '^DJI', 'currency': None},              # Dow Jones (already in USD)\n",
    "        'N225': {'symbol': '^N225', 'currency': 'JPYUSD=X'},      # Nikkei\n",
    "        'STOXX50E': {'symbol': '^STOXX', 'currency': 'EURUSD=X'}, # Euro STOXX 50\n",
    "        'HSI': {'symbol': '^HSI', 'currency': 'HKDUSD=X'},        # Hang Seng\n",
    "        'FTSE': {'symbol': '^FTSE', 'currency': 'GBPUSD=X'}       # FTSE 100\n",
    "    }\n",
    "    \n",
    "    # Fetch data for all indices\n",
    "    combined_df = pd.DataFrame(index=date_range)\n",
    "    \n",
    "    for name, info in indices.items():\n",
    "        index_data = fetch_stock_data(info['symbol'], info['currency'], start_date, end_date)\n",
    "        if not index_data.empty and len(index_data.columns) > 0:\n",
    "            combined_df = pd.concat([combined_df, index_data], axis=1)\n",
    "    \n",
    "    # Calculate global averages\n",
    "    close_cols = [col for col in combined_df.columns if str(col).endswith('_Close_USD')]\n",
    "    volume_cols = [col for col in combined_df.columns if str(col).endswith('_Volume_M')]\n",
    "    \n",
    "    if close_cols and volume_cols:\n",
    "        result_df = pd.DataFrame(index=date_range)\n",
    "        result_df['Global averaged stocks(USD)'] = combined_df[close_cols].mean(axis=1, skipna=True)\n",
    "        result_df['Global averaged stocks (volume)'] = combined_df[volume_cols].mean(axis=1, skipna=True)\n",
    "        \n",
    "        return result_df\n",
    "    return pd.DataFrame(index=date_range, columns=['Global averaged stocks(USD)', 'Global averaged stocks (volume)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function works similar to the previous, collecting the US Dollar index (DXY) and the gold futures data from Yahoo Finance. Along with the Bitcoin-USD paring with its respective volume data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_currency_metrics(start_date, end_date):   \n",
    "    result_df = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "    # Get DXY (US Dollar Index)\n",
    "    dxy = yf.download(\"DX-Y.NYB\", start=start_date, end=end_date)\n",
    "    result_df['Currency US Dollar Index'] = dxy['Close']\n",
    "    \n",
    "    # Get Gold Futures\n",
    "    gold = yf.download(\"GC=F\", start=start_date, end=end_date)\n",
    "    result_df['Currency Gold Futures'] = gold['Close']\n",
    "    \n",
    "    # Get Bitcoin price data\n",
    "    btc = yf.download(\"BTC-USD\", start=start_date, end=end_date)\n",
    "    result_df['BTC/USD'] = btc['Close']\n",
    "    \n",
    "    # Get Bitcoin price data\n",
    "    btc = yf.download(\"BTC-USD\", start=start_date, end=end_date)\n",
    "    result_df['BTC Volume'] = btc['Volume']\n",
    "    \n",
    "    # Calculate Gold/BTC Ratio where BTC price is not zero or null\n",
    "    result_df['Gold/BTC Ratio'] = result_df['Currency Gold Futures'].div(result_df['BTC/USD'].replace(0, float('nan')))\n",
    "    result_df['Gold/BTC Ratio'] = result_df['Gold/BTC Ratio']\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same again here, with some additional assistence from Clude AI, and using the blockchain.info API, this function collects the individual \"on chain\" metrics that were chosen for inclusion in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blockchain_metric(metric_name, start_date, end_date):\n",
    "    \n",
    "    # Fetch single blockchain metric one by one, from the Blockchain.info API.\n",
    " \n",
    "    # Convert dates to timestamps\n",
    "    start_ts = int(datetime.strptime(start_date, \"%Y-%m-%d\").timestamp())\n",
    "    end_ts = int(datetime.strptime(end_date, \"%Y-%m-%d\").timestamp())\n",
    "    \n",
    "    # Fetch data from API with updated URL structure\n",
    "    url = f\"{\"https://api.blockchain.info\"}/{metric_name}\"\n",
    "    params = {\n",
    "        \"timespan\": \"all\",\n",
    "        \"start\": start_ts,\n",
    "        \"end\": end_ts,\n",
    "        \"format\": \"json\",\n",
    "        \"sampled\": \"true\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        return pd.Series(index=pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "        \n",
    "    data = response.json()\n",
    "    \n",
    "    # Check if the response has the expected structure\n",
    "    if not isinstance(data, dict) or 'values' not in data:\n",
    "        return pd.Series(index=pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "    \n",
    "    # Process the values\n",
    "    values = []\n",
    "    timestamps = []\n",
    "    for entry in data['values']:\n",
    "        if isinstance(entry, (list, tuple)) and len(entry) >= 2:\n",
    "            timestamps.append(entry[0])\n",
    "            values.append(float(entry[1]))\n",
    "        elif isinstance(entry, dict) and 'x' in entry and 'y' in entry:\n",
    "            timestamps.append(entry['x'])\n",
    "            values.append(float(entry['y']))\n",
    "    \n",
    "    if not values:\n",
    "        return pd.Series(index=pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "    \n",
    "    # Create DataFrame and handle data types\n",
    "    df = pd.DataFrame({'timestamp': timestamps, 'value': values})\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s').dt.normalize()\n",
    "    df = df.drop_duplicates('timestamp', keep='last')\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    # Handle potential overflow for large numbers\n",
    "    df['value'] = df['value'].astype('float64')\n",
    "    \n",
    "    # Reindex to ensure consistent date range\n",
    "    return df['value'].reindex(date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calls the previous block iteratively for each metric of \"on chain\" data, it is unclear to me which if any of these metrics have high enough correlation with the BTC-USD price movement to warrent final selection. As a reult I decided to include more than I would expect are required in the interest of thoroughness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onchain_metrics(start_date, end_date):\n",
    "    result_df = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "    # Define metrics and their API endpoints with updated paths\n",
    "    metrics = {\n",
    "        'Onchain Active Addresses': 'charts/n-unique-addresses',\n",
    "        'Onchain Transaction Count': 'charts/n-transactions',\n",
    "        'Onchain Hash Rate (GH/s)': 'charts/hash-rate',\n",
    "        'Onchain Mining Difficulty': 'charts/difficulty',\n",
    "        'Onchain Transaction Fees (BTC)': 'charts/transaction-fees',\n",
    "        'Onchain Median Confirmation Time (min)': 'charts/median-confirmation-time'\n",
    "    }\n",
    "    \n",
    "    # Fetch each metric\n",
    "    for col_name, metric_name in metrics.items():\n",
    "        series = get_blockchain_metric(metric_name, start_date, end_date)\n",
    "        result_df[col_name] = series\n",
    "        \n",
    "        # Handle missing values for each metric appropriately\n",
    "        if col_name in ['Onchain Mining Difficulty', 'Onchain Hash Rate (GH/s)']:\n",
    "            result_df[col_name] = result_df[col_name]\n",
    "        elif col_name in ['Onchain Transaction Count', 'Onchain Active Addresses']:\n",
    "            result_df[col_name] = result_df[col_name]\n",
    "        else:\n",
    "            result_df[col_name] = result_df[col_name]\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These additional metrics track the volatility of the S&P500 stock market and the crude oil volatility index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_volatility_indices(start_date, end_date):    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Get CBOE SKEW Index from Yahoo Finance\n",
    "    skew = yf.download(\"^SKEW\", start=start_date, end=end_date)\n",
    "    df['Volatility_CBOE SKEW Index'] = skew['Close']\n",
    "    \n",
    "    # Get VIX\n",
    "    vix = yf.download(\"^VIX\", start=start_date, end=end_date)\n",
    "    df['Volatility_CBOE Volatility Index (VIX)'] = vix['Close']\n",
    "    \n",
    "    # Get Oil VIX\n",
    "    ovx = yf.download(\"^OVX\", start=start_date, end=end_date)\n",
    "    df['Volatility_Crude Oil Volatility Index (OVX)'] = ovx['Close']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@100trillionUSD/modeling-bitcoins-value-with-scarcity-91fa0fc03e25\n",
    "\n",
    "https://medium.com/@100trillionUSD/bitcoin-stock-to-flow-cross-asset-model-50d260feed12\n",
    "\n",
    "https://newhedge.io/bitcoin/stock-to-flow\n",
    "\n",
    "Here I would like to include and calculate the \"Stock to Flow\" model intially conceptualized by \"PlanB\".\n",
    "\n",
    "Cluade AI helped with the S2F model calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stock_to_flow(start_date, end_date):\n",
    "    \n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    s2f_df = pd.DataFrame(index=date_range)\n",
    "    \n",
    "    # API parameters\n",
    "    params = {\n",
    "        \"timespan\": \"all\",\n",
    "        \"start\": int(pd.Timestamp(start_date).timestamp()),\n",
    "        \"end\": int(pd.Timestamp(end_date).timestamp()),\n",
    "        \"format\": \"json\",\n",
    "        \"sampled\": \"false\"\n",
    "    }\n",
    "    \n",
    "    # Get total supply\n",
    "    response = requests.get(\"https://api.blockchain.info/charts/total-bitcoins\", params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()['values']\n",
    "        df = pd.DataFrame(data, columns=['x', 'y'])\n",
    "        df['timestamp'] = pd.to_datetime(df['x'], unit='s').dt.normalize()\n",
    "        stock = df.groupby('timestamp')['y'].mean()\n",
    "        stock = stock.reindex(date_range).interpolate(method='linear')\n",
    "        \n",
    "        # Calculate flow based on Bitcoin halving schedule\n",
    "        s2f_df['timestamp'] = date_range\n",
    "        s2f_df['block height'] = ((s2f_df['timestamp'] - pd.Timestamp('2009-01-03')) / pd.Timedelta(minutes=10)).astype(int) # \"genesis block\" date (January 3, 2009) the first BTC block to be mined.\n",
    "        \n",
    "        # Calculate daily block rewards based on halving schedule\n",
    "        def get_block_reward(block_height):\n",
    "            halvings = block_height // 210000 # Roughly every 4 years there is a BTC \"halving event\" (when the mining rewards are halved) this is every 210,000 blocks.\n",
    "            return 50 / (2 ** halvings)\n",
    "        \n",
    "        s2f_df['daily production'] = s2f_df['block height'].apply(get_block_reward) * 144  # Timing by 144 gives us the total daily Bitcoin production (24 hours * 60 minutes) / 10 minutes = 144 blocks per day, \".apply(get_block_reward)\" calculates the reward for each block height.\n",
    "        \n",
    "        # Calculate S2F ratio (stock divided by yearly flow)\n",
    "        s2f_df['s2f ratio'] = stock / (s2f_df['daily production'] * 365)\n",
    "        \n",
    "        # Calculate expected price using S2F model\n",
    "        # Using the formula: Price = exp(-1.84) * S2F^3.36\n",
    "        s2f_df['S2F Model'] = np.exp(-1.84) * (s2f_df['s2f ratio'] ** 3.36)\n",
    "        \n",
    "        # Convert to USD and handle any extreme values\n",
    "        s2f_df['S2F Model'] = s2f_df['S2F Model']\n",
    "        \n",
    "        return s2f_df[['S2F Model']]\n",
    "    \n",
    "    return pd.DataFrame(index=date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main fuction for compiling, saving and ordering all the columns required for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to e:\\Documents\\skool work\\Msc\\AI module\\Msc-COM7003-AI-Model\\dataset\\2015-2025_dataset.csv\n",
      "Shape: (3685, 17)\n"
     ]
    }
   ],
   "source": [
    "# Get all data components\n",
    "components = [\n",
    "    ('Stockmarket', get_market_stock_data(start_date, end_date)),\n",
    "    ('Currency Metrics', get_currency_metrics(start_date, end_date)),\n",
    "    ('On-chain Metrics', get_onchain_metrics(start_date, end_date)),\n",
    "    ('Volatility Indices', get_volatility_indices(start_date, end_date)),\n",
    "    ('S2F Model', calculate_stock_to_flow(start_date, end_date))\n",
    "    ]\n",
    "\n",
    "# Combine all components\n",
    "for name, component_df in components:\n",
    "    if component_df is not None and not component_df.empty:\n",
    "        for column in component_df.columns:\n",
    "            df[column] = component_df[column]\n",
    "\n",
    "# Reorder columns to group related metrics together\n",
    "column_order = [\n",
    "    'Global averaged stocks(USD)',\n",
    "    'Global averaged stocks (volume)',\n",
    "    'Currency US Dollar Index',\n",
    "    'Currency Gold Futures',\n",
    "    'Volatility_CBOE SKEW Index',\n",
    "    'Volatility_CBOE Volatility Index (VIX)',\n",
    "    'Volatility_Crude Oil Volatility Index (OVX)',\n",
    "    'Gold/BTC Ratio',\n",
    "    'BTC/USD',\n",
    "    'BTC Volume',\n",
    "    'S2F Model',\n",
    "    'Onchain Active Addresses',\n",
    "    'Onchain Transaction Count',\n",
    "    'Onchain Hash Rate (GH/s)',\n",
    "    'Onchain Mining Difficulty',\n",
    "    'Onchain Transaction Fees (BTC)',\n",
    "    'Onchain Median Confirmation Time (min)'\n",
    "]\n",
    "\n",
    "# Reorder the columns\n",
    "df = df[column_order]\n",
    "\n",
    "# Save the dataset\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "df.to_csv(output_path)\n",
    "print(f\"Dataset saved to {output_path}\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Shape: (3685, 17)\" tells us that we have collected all the required columns for the dataset. We should have a dataset that is largly complete, except for missing entries in the weekends for stockmarket data as trading closes on the weekends, unlike crypto which trades 24/7. We can easily view missing data in the CSV file by using \"print(\"missing data by column:\")\" and \"print(df.isna().sum())\". \n",
    "\n",
    "Additionally since we are using financial sequencial data (or time series forecasting) for the models we'll be training, duplicate data is generally less of a concern however its good practice to verify the quality and integrity of the dataset regardless and the unique entries with \"df.unique()\" (then view \"df.head()\" to see the first five values in the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing data by column: \n",
      "Global averaged stocks(USD)                    1149\n",
      "Global averaged stocks (volume)                1149\n",
      "Currency US Dollar Index                       1149\n",
      "Currency Gold Futures                          1151\n",
      "Volatility_CBOE SKEW Index                     1247\n",
      "Volatility_CBOE Volatility Index (VIX)         1247\n",
      "Volatility_Crude Oil Volatility Index (OVX)    1248\n",
      "Gold/BTC Ratio                                 1151\n",
      "BTC/USD                                           1\n",
      "BTC Volume                                        1\n",
      "S2F Model                                         0\n",
      "Onchain Active Addresses                       1844\n",
      "Onchain Transaction Count                      1842\n",
      "Onchain Hash Rate (GH/s)                       1842\n",
      "Onchain Mining Difficulty                      1842\n",
      "Onchain Transaction Fees (BTC)                 1842\n",
      "Onchain Median Confirmation Time (min)         1842\n",
      "dtype: int64 \n",
      "\n",
      "duplicates by column: \n",
      "0\n",
      "\n",
      "duplicates: \n",
      "Empty DataFrame\n",
      "Columns: [Global averaged stocks(USD), Global averaged stocks (volume), Currency US Dollar Index, Currency Gold Futures, Volatility_CBOE SKEW Index, Volatility_CBOE Volatility Index (VIX), Volatility_Crude Oil Volatility Index (OVX), Gold/BTC Ratio, BTC/USD, BTC Volume, S2F Model, Onchain Active Addresses, Onchain Transaction Count, Onchain Hash Rate (GH/s), Onchain Mining Difficulty, Onchain Transaction Fees (BTC), Onchain Median Confirmation Time (min)]\n",
      "Index: []\n",
      "\n",
      "unique: \n",
      "Global averaged stocks(USD)                    2536\n",
      "Global averaged stocks (volume)                2531\n",
      "Currency US Dollar Index                       1339\n",
      "Currency Gold Futures                          2156\n",
      "Volatility_CBOE SKEW Index                     1808\n",
      "Volatility_CBOE Volatility Index (VIX)         1369\n",
      "Volatility_Crude Oil Volatility Index (OVX)    1737\n",
      "Gold/BTC Ratio                                 2534\n",
      "BTC/USD                                        3681\n",
      "BTC Volume                                     3684\n",
      "S2F Model                                      3685\n",
      "Onchain Active Addresses                       1834\n",
      "Onchain Transaction Count                      1834\n",
      "Onchain Hash Rate (GH/s)                       1741\n",
      "Onchain Mining Difficulty                       479\n",
      "Onchain Transaction Fees (BTC)                 1843\n",
      "Onchain Median Confirmation Time (min)         1117\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Global averaged stocks(USD)</th>\n",
       "      <th>Global averaged stocks (volume)</th>\n",
       "      <th>Currency US Dollar Index</th>\n",
       "      <th>Currency Gold Futures</th>\n",
       "      <th>Volatility_CBOE SKEW Index</th>\n",
       "      <th>Volatility_CBOE Volatility Index (VIX)</th>\n",
       "      <th>Volatility_Crude Oil Volatility Index (OVX)</th>\n",
       "      <th>Gold/BTC Ratio</th>\n",
       "      <th>BTC/USD</th>\n",
       "      <th>BTC Volume</th>\n",
       "      <th>S2F Model</th>\n",
       "      <th>Onchain Active Addresses</th>\n",
       "      <th>Onchain Transaction Count</th>\n",
       "      <th>Onchain Hash Rate (GH/s)</th>\n",
       "      <th>Onchain Mining Difficulty</th>\n",
       "      <th>Onchain Transaction Fees (BTC)</th>\n",
       "      <th>Onchain Median Confirmation Time (min)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>314.248993</td>\n",
       "      <td>8036550.0</td>\n",
       "      <td>415.891844</td>\n",
       "      <td>117529.0</td>\n",
       "      <td>59344.0</td>\n",
       "      <td>335365.290092</td>\n",
       "      <td>4.064096e+10</td>\n",
       "      <td>8.054165</td>\n",
       "      <td>7.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>11279.900146</td>\n",
       "      <td>755.710</td>\n",
       "      <td>91.080002</td>\n",
       "      <td>1186.000000</td>\n",
       "      <td>128.660004</td>\n",
       "      <td>17.790001</td>\n",
       "      <td>54.250000</td>\n",
       "      <td>3.764697</td>\n",
       "      <td>315.032013</td>\n",
       "      <td>7860650.0</td>\n",
       "      <td>416.317366</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>281.082001</td>\n",
       "      <td>33054400.0</td>\n",
       "      <td>416.729918</td>\n",
       "      <td>205926.0</td>\n",
       "      <td>82227.0</td>\n",
       "      <td>331324.744428</td>\n",
       "      <td>4.064096e+10</td>\n",
       "      <td>11.798413</td>\n",
       "      <td>6.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>264.195007</td>\n",
       "      <td>55629100.0</td>\n",
       "      <td>417.156111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>11077.110107</td>\n",
       "      <td>955.315</td>\n",
       "      <td>91.379997</td>\n",
       "      <td>1203.900024</td>\n",
       "      <td>127.790001</td>\n",
       "      <td>19.920000</td>\n",
       "      <td>57.669998</td>\n",
       "      <td>4.386208</td>\n",
       "      <td>274.473999</td>\n",
       "      <td>43962800.0</td>\n",
       "      <td>417.583604</td>\n",
       "      <td>200612.0</td>\n",
       "      <td>95585.0</td>\n",
       "      <td>339405.835756</td>\n",
       "      <td>4.064096e+10</td>\n",
       "      <td>16.091355</td>\n",
       "      <td>6.816667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Global averaged stocks(USD)  Global averaged stocks (volume)  \\\n",
       "2015-01-01                          NaN                              NaN   \n",
       "2015-01-02                 11279.900146                          755.710   \n",
       "2015-01-03                          NaN                              NaN   \n",
       "2015-01-04                          NaN                              NaN   \n",
       "2015-01-05                 11077.110107                          955.315   \n",
       "\n",
       "            Currency US Dollar Index  Currency Gold Futures  \\\n",
       "2015-01-01                       NaN                    NaN   \n",
       "2015-01-02                 91.080002            1186.000000   \n",
       "2015-01-03                       NaN                    NaN   \n",
       "2015-01-04                       NaN                    NaN   \n",
       "2015-01-05                 91.379997            1203.900024   \n",
       "\n",
       "            Volatility_CBOE SKEW Index  \\\n",
       "2015-01-01                         NaN   \n",
       "2015-01-02                  128.660004   \n",
       "2015-01-03                         NaN   \n",
       "2015-01-04                         NaN   \n",
       "2015-01-05                  127.790001   \n",
       "\n",
       "            Volatility_CBOE Volatility Index (VIX)  \\\n",
       "2015-01-01                                     NaN   \n",
       "2015-01-02                               17.790001   \n",
       "2015-01-03                                     NaN   \n",
       "2015-01-04                                     NaN   \n",
       "2015-01-05                               19.920000   \n",
       "\n",
       "            Volatility_Crude Oil Volatility Index (OVX)  Gold/BTC Ratio  \\\n",
       "2015-01-01                                          NaN             NaN   \n",
       "2015-01-02                                    54.250000        3.764697   \n",
       "2015-01-03                                          NaN             NaN   \n",
       "2015-01-04                                          NaN             NaN   \n",
       "2015-01-05                                    57.669998        4.386208   \n",
       "\n",
       "               BTC/USD  BTC Volume   S2F Model  Onchain Active Addresses  \\\n",
       "2015-01-01  314.248993   8036550.0  415.891844                  117529.0   \n",
       "2015-01-02  315.032013   7860650.0  416.317366                       NaN   \n",
       "2015-01-03  281.082001  33054400.0  416.729918                  205926.0   \n",
       "2015-01-04  264.195007  55629100.0  417.156111                       NaN   \n",
       "2015-01-05  274.473999  43962800.0  417.583604                  200612.0   \n",
       "\n",
       "            Onchain Transaction Count  Onchain Hash Rate (GH/s)  \\\n",
       "2015-01-01                    59344.0             335365.290092   \n",
       "2015-01-02                        NaN                       NaN   \n",
       "2015-01-03                    82227.0             331324.744428   \n",
       "2015-01-04                        NaN                       NaN   \n",
       "2015-01-05                    95585.0             339405.835756   \n",
       "\n",
       "            Onchain Mining Difficulty  Onchain Transaction Fees (BTC)  \\\n",
       "2015-01-01               4.064096e+10                        8.054165   \n",
       "2015-01-02                        NaN                             NaN   \n",
       "2015-01-03               4.064096e+10                       11.798413   \n",
       "2015-01-04                        NaN                             NaN   \n",
       "2015-01-05               4.064096e+10                       16.091355   \n",
       "\n",
       "            Onchain Median Confirmation Time (min)  \n",
       "2015-01-01                                7.150000  \n",
       "2015-01-02                                     NaN  \n",
       "2015-01-03                                6.433333  \n",
       "2015-01-04                                     NaN  \n",
       "2015-01-05                                6.816667  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(output_path, index_col=0, parse_dates=True)\n",
    "\n",
    "# Print data by by column\n",
    "print(\"missing data by column: \")\n",
    "print(df.isna().sum(), \"\\n\")\n",
    "\n",
    "# Print duplicates by column\n",
    "print(\"duplicates by column: \")\n",
    "print(df.duplicated(subset=None, keep='first').sum())\n",
    "\n",
    "print(\"\\nduplicates: \")\n",
    "print(df[df.duplicated(keep='first')])\n",
    "\n",
    "#Print unique values by column\n",
    "print(\"\\nunique: \")\n",
    "print(df.nunique())\n",
    "\n",
    "# display first five entries\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears the \"on chain metrics\" are also missing data (every other day), it is here that we begin the process of \"cleaning\" the data, we'll start by remedying the missing entries with linear interpolation. This creating a smooth transition of values between the missing data in the dataset, crusially preserving the original data's time series characteristics. \n",
    "\n",
    "We would otherwise lose fidelity if the sequence of entries were to become incorrectly scaled by dropping empty rows, or unintentionally added new characteristics to the data by using averages to fill the empty rows. Using backward or forward fill would also be sub optimal as it duplicates values creating unrepresentative square wave patterns not typical of financial market data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interpolated dataset saved to e:\\Documents\\skool work\\Msc\\AI module\\Msc-COM7003-AI-Model\\dataset\\2015-2025_dataset_interpolated.csv\n",
      "Shape: (3685, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Global averaged stocks(USD)</th>\n",
       "      <th>Global averaged stocks (volume)</th>\n",
       "      <th>Currency US Dollar Index</th>\n",
       "      <th>Currency Gold Futures</th>\n",
       "      <th>Volatility_CBOE SKEW Index</th>\n",
       "      <th>Volatility_CBOE Volatility Index (VIX)</th>\n",
       "      <th>Volatility_Crude Oil Volatility Index (OVX)</th>\n",
       "      <th>Gold/BTC Ratio</th>\n",
       "      <th>BTC/USD</th>\n",
       "      <th>BTC Volume</th>\n",
       "      <th>S2F Model</th>\n",
       "      <th>Onchain Active Addresses</th>\n",
       "      <th>Onchain Transaction Count</th>\n",
       "      <th>Onchain Hash Rate (GH/s)</th>\n",
       "      <th>Onchain Mining Difficulty</th>\n",
       "      <th>Onchain Transaction Fees (BTC)</th>\n",
       "      <th>Onchain Median Confirmation Time (min)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>314.248993</td>\n",
       "      <td>8036550.0</td>\n",
       "      <td>415.891844</td>\n",
       "      <td>117529.0</td>\n",
       "      <td>59344.0</td>\n",
       "      <td>335365.290092</td>\n",
       "      <td>4.064096e+10</td>\n",
       "      <td>8.054165</td>\n",
       "      <td>7.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>11279.900146</td>\n",
       "      <td>755.710</td>\n",
       "      <td>91.080002</td>\n",
       "      <td>1186.000000</td>\n",
       "      <td>128.660004</td>\n",
       "      <td>17.790001</td>\n",
       "      <td>54.250000</td>\n",
       "      <td>3.764697</td>\n",
       "      <td>315.032013</td>\n",
       "      <td>7860650.0</td>\n",
       "      <td>416.317366</td>\n",
       "      <td>161727.5</td>\n",
       "      <td>70785.5</td>\n",
       "      <td>333345.017260</td>\n",
       "      <td>4.064096e+10</td>\n",
       "      <td>9.926289</td>\n",
       "      <td>6.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>11212.303467</td>\n",
       "      <td>822.245</td>\n",
       "      <td>91.180000</td>\n",
       "      <td>1191.966675</td>\n",
       "      <td>128.370003</td>\n",
       "      <td>18.500001</td>\n",
       "      <td>55.389999</td>\n",
       "      <td>3.971867</td>\n",
       "      <td>281.082001</td>\n",
       "      <td>33054400.0</td>\n",
       "      <td>416.729918</td>\n",
       "      <td>205926.0</td>\n",
       "      <td>82227.0</td>\n",
       "      <td>331324.744428</td>\n",
       "      <td>4.064096e+10</td>\n",
       "      <td>11.798413</td>\n",
       "      <td>6.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-04</th>\n",
       "      <td>11144.706787</td>\n",
       "      <td>888.780</td>\n",
       "      <td>91.279999</td>\n",
       "      <td>1197.933350</td>\n",
       "      <td>128.080002</td>\n",
       "      <td>19.210000</td>\n",
       "      <td>56.529999</td>\n",
       "      <td>4.179038</td>\n",
       "      <td>264.195007</td>\n",
       "      <td>55629100.0</td>\n",
       "      <td>417.156111</td>\n",
       "      <td>203269.0</td>\n",
       "      <td>88906.0</td>\n",
       "      <td>335365.290092</td>\n",
       "      <td>4.064096e+10</td>\n",
       "      <td>13.944884</td>\n",
       "      <td>6.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>11077.110107</td>\n",
       "      <td>955.315</td>\n",
       "      <td>91.379997</td>\n",
       "      <td>1203.900024</td>\n",
       "      <td>127.790001</td>\n",
       "      <td>19.920000</td>\n",
       "      <td>57.669998</td>\n",
       "      <td>4.386208</td>\n",
       "      <td>274.473999</td>\n",
       "      <td>43962800.0</td>\n",
       "      <td>417.583604</td>\n",
       "      <td>200612.0</td>\n",
       "      <td>95585.0</td>\n",
       "      <td>339405.835756</td>\n",
       "      <td>4.064096e+10</td>\n",
       "      <td>16.091355</td>\n",
       "      <td>6.816667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Global averaged stocks(USD)  Global averaged stocks (volume)  \\\n",
       "2015-01-01                          NaN                              NaN   \n",
       "2015-01-02                 11279.900146                          755.710   \n",
       "2015-01-03                 11212.303467                          822.245   \n",
       "2015-01-04                 11144.706787                          888.780   \n",
       "2015-01-05                 11077.110107                          955.315   \n",
       "\n",
       "            Currency US Dollar Index  Currency Gold Futures  \\\n",
       "2015-01-01                       NaN                    NaN   \n",
       "2015-01-02                 91.080002            1186.000000   \n",
       "2015-01-03                 91.180000            1191.966675   \n",
       "2015-01-04                 91.279999            1197.933350   \n",
       "2015-01-05                 91.379997            1203.900024   \n",
       "\n",
       "            Volatility_CBOE SKEW Index  \\\n",
       "2015-01-01                         NaN   \n",
       "2015-01-02                  128.660004   \n",
       "2015-01-03                  128.370003   \n",
       "2015-01-04                  128.080002   \n",
       "2015-01-05                  127.790001   \n",
       "\n",
       "            Volatility_CBOE Volatility Index (VIX)  \\\n",
       "2015-01-01                                     NaN   \n",
       "2015-01-02                               17.790001   \n",
       "2015-01-03                               18.500001   \n",
       "2015-01-04                               19.210000   \n",
       "2015-01-05                               19.920000   \n",
       "\n",
       "            Volatility_Crude Oil Volatility Index (OVX)  Gold/BTC Ratio  \\\n",
       "2015-01-01                                          NaN             NaN   \n",
       "2015-01-02                                    54.250000        3.764697   \n",
       "2015-01-03                                    55.389999        3.971867   \n",
       "2015-01-04                                    56.529999        4.179038   \n",
       "2015-01-05                                    57.669998        4.386208   \n",
       "\n",
       "               BTC/USD  BTC Volume   S2F Model  Onchain Active Addresses  \\\n",
       "2015-01-01  314.248993   8036550.0  415.891844                  117529.0   \n",
       "2015-01-02  315.032013   7860650.0  416.317366                  161727.5   \n",
       "2015-01-03  281.082001  33054400.0  416.729918                  205926.0   \n",
       "2015-01-04  264.195007  55629100.0  417.156111                  203269.0   \n",
       "2015-01-05  274.473999  43962800.0  417.583604                  200612.0   \n",
       "\n",
       "            Onchain Transaction Count  Onchain Hash Rate (GH/s)  \\\n",
       "2015-01-01                    59344.0             335365.290092   \n",
       "2015-01-02                    70785.5             333345.017260   \n",
       "2015-01-03                    82227.0             331324.744428   \n",
       "2015-01-04                    88906.0             335365.290092   \n",
       "2015-01-05                    95585.0             339405.835756   \n",
       "\n",
       "            Onchain Mining Difficulty  Onchain Transaction Fees (BTC)  \\\n",
       "2015-01-01               4.064096e+10                        8.054165   \n",
       "2015-01-02               4.064096e+10                        9.926289   \n",
       "2015-01-03               4.064096e+10                       11.798413   \n",
       "2015-01-04               4.064096e+10                       13.944884   \n",
       "2015-01-05               4.064096e+10                       16.091355   \n",
       "\n",
       "            Onchain Median Confirmation Time (min)  \n",
       "2015-01-01                                7.150000  \n",
       "2015-01-02                                6.791667  \n",
       "2015-01-03                                6.433333  \n",
       "2015-01-04                                6.625000  \n",
       "2015-01-05                                6.816667  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved dataset\n",
    "df = pd.read_csv(output_path, index_col=0, parse_dates=True)\n",
    "\n",
    "# Interpolate each column based on its data type\n",
    "for column in df.columns:\n",
    "    # For all other metrics (prices, volumes, etc), use linear interpolation\n",
    "    df[column] = df[column].interpolate(method='linear', limit=5)\n",
    "\n",
    "# Save the interpolated dataset with a new name\n",
    "interpolated_path = output_dir / \"2015-2025_dataset_interpolated.csv\"\n",
    "df.to_csv(interpolated_path)\n",
    "\n",
    "print(f\"\\nInterpolated dataset saved to {interpolated_path}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "# display first five entries, validation.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next important step is to process the data, we'll need to normalize it making it more machine readable and comparable between columns. We will also re run the interp to attempt to fill the missing values in the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interpolated dataset saved to e:\\Documents\\skool work\\Msc\\AI module\\Msc-COM7003-AI-Model\\dataset\\2015-2025_dataset_interpolated.csv\n",
      "Shape: (3685, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Global averaged stocks(USD)</th>\n",
       "      <th>Global averaged stocks (volume)</th>\n",
       "      <th>Currency US Dollar Index</th>\n",
       "      <th>Currency Gold Futures</th>\n",
       "      <th>Volatility_CBOE SKEW Index</th>\n",
       "      <th>Volatility_CBOE Volatility Index (VIX)</th>\n",
       "      <th>Volatility_Crude Oil Volatility Index (OVX)</th>\n",
       "      <th>Gold/BTC Ratio</th>\n",
       "      <th>BTC/USD</th>\n",
       "      <th>BTC Volume</th>\n",
       "      <th>S2F Model</th>\n",
       "      <th>Onchain Active Addresses</th>\n",
       "      <th>Onchain Transaction Count</th>\n",
       "      <th>Onchain Hash Rate (GH/s)</th>\n",
       "      <th>Onchain Mining Difficulty</th>\n",
       "      <th>Onchain Transaction Fees (BTC)</th>\n",
       "      <th>Onchain Median Confirmation Time (min)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>0.058738</td>\n",
       "      <td>0.057694</td>\n",
       "      <td>0.097571</td>\n",
       "      <td>0.076289</td>\n",
       "      <td>0.262652</td>\n",
       "      <td>0.117607</td>\n",
       "      <td>0.118422</td>\n",
       "      <td>0.541534</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>5.011966e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002585</td>\n",
       "      <td>0.173198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>0.058738</td>\n",
       "      <td>0.057694</td>\n",
       "      <td>0.097571</td>\n",
       "      <td>0.076289</td>\n",
       "      <td>0.262652</td>\n",
       "      <td>0.117607</td>\n",
       "      <td>0.118422</td>\n",
       "      <td>0.541534</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.717128e-07</td>\n",
       "      <td>0.046265</td>\n",
       "      <td>0.013187</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.159608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>0.055722</td>\n",
       "      <td>0.067741</td>\n",
       "      <td>0.101489</td>\n",
       "      <td>0.079656</td>\n",
       "      <td>0.258495</td>\n",
       "      <td>0.127260</td>\n",
       "      <td>0.122132</td>\n",
       "      <td>0.571533</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>7.178523e-05</td>\n",
       "      <td>5.351443e-07</td>\n",
       "      <td>0.092530</td>\n",
       "      <td>0.026373</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.146018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-04</th>\n",
       "      <td>0.052706</td>\n",
       "      <td>0.077787</td>\n",
       "      <td>0.105408</td>\n",
       "      <td>0.083023</td>\n",
       "      <td>0.254337</td>\n",
       "      <td>0.136914</td>\n",
       "      <td>0.125842</td>\n",
       "      <td>0.601532</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>1.361079e-04</td>\n",
       "      <td>8.072860e-07</td>\n",
       "      <td>0.089749</td>\n",
       "      <td>0.034071</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006534</td>\n",
       "      <td>0.153287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>0.049690</td>\n",
       "      <td>0.087833</td>\n",
       "      <td>0.109326</td>\n",
       "      <td>0.086390</td>\n",
       "      <td>0.250179</td>\n",
       "      <td>0.146567</td>\n",
       "      <td>0.129552</td>\n",
       "      <td>0.631531</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>1.028668e-04</td>\n",
       "      <td>1.080257e-06</td>\n",
       "      <td>0.086968</td>\n",
       "      <td>0.041768</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007973</td>\n",
       "      <td>0.160556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Global averaged stocks(USD)  Global averaged stocks (volume)  \\\n",
       "2015-01-01                     0.058738                         0.057694   \n",
       "2015-01-02                     0.058738                         0.057694   \n",
       "2015-01-03                     0.055722                         0.067741   \n",
       "2015-01-04                     0.052706                         0.077787   \n",
       "2015-01-05                     0.049690                         0.087833   \n",
       "\n",
       "            Currency US Dollar Index  Currency Gold Futures  \\\n",
       "2015-01-01                  0.097571               0.076289   \n",
       "2015-01-02                  0.097571               0.076289   \n",
       "2015-01-03                  0.101489               0.079656   \n",
       "2015-01-04                  0.105408               0.083023   \n",
       "2015-01-05                  0.109326               0.086390   \n",
       "\n",
       "            Volatility_CBOE SKEW Index  \\\n",
       "2015-01-01                    0.262652   \n",
       "2015-01-02                    0.262652   \n",
       "2015-01-03                    0.258495   \n",
       "2015-01-04                    0.254337   \n",
       "2015-01-05                    0.250179   \n",
       "\n",
       "            Volatility_CBOE Volatility Index (VIX)  \\\n",
       "2015-01-01                                0.117607   \n",
       "2015-01-02                                0.117607   \n",
       "2015-01-03                                0.127260   \n",
       "2015-01-04                                0.136914   \n",
       "2015-01-05                                0.146567   \n",
       "\n",
       "            Volatility_Crude Oil Volatility Index (OVX)  Gold/BTC Ratio  \\\n",
       "2015-01-01                                     0.118422        0.541534   \n",
       "2015-01-02                                     0.118422        0.541534   \n",
       "2015-01-03                                     0.122132        0.571533   \n",
       "2015-01-04                                     0.125842        0.601532   \n",
       "2015-01-05                                     0.129552        0.631531   \n",
       "\n",
       "             BTC/USD    BTC Volume     S2F Model  Onchain Active Addresses  \\\n",
       "2015-01-01  0.001285  5.011966e-07  0.000000e+00                  0.000000   \n",
       "2015-01-02  0.001292  0.000000e+00  2.717128e-07                  0.046265   \n",
       "2015-01-03  0.000972  7.178523e-05  5.351443e-07                  0.092530   \n",
       "2015-01-04  0.000812  1.361079e-04  8.072860e-07                  0.089749   \n",
       "2015-01-05  0.000909  1.028668e-04  1.080257e-06                  0.086968   \n",
       "\n",
       "            Onchain Transaction Count  Onchain Hash Rate (GH/s)  \\\n",
       "2015-01-01                   0.000000                  0.000107   \n",
       "2015-01-02                   0.013187                  0.000105   \n",
       "2015-01-03                   0.026373                  0.000103   \n",
       "2015-01-04                   0.034071                  0.000107   \n",
       "2015-01-05                   0.041768                  0.000111   \n",
       "\n",
       "            Onchain Mining Difficulty  Onchain Transaction Fees (BTC)  \\\n",
       "2015-01-01                        0.0                        0.002585   \n",
       "2015-01-02                        0.0                        0.003840   \n",
       "2015-01-03                        0.0                        0.005095   \n",
       "2015-01-04                        0.0                        0.006534   \n",
       "2015-01-05                        0.0                        0.007973   \n",
       "\n",
       "            Onchain Median Confirmation Time (min)  \n",
       "2015-01-01                                0.173198  \n",
       "2015-01-02                                0.159608  \n",
       "2015-01-03                                0.146018  \n",
       "2015-01-04                                0.153287  \n",
       "2015-01-05                                0.160556  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(output_dir / \"2015-2025_dataset_interpolated.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Create a copy of the dataframe and normalize all columns\n",
    "df_normalized = pd.DataFrame(\n",
    "    scaler.fit_transform(df),\n",
    "    columns=df.columns,\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "# Save as new\n",
    "df_normalized.to_csv(output_dir / \"2015-2025_dataset_normalized.csv\")\n",
    "\n",
    "# display first five entries, validation.\n",
    "df_normalized.head()\n",
    "\n",
    "df = pd.read_csv(output_dir / \"2015-2025_dataset_normalized.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Interpolate each column based on its data type\n",
    "for column in df.columns:\n",
    "    # For all other metrics (prices, volumes, etc), use linear interpolation\n",
    "    df[column] = df[column].interpolate(method='linear', limit=20, limit_direction='backward')\n",
    "\n",
    "\n",
    "# Save the interpolated dataset with a new name\n",
    "df_re_norm = output_dir / \"2015-2025_dataset_normalized.csv\"\n",
    "\n",
    "df.to_csv(df_re_norm)\n",
    "\n",
    "print(f\"\\nInterpolated dataset saved to {interpolated_path}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "# display first five entries, validation.\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately there are still missing values in \"on chain mining dificulty\" so we'll resort to back and farward filling any remaining missing values with \"bfill() and .ffill()\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Global averaged stocks(USD)</th>\n",
       "      <th>Global averaged stocks (volume)</th>\n",
       "      <th>Currency US Dollar Index</th>\n",
       "      <th>Currency Gold Futures</th>\n",
       "      <th>Volatility_CBOE SKEW Index</th>\n",
       "      <th>Volatility_CBOE Volatility Index (VIX)</th>\n",
       "      <th>Volatility_Crude Oil Volatility Index (OVX)</th>\n",
       "      <th>Gold/BTC Ratio</th>\n",
       "      <th>BTC/USD</th>\n",
       "      <th>BTC Volume</th>\n",
       "      <th>S2F Model</th>\n",
       "      <th>Onchain Active Addresses</th>\n",
       "      <th>Onchain Transaction Count</th>\n",
       "      <th>Onchain Hash Rate (GH/s)</th>\n",
       "      <th>Onchain Mining Difficulty</th>\n",
       "      <th>Onchain Transaction Fees (BTC)</th>\n",
       "      <th>Onchain Median Confirmation Time (min)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>0.058738</td>\n",
       "      <td>0.057694</td>\n",
       "      <td>0.097571</td>\n",
       "      <td>0.076289</td>\n",
       "      <td>0.262652</td>\n",
       "      <td>0.117607</td>\n",
       "      <td>0.118422</td>\n",
       "      <td>0.541534</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>5.011966e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002585</td>\n",
       "      <td>0.173198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>0.058738</td>\n",
       "      <td>0.057694</td>\n",
       "      <td>0.097571</td>\n",
       "      <td>0.076289</td>\n",
       "      <td>0.262652</td>\n",
       "      <td>0.117607</td>\n",
       "      <td>0.118422</td>\n",
       "      <td>0.541534</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.717128e-07</td>\n",
       "      <td>0.046265</td>\n",
       "      <td>0.013187</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.159608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>0.055722</td>\n",
       "      <td>0.067741</td>\n",
       "      <td>0.101489</td>\n",
       "      <td>0.079656</td>\n",
       "      <td>0.258495</td>\n",
       "      <td>0.127260</td>\n",
       "      <td>0.122132</td>\n",
       "      <td>0.571533</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>7.178523e-05</td>\n",
       "      <td>5.351443e-07</td>\n",
       "      <td>0.092530</td>\n",
       "      <td>0.026373</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.146018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-04</th>\n",
       "      <td>0.052706</td>\n",
       "      <td>0.077787</td>\n",
       "      <td>0.105408</td>\n",
       "      <td>0.083023</td>\n",
       "      <td>0.254337</td>\n",
       "      <td>0.136914</td>\n",
       "      <td>0.125842</td>\n",
       "      <td>0.601532</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>1.361079e-04</td>\n",
       "      <td>8.072860e-07</td>\n",
       "      <td>0.089749</td>\n",
       "      <td>0.034071</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006534</td>\n",
       "      <td>0.153287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>0.049690</td>\n",
       "      <td>0.087833</td>\n",
       "      <td>0.109326</td>\n",
       "      <td>0.086390</td>\n",
       "      <td>0.250179</td>\n",
       "      <td>0.146567</td>\n",
       "      <td>0.129552</td>\n",
       "      <td>0.631531</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>1.028668e-04</td>\n",
       "      <td>1.080257e-06</td>\n",
       "      <td>0.086968</td>\n",
       "      <td>0.041768</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007973</td>\n",
       "      <td>0.160556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Global averaged stocks(USD)  Global averaged stocks (volume)  \\\n",
       "2015-01-01                     0.058738                         0.057694   \n",
       "2015-01-02                     0.058738                         0.057694   \n",
       "2015-01-03                     0.055722                         0.067741   \n",
       "2015-01-04                     0.052706                         0.077787   \n",
       "2015-01-05                     0.049690                         0.087833   \n",
       "\n",
       "            Currency US Dollar Index  Currency Gold Futures  \\\n",
       "2015-01-01                  0.097571               0.076289   \n",
       "2015-01-02                  0.097571               0.076289   \n",
       "2015-01-03                  0.101489               0.079656   \n",
       "2015-01-04                  0.105408               0.083023   \n",
       "2015-01-05                  0.109326               0.086390   \n",
       "\n",
       "            Volatility_CBOE SKEW Index  \\\n",
       "2015-01-01                    0.262652   \n",
       "2015-01-02                    0.262652   \n",
       "2015-01-03                    0.258495   \n",
       "2015-01-04                    0.254337   \n",
       "2015-01-05                    0.250179   \n",
       "\n",
       "            Volatility_CBOE Volatility Index (VIX)  \\\n",
       "2015-01-01                                0.117607   \n",
       "2015-01-02                                0.117607   \n",
       "2015-01-03                                0.127260   \n",
       "2015-01-04                                0.136914   \n",
       "2015-01-05                                0.146567   \n",
       "\n",
       "            Volatility_Crude Oil Volatility Index (OVX)  Gold/BTC Ratio  \\\n",
       "2015-01-01                                     0.118422        0.541534   \n",
       "2015-01-02                                     0.118422        0.541534   \n",
       "2015-01-03                                     0.122132        0.571533   \n",
       "2015-01-04                                     0.125842        0.601532   \n",
       "2015-01-05                                     0.129552        0.631531   \n",
       "\n",
       "             BTC/USD    BTC Volume     S2F Model  Onchain Active Addresses  \\\n",
       "2015-01-01  0.001285  5.011966e-07  0.000000e+00                  0.000000   \n",
       "2015-01-02  0.001292  0.000000e+00  2.717128e-07                  0.046265   \n",
       "2015-01-03  0.000972  7.178523e-05  5.351443e-07                  0.092530   \n",
       "2015-01-04  0.000812  1.361079e-04  8.072860e-07                  0.089749   \n",
       "2015-01-05  0.000909  1.028668e-04  1.080257e-06                  0.086968   \n",
       "\n",
       "            Onchain Transaction Count  Onchain Hash Rate (GH/s)  \\\n",
       "2015-01-01                   0.000000                  0.000107   \n",
       "2015-01-02                   0.013187                  0.000105   \n",
       "2015-01-03                   0.026373                  0.000103   \n",
       "2015-01-04                   0.034071                  0.000107   \n",
       "2015-01-05                   0.041768                  0.000111   \n",
       "\n",
       "            Onchain Mining Difficulty  Onchain Transaction Fees (BTC)  \\\n",
       "2015-01-01                        0.0                        0.002585   \n",
       "2015-01-02                        0.0                        0.003840   \n",
       "2015-01-03                        0.0                        0.005095   \n",
       "2015-01-04                        0.0                        0.006534   \n",
       "2015-01-05                        0.0                        0.007973   \n",
       "\n",
       "            Onchain Median Confirmation Time (min)  \n",
       "2015-01-01                                0.173198  \n",
       "2015-01-02                                0.159608  \n",
       "2015-01-03                                0.146018  \n",
       "2015-01-04                                0.153287  \n",
       "2015-01-05                                0.160556  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(output_dir / \"2015-2025_dataset_normalized.csv\", index_col=0, parse_dates=True)\n",
    "df = df.bfill().ffill() # back and forward fill.\n",
    "\n",
    "df_save = output_dir / \"2015-2025_dataset_normalized.csv\"\n",
    "# save\n",
    "df.to_csv(df_save)\n",
    "# validate\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common plotting function for time series visualization\n",
    "def plot_time_series(df, title, ax, color='blue', alpha=0.8, linewidth=1.5):\n",
    "    \"\"\"\n",
    "    Reusable function to plot time series data with consistent styling\n",
    "    \"\"\"\n",
    "    # Plot the time series\n",
    "    ax.plot(df.index, df, color=color, alpha=alpha, linewidth=linewidth)\n",
    "    \n",
    "    # Set title and labels\n",
    "    ax.set_title(title, fontsize=11)\n",
    "    ax.set_ylabel('Value', fontsize=9)\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Explicitly set x-axis limits to ensure full timeline is used\n",
    "    ax.set_xlim(df.index.min(), df.index.max())\n",
    "    \n",
    "    return ax\n",
    "\n",
    "def format_time_axis(ax, is_last=False):\n",
    "    \"\"\"\n",
    "    Reusable function to format time axis with consistent styling\n",
    "    \"\"\"\n",
    "    if not is_last:\n",
    "        ax.set_xlabel('')\n",
    "        plt.setp(ax.get_xticklabels(), visible=False)\n",
    "    else:\n",
    "        ax.set_xlabel('Date', fontsize=10)\n",
    "        # Set fewer x-ticks for better readability\n",
    "        locator = mdates.YearLocator()  # Show only years\n",
    "        ax.xaxis.set_major_locator(locator)\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "        \n",
    "        # Add minor ticks for quarters without labels\n",
    "        minor_locator = mdates.MonthLocator(bymonth=[1, 4, 7, 10])\n",
    "        ax.xaxis.set_minor_locator(minor_locator)\n",
    "        \n",
    "        # Rotate labels for better readability\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_comparison(x, y1, y2, title, ax, labels=None):\n",
    "    if labels is None:\n",
    "        labels = ['Series 1', 'Series 2']\n",
    "    \n",
    "    ax.plot(x, y1, color='#1f77b4', linewidth=1.5, label=labels[0])\n",
    "    ax.plot(x, y2, color='#ff7f0e', linewidth=1.5, linestyle='--', label=labels[1])\n",
    "    \n",
    "    ax.set_title(title, fontsize=11)\n",
    "    ax.set_xlabel('Sample Index', fontsize=9)\n",
    "    ax.set_ylabel('Value', fontsize=9)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bar_plot(ax, categories, values, title, ylabel, color='skyblue', percentage=True):\n",
    "    bars = ax.bar(categories, values, color=color)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if percentage:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{height:.2%}', ha='center', va='bottom', fontsize=9)\n",
    "        else:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{height:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    ax.set_title(title, fontsize=11)\n",
    "    ax.set_ylabel(ylabel, fontsize=9)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_bar_plot(labels, values, title, ylabel, colors=None, baseline=None, annotations=None, legend_elements=None):\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    # Use default color if none provided\n",
    "    if colors is None:\n",
    "        colors = ['#1f77b4'] * len(labels)\n",
    "    \n",
    "    # Create bars\n",
    "    bars = plt.bar(labels, values, color=colors, alpha=0.7)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.05 * max(values),\n",
    "                f'{height:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.ylabel(ylabel, fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add a horizontal baseline if specified\n",
    "    if baseline is not None:\n",
    "        plt.axhline(y=baseline, color='#d62728', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add annotations if provided\n",
    "    if annotations:\n",
    "        for i, (text, position) in enumerate(annotations):\n",
    "            plt.annotate(text, \n",
    "                        xy=position, xycoords='axes fraction',\n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "                        fontsize=10)\n",
    "    \n",
    "    # Add legend if provided\n",
    "    if legend_elements:\n",
    "        plt.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv(output_dir / \"2015-2025_dataset_normalized.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# 1. Correlation Matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.set(font_scale=0.8)\n",
    "corr = df.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "            square=True, linewidths=.5, annot=True, fmt=\".2f\", annot_kws={\"size\": 7})\n",
    "plt.title('Correlation Matrix of Features', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"charts\" / \"data_visualizations\" / \"correlation_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2. Missing Value Heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.isna(), cmap='viridis', cbar_kws={'label': 'Missing Values'})\n",
    "plt.title('Missing Values Heatmap', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"charts\" / \"data_visualizations\" / \"missing_values_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 3. Distribution and Outliers - Box and Violin Plots\n",
    "n_cols = len(df.columns)\n",
    "n_rows = (n_cols + 2) // 3  # Arrange in rows of 3 plots\n",
    "\n",
    "fig = plt.figure(figsize=(18, n_rows * 5))\n",
    "gs = GridSpec(n_rows, 3, figure=fig)\n",
    "\n",
    "for i, column in enumerate(df.columns):\n",
    "    row, col = divmod(i, 3)\n",
    "    ax = fig.add_subplot(gs[row, col])\n",
    "    \n",
    "    # Fix the warning by using color instead of palette\n",
    "    sns.violinplot(y=df[column], ax=ax, inner='box', color='skyblue')\n",
    "    ax.set_title(f'Distribution of {column}', fontsize=12)\n",
    "    ax.set_ylabel('Value')\n",
    "    \n",
    "    # Add scatter points for outliers\n",
    "    outliers = df[abs(df[column] - df[column].mean()) > (3 * df[column].std())]\n",
    "    if not outliers.empty:\n",
    "        sns.stripplot(y=outliers[column], ax=ax, color='red', size=4, jitter=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"charts\" / \"data_visualizations\" / \"distribution_outliers.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 4. Time Series Plots - Using the common functions\n",
    "fig, axes = plt.subplots(n_cols, 1, figsize=(15, n_cols * 2.5), sharex=True)\n",
    "\n",
    "# Handle case where there's only one column\n",
    "if n_cols == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, column in enumerate(df.columns):\n",
    "    # Use the common plot_time_series function\n",
    "    plot_time_series(df[column], f'Time Series: {column}', axes[i], \n",
    "                    color='#1f77b4', alpha=0.8, linewidth=1.5)\n",
    "    \n",
    "    # Use the common format_time_axis function\n",
    "    format_time_axis(axes[i], is_last=(i == n_cols-1))\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "# Use tight_layout with appropriate padding\n",
    "plt.tight_layout(pad=1.2)\n",
    "plt.savefig(output_dir / \"charts\" / \"data_visualizations\" / \"time_series.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 5. BONUS: Combined Pairplot for selected features\n",
    "if n_cols <= 10:  # Only do this if you have a reasonable number of columns\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.pairplot(df, diag_kind='kde', plot_kws={'alpha': 0.6, 's': 30, 'edgecolor': 'k'})\n",
    "    plt.suptitle('Pairwise Relationships Between Features', y=1.02, fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"charts\" / \"data_visualizations\" / \"pairplot.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    # If too many columns, select a subset based on correlation\n",
    "    # Get the most correlated features\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "    \n",
    "    # Keep correlated features and a few others\n",
    "    important_cols = list(set(df.columns) - set(to_drop))\n",
    "    if len(important_cols) > 8:\n",
    "        important_cols = important_cols[:8]\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.pairplot(df[important_cols], diag_kind='kde', \n",
    "                plot_kws={'alpha': 0.6, 's': 30, 'edgecolor': 'k'})\n",
    "    plt.suptitle('Pairwise Relationships Between Key Features', y=1.02, fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"charts\" / \"data_visualizations\" / \"pairplot_selected.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://dx.doi.org/10.3390/info12100388 - Method used for denoising [decription of how to do \"wavelet decomposition\" and/or \"wavelet\" denoising]\n",
    "\n",
    "Instead of handling outliers in a more traditional approach, since financial data is real world data and I am not versed enough in finance and economics to understand fully what kind of data could or should classify as \"outliers\" with much confidence. I instead would prefer to try \"denoising\" from the literacture I've found on the similar projects.\n",
    "\n",
    "\"Wavelet transforms analyse stock market trends over different periods and often show superior performance. Peng et al. (2021) demonstrated that combining multiresolution wavelet reconstruction with deep learning significantly improves medium-term stock prediction accuracy, achieving a 75% hit rate for US stocks. Another study introduced the Adaptive Multi-Scale Wavelet Neural Network (AMSW-NN), which performs well but depends on dataset quality (Ouyang et al., 2021).\" - https://arxiv.org/html/2408.12408v1#S3.SS3 TL;DR \"multiresolution wavelet reconstruction\"  is very good and preferable over \"adaptive Multi-Scale Wavelet Neural Network (AMSW-NN)\" due to its increased dependance on quality data. - \"multiresolution wavelet\" method explained in greater detail here: Peng et al. (2021) [https://www.mdpi.com/2078-2489/12/10/388] - only had a 0.63% improvement with much greater complexity, Best to keep things simple for both my sanity in programming and the \"computational efficienty\" of Pan Tang, Cheng Tang and Keren Wang's [https://doi.org/10.1002/for.3071] apporach:\n",
    "\n",
    "\n",
    "\"LSTM (long short-term memory), we propose a hybrid model of wavelet transform (WT) and multi-input LSTM\"\n",
    "\n",
    "LSTM + WT = flexible model.\n",
    "\n",
    "\"level 1 decomposition with db4 mother wavelet to eliminate noise. originall used in image processing. it is more widely\n",
    "used in stock price forecasting (Aussem, 1998; Alru-maih & Al-Fawzan, 2002; Caetano & Yoneyama, 2007; Huang, 2011)\"\n",
    "\n",
    "y[n] =  x[k]g[2n-k]  # Low-pass filter\n",
    "y[n] =  x[k]h[2n-k]  # High-pass filter\n",
    "\n",
    "\"db4\" stands for \"Daubechies-4\" wavelet\n",
    "It's called a \"mother\" wavelet because it's the original pattern that gets scaled and shifted\n",
    "The \"4\" represents the number of vanishing moments (a measure of complexity)\n",
    "\n",
    "Tang's Approach (2024):\n",
    "Simple level 1 decomposition with db4\n",
    "Complete zeroing of high-frequency coefficients\n",
    "Claimed Results:\n",
    "> Test accuracy increased from 51.72% - 57.76% to 64.66% - 72.19% after applying their denoising method\n",
    "> Focused on LSTM model performance improvement\n",
    "Lpez Gil/Peng's Approach (2021, 2024):\n",
    "Multi-level (3-level) decomposition with db4\n",
    "Adaptive thresholding at each level\n",
    "Lpez Gil's Results:\n",
    "> Achieved 72.82% test accuracy and 73.16% F1 score on the EWZ daily dataset\n",
    "Peng's Original Results:\n",
    "> Achieved 75% hit rate for US stocks\n",
    "\n",
    "The following denoising code was block was largely assisted by deepseek-r1, based on Peng's paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wavelet denoising function\n",
    "def wavelet_denoising(df, wavelet='db4', level=3):\n",
    "    \"\"\"\n",
    "    Apply wavelet denoising to all columns in a DataFrame\n",
    "    \"\"\"\n",
    "    df_denoised = df.copy()\n",
    "    \n",
    "    for column in df.columns:\n",
    "        # 1. Multi-level decomposition\n",
    "        coeffs = pywt.wavedec(df[column].values, wavelet, level=level)\n",
    "        \n",
    "        # 2. Calculate noise threshold (Lpez Gil's method)\n",
    "        sigma = mad(coeffs[-1])\n",
    "        n = len(df[column])\n",
    "        threshold = sigma * np.sqrt(2 * np.log(n)) * 0.8  # Conservative thresholding\n",
    "        \n",
    "        # 3. Apply soft thresholding\n",
    "        coeffs_modified = [coeffs[0]]\n",
    "        for i in range(1, len(coeffs)):\n",
    "            coeffs_modified.append(pywt.threshold(coeffs[i], threshold, 'soft'))\n",
    "        \n",
    "        # 4. Reconstruct signal\n",
    "        denoised_data = pywt.waverec(coeffs_modified, wavelet)\n",
    "        \n",
    "        # 5. Handle boundary effects\n",
    "        if len(denoised_data) > len(df):\n",
    "            denoised_data = denoised_data[:len(df)]\n",
    "        elif len(denoised_data) < len(df):\n",
    "            denoised_data = np.pad(denoised_data, (0, len(df)-len(denoised_data)), 'edge')\n",
    "            \n",
    "        df_denoised[column] = denoised_data\n",
    "    \n",
    "    return df_denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_denoising_results(original_data, denoised_data, column_name):\n",
    "    \"\"\"\n",
    "    Plot denoising results with consistent styling matching the time series plots\n",
    "    \"\"\"\n",
    "    noise = original_data[column_name] - denoised_data[column_name]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 8), sharex=True)\n",
    "    \n",
    "    # Original and denoised data\n",
    "    plot_time_series(original_data[column_name], f'Denoising Results for {column_name}', \n",
    "                    axes[0], color='#1f77b4', alpha=0.5, linewidth=1.5)\n",
    "    plot_time_series(denoised_data[column_name], None, \n",
    "                    axes[0], color='#ff7f0e', alpha=0.8, linewidth=1.5)\n",
    "    axes[0].legend(['Original', 'Denoised'], loc='upper right', fontsize=9)\n",
    "    \n",
    "    # Removed noise\n",
    "    plot_time_series(noise, 'Removed Noise Component', \n",
    "                    axes[1], color='#d62728', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    # Format time axis\n",
    "    format_time_axis(axes[0], is_last=False)\n",
    "    format_time_axis(axes[1], is_last=True)\n",
    "    \n",
    "    # Adjust spacing\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    plt.tight_layout(pad=1.2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Dataset Summary Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Print missing value counts\n",
    "print(\"\\nMissing Values Count:\")\n",
    "print(df.isna().sum())\n",
    "print(f\"Total missing values: {df.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_denoised_columns(original_df, denoised_df):\n",
    "    \"\"\"\n",
    "    Plot all columns in a single figure with consistent styling,\n",
    "    including noise charts underneath each time series\n",
    "    \"\"\"\n",
    "    n_cols = len(original_df.columns)\n",
    "    # Create twice as many rows - one for the signal, one for the noise\n",
    "    fig, axes = plt.subplots(n_cols * 2, 1, figsize=(15, n_cols * 4), sharex=True)\n",
    "    \n",
    "    # Handle case where there's only one column\n",
    "    if n_cols == 1:\n",
    "        axes = np.array([axes[0], axes[1]])\n",
    "    \n",
    "    for i, column in enumerate(original_df.columns):\n",
    "        # Calculate the noise component\n",
    "        noise = original_df[column] - denoised_df[column]\n",
    "        \n",
    "        # Index for the signal plot\n",
    "        signal_idx = i * 2\n",
    "        # Index for the noise plot\n",
    "        noise_idx = i * 2 + 1\n",
    "        \n",
    "        # Plot original and denoised data on the signal plot\n",
    "        plot_time_series(original_df[column], f'Time Series: {column}', \n",
    "                        axes[signal_idx], color='#1f77b4', alpha=0.5, linewidth=1.5)\n",
    "        plot_time_series(denoised_df[column], None, \n",
    "                        axes[signal_idx], color='#ff7f0e', alpha=0.8, linewidth=1.5)\n",
    "        axes[signal_idx].legend(['Original', 'Denoised'], loc='upper right', fontsize=8)\n",
    "        \n",
    "        # Plot noise component on the noise plot\n",
    "        plot_time_series(noise, f'Noise Component: {column}', \n",
    "                        axes[noise_idx], color='#d62728', alpha=0.7, linewidth=1)\n",
    "        \n",
    "        # Format time axes\n",
    "        format_time_axis(axes[signal_idx], is_last=False)\n",
    "        format_time_axis(axes[noise_idx], is_last=(i == n_cols-1 and noise_idx == (n_cols*2)-1))\n",
    "        \n",
    "        # Adjust y-axis limits for noise plot to better visualize the noise\n",
    "        noise_std = noise.std()\n",
    "        noise_mean = noise.mean()\n",
    "        axes[noise_idx].set_ylim([noise_mean - 3*noise_std, noise_mean + 3*noise_std])\n",
    "        \n",
    "        # Add a horizontal line at y=0 for the noise plot\n",
    "        axes[noise_idx].axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Adjust spacing\n",
    "    plt.subplots_adjust(hspace=0.3)\n",
    "    plt.tight_layout(pad=1.2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Main execution code\n",
    "df = pd.read_csv(output_dir / \"2015-2025_dataset_normalized.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Apply wavelet denoising\n",
    "df_denoised = wavelet_denoising(df)\n",
    "\n",
    "# Plot individual denoising results for each column\n",
    "for column in df.columns:\n",
    "    fig = plot_denoising_results(df, df_denoised, column)\n",
    "    plt.savefig(output_dir / \"charts\" / \"denoising\" /  f\"denoising_{column.replace('/', '_')}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "# Plot all columns in a single figure with noise charts\n",
    "fig = plot_all_denoised_columns(df, df_denoised)\n",
    "plt.savefig(output_dir / \"charts\" / \"denoising\" /  \"all_denoised_columns_with_noise.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save the denoised dataset\n",
    "df_denoised.to_csv(output_dir / \"2015-2025_dataset_denoised.csv\")\n",
    "print(f\"Denoised dataset saved to {output_dir / '2015-2025_dataset_denoised.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for feature selection:\n",
    "\n",
    "[https://arxiv.org/pdf/2303.02223v2] - Pabuccu's - \"Feature Selection for Forecasting\" - Actually used \"FSA\" and \"boruta\" for validation: https://github.com/scikit-learn-contrib/boruta_py\n",
    "\n",
    "[https://doi.org/10.1002/for.3071] - Teng's did not feature select, he manually did it based on domain \"expertize\" - https://doi.org/10.1002/for.3071 - \"Stock movement prediction: A multi-input LSTM approach\"\n",
    "\n",
    "[https://www.mdpi.com/1999-4893/10/4/114] - tyralis's method - \"Variable Selection in Time Series Forecasting Using Random Forests\" - https://doi.org/10.3390/a10040114 - using both his prediction and feature selection method.\n",
    "\n",
    "LASSO feature selection method: https://www.tandfonline.com/doi/epdf/10.1080/09540091.2023.2286188?needAccess=true\n",
    "\n",
    "The original text block overestimated the mathematical parity of the implementations. This updated analysis provides a more accurate assessment of the actual implementation fidelity to the original papers.\n",
    "\n",
    "the code for this section, as well as help with understanding the underlying papers math, methods and processes were completed with the aid of both Clude sonnet 3.5 and deepseek r-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and plotting the feature selection of choice:\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
    "    return df\n",
    "\n",
    "# Use 'Score' column consistently - AI contributed:\n",
    "def plot_feature_importance(scores_df, method_name):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get selected features and their scores\n",
    "    selected_scores = scores_df[scores_df['Selected']]\n",
    "    score_col = 'Score' if 'Score' in selected_scores.columns else 'Importance'\n",
    "    \n",
    "    # Create horizontal bar plot\n",
    "    plt.barh(range(len(selected_scores)), selected_scores[score_col])\n",
    "    plt.yticks(range(len(selected_scores)), selected_scores['Feature'])\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title(f'Feature Importance Scores - {method_name}')\n",
    "    plt.grid(True, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def save_selected_features(df, selected_features, target, method_name):\n",
    "    final_features = selected_features + [target]\n",
    "    df_selected = df[final_features]\n",
    "    output_path = output_dir / f\"2015-2025_dataset_selected_features_{method_name}.csv\"\n",
    "    df_selected.to_csv(output_path)\n",
    "    return df_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forrest Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tyralis_selection(df, target='BTC/USD', window_size=30):\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    \n",
    "    # Paper's RF configuration\n",
    "    rf = RandomForestRegressor(n_estimators=50, #500\n",
    "                             max_features='sqrt',\n",
    "                             random_state=42) \n",
    "    \n",
    "    n_windows = len(X) - window_size\n",
    "    importance_matrix = np.zeros((n_windows, X.shape[1]))\n",
    "    \n",
    "    # Process windows sequentially as per paper\n",
    "    with tqdm(total=n_windows, desc='Processing windows') as pbar:\n",
    "        for i in range(n_windows):\n",
    "            # Paper's window approach\n",
    "            window_data = X.iloc[i:i + window_size]\n",
    "            window_target = y.iloc[i:i + window_size]\n",
    "            \n",
    "            # Fit RF on window\n",
    "            rf.fit(window_data, window_target)\n",
    "            baseline_score = rf.score(window_data, window_target)\n",
    "            \n",
    "            # Sequential permutation importance (paper's method)\n",
    "            for j in range(X.shape[1]):\n",
    "                X_perm = window_data.copy()\n",
    "                X_perm.iloc[:,j] = np.random.permutation(X_perm.iloc[:,j])\n",
    "                perm_score = rf.score(X_perm, window_target)\n",
    "                importance_matrix[i,j] = baseline_score - perm_score\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Paper's quantile threshold (75th percentile)\n",
    "    threshold = np.quantile(importance_matrix, 0.75, axis=0)\n",
    "    selected_features = X.columns[importance_matrix.mean(axis=0) > threshold].tolist()\n",
    "    \n",
    "    return selected_features, pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Score': importance_matrix.mean(axis=0),\n",
    "        'Selected': importance_matrix.mean(axis=0) > threshold\n",
    "    }).sort_values('Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_feature_selection(df, target='BTC/USD', alpha='auto'):\n",
    "    from sklearn.linear_model import LassoCV, Lasso\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Determine optimal alpha if auto\n",
    "    if alpha == 'auto':\n",
    "        lasso_cv = LassoCV(cv=5, random_state=42)\n",
    "        lasso_cv.fit(X_scaled, y)\n",
    "        alpha = lasso_cv.alpha_\n",
    "    \n",
    "    # Fit LASSO\n",
    "    lasso = Lasso(alpha=alpha, random_state=42)\n",
    "    lasso.fit(X_scaled, y)\n",
    "    \n",
    "    # Get selected features\n",
    "    selected_features = X.columns[lasso.coef_ != 0].tolist()\n",
    "    \n",
    "    # Create importance scores\n",
    "    importance_scores = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Score': np.abs(lasso.coef_),\n",
    "        'Selected': lasso.coef_ != 0\n",
    "    }).sort_values('Score', ascending=False)\n",
    "    \n",
    "    print(f\"\\nSelected {len(selected_features)} features\")\n",
    "    print(\"\\nTop features and weights:\")\n",
    "    print(importance_scores[importance_scores['Selected']].head())\n",
    "    \n",
    "    return selected_features, importance_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boruta's method: You can adjust the parameters like max_depth, n_estimators, and alpha based on your specific needs. The verbose=2 parameter will show you the progress of feature selection.\n",
    "\n",
    "Please suggest me code to replace this pabuccu selection function with the one mentioned in: @https://arxiv.org/pdf/2303.02223v2  They helpfully provided a github link to use someone else library, I would like to implement their method if possible (found here:@https://github.com/scikit-learn-contrib/boruta_py ). Thank you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boruta_selection(df, target='BTC/USD', max_iter=100, importance_threshold=0.01):\n",
    "    from boruta import BorutaPy\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from tqdm.notebook import tqdm\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "    \n",
    "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "    \n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    \n",
    "    rf = RandomForestRegressor(\n",
    "        n_jobs=-1,\n",
    "        max_depth=7,\n",
    "        n_estimators=250,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    boruta = BorutaPy(\n",
    "        rf,\n",
    "        n_estimators='auto',\n",
    "        max_iter=max_iter,\n",
    "        perc=98,\n",
    "        alpha=0.001,\n",
    "        two_step=True,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    with tqdm(total=1, desc=\"Running Boruta Selection\") as pbar:\n",
    "        boruta.fit(X.values, y.values)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    if hasattr(boruta, 'importance_history_'):\n",
    "        feature_importance = boruta.importance_history_.mean(axis=0)[:X.shape[1]]\n",
    "        scores = feature_importance / np.max(feature_importance)\n",
    "        \n",
    "        min_nonzero = np.min(feature_importance[feature_importance > 0])\n",
    "        relative_scores = np.zeros_like(feature_importance)\n",
    "        nonzero_mask = feature_importance > 0\n",
    "        relative_scores[nonzero_mask] = 10 * (np.log1p(feature_importance[nonzero_mask]) - np.log1p(min_nonzero)) / (np.log1p(np.max(feature_importance)) - np.log1p(min_nonzero))\n",
    "    else:\n",
    "        scores = np.ones(len(X.columns))\n",
    "        relative_scores = np.ones(len(X.columns))\n",
    "        feature_importance = np.ones(len(X.columns))\n",
    "    \n",
    "    significant_features = scores >= importance_threshold\n",
    "    \n",
    "    importance_scores = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': scores.round(4),\n",
    "        'Relative_Importance': relative_scores.round(2),\n",
    "        'Raw_Score': feature_importance.round(6),\n",
    "        'Selected': significant_features,\n",
    "        'Score': scores.round(4)  # Add Score column for compatibility\n",
    "    })\n",
    "    \n",
    "    importance_scores = importance_scores.sort_values('Importance', ascending=False)\n",
    "    selected_features = importance_scores[importance_scores['Selected']]['Feature'].tolist()\n",
    "    \n",
    "    print(f\"\\nSelected {len(selected_features)} features\")\n",
    "    print(\"\\nSelected features by importance:\")\n",
    "    print(importance_scores[importance_scores['Selected']].to_string(index=False))\n",
    "    \n",
    "    return selected_features, importance_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main execution block, run all three or only one of the feature at a time if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_feature_selection(method_choice):\n",
    "    df = pd.read_csv(output_dir / \"2015-2025_dataset_denoised.csv\", index_col=0, parse_dates=True)\n",
    "    target = 'BTC/USD'\n",
    "    \n",
    "    methods = {\n",
    "        'tyralis': {\n",
    "            'func': tyralis_selection,\n",
    "            'params': {'window_size': 30}\n",
    "        },\n",
    "        'lasso': {\n",
    "            'func': lasso_feature_selection,\n",
    "            'params': {}\n",
    "        },\n",
    "        'boruta': {\n",
    "            'func': boruta_selection,\n",
    "            'params': {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Execute selected method\n",
    "    if method_choice in ['tyralis', 'lasso', 'boruta']:\n",
    "        selected_features, importance_scores = methods[method_choice]['func'](\n",
    "            df, target, **methods[method_choice]['params']\n",
    "        )\n",
    "        \n",
    "        # Ensure consistent column naming\n",
    "        if 'Importance' in importance_scores.columns:\n",
    "            importance_scores['Score'] = importance_scores['Importance']\n",
    "        \n",
    "        # Save results and plot\n",
    "        shape = save_selected_features(df, selected_features, target, method_choice)\n",
    "        plot_feature_importance(importance_scores, method_choice)\n",
    "        \n",
    "        print(f\"\\nSelected {len(selected_features)} features using {method_choice.upper()} method\")\n",
    "        print(\"Features:\", ', '.join(selected_features))\n",
    "        print(f\"Output shape: {shape}\")\n",
    "\n",
    "#run_feature_selection('tyralis')\n",
    "#run_feature_selection('lasso')\n",
    "#run_feature_selection('boruta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is the model, 3 AI algorithms were chosem for this: \n",
    "\n",
    "1. Deep direct reinforcement learning:\n",
    "2. Random forrest:\n",
    "3. xLSTM-TS (Supervised Learning - Regression):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tyralis's model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tyralis_prediction_model(X_train, y_train, X_test=None, window_size=1):\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=1,  # Reduced for testing, 50 is needed for full run, with a window size of 30.\n",
    "        max_features='sqrt',\n",
    "        criterion='squared_error',  \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # If X_test is provided, we're doing out-of-sample prediction\n",
    "    if X_test is not None:\n",
    "        rf.fit(X_train, y_train)\n",
    "        predictions = rf.predict(X_test)\n",
    "        model = {'rf': rf}\n",
    "        return model, predictions\n",
    "    \n",
    "    # Otherwise, do rolling window prediction\n",
    "    predictions = []\n",
    "    feature_importance_history = []\n",
    "    \n",
    "    # Rolling window prediction\n",
    "    for i in range(len(X_train) - window_size):\n",
    "        # Get window data\n",
    "        X_window = X_train[i:i+window_size]\n",
    "        y_window = y_train[i:i+window_size]\n",
    "        \n",
    "        # Fit RF on current window\n",
    "        rf.fit(X_window, y_window)\n",
    "        \n",
    "        # Store feature importance for this window\n",
    "        feature_importance_history.append(rf.feature_importances_)\n",
    "        \n",
    "        # Predict next point\n",
    "        if i + window_size < len(X_train):\n",
    "            next_X = X_train[i+window_size].reshape(1, -1)\n",
    "            pred = rf.predict(next_X)[0]\n",
    "            predictions.append(pred)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    model = {\n",
    "        'rf': rf,\n",
    "        'feature_importance_history': np.array(feature_importance_history)\n",
    "    }\n",
    "    \n",
    "    return model, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lopez's model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientClippingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, max_norm=1.0):\n",
    "        super().__init__()\n",
    "        self.max_norm = max_norm\n",
    "        \n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        # Get the optimizer\n",
    "        optimizer = self.model.optimizer\n",
    "        \n",
    "        # Add gradient clipping to the optimizer\n",
    "        if not hasattr(optimizer, '_was_clipping_added'):\n",
    "            optimizer.clipnorm = self.max_norm\n",
    "            optimizer._was_clipping_added = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function combining MSE and directional accuracy as per paper\n",
    "    \"\"\"\n",
    "    # MSE component\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    \n",
    "    # Directional component\n",
    "    # Get directional movement (up/down) for true and predicted values\n",
    "    y_true_direction = tf.cast(y_true[:, 1:] > y_true[:, :-1], tf.float32)\n",
    "    y_pred_direction = tf.cast(y_pred[:, 1:] > y_pred[:, :-1], tf.float32)\n",
    "    \n",
    "    # Binary cross-entropy for directional accuracy\n",
    "    directional = tf.reduce_mean(tf.keras.losses.binary_crossentropy(\n",
    "        y_true_direction, \n",
    "        y_pred_direction,\n",
    "        from_logits=False\n",
    "    ))\n",
    "    \n",
    "    # Combine losses with weighting (=0.7 as per paper)\n",
    "    alpha = 0.7\n",
    "    return alpha * mse + (1 - alpha) * directional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sLSTM_block(inputs, embedding_dim=64, kernel_size=2, num_heads=2, ff_factor=1.1):\n",
    "    # Layer normalization\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    \n",
    "    # Scalar memory LSTM with convolutional processing\n",
    "    # Simulate sLSTM behavior using Conv1D + LSTM with different configuration\n",
    "    conv = Conv1D(filters=embedding_dim, kernel_size=kernel_size, padding='same')(x)\n",
    "    lstm = LSTM(embedding_dim, return_sequences=True, recurrent_activation='sigmoid')(conv)\n",
    "    \n",
    "    # Multi-head attention mechanism\n",
    "    attention = MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=embedding_dim // num_heads,\n",
    "        value_dim=embedding_dim // num_heads\n",
    "    )(lstm, lstm, lstm)\n",
    "    \n",
    "    # Feedforward network with projection factor\n",
    "    ff_dim = int(embedding_dim * ff_factor)\n",
    "    ff = Dense(ff_dim, activation='relu')(attention)\n",
    "    ff = Dense(embedding_dim)(ff)\n",
    "    \n",
    "    # Residual connection\n",
    "    output = Add()([inputs, ff])\n",
    "    \n",
    "    return output\n",
    "\n",
    "def create_mLSTM_block(inputs, embedding_dim=64, kernel_size=4, projection_size=2, num_heads=2):\n",
    "    # Layer normalization\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    \n",
    "    # Matrix memory LSTM with convolutional processing\n",
    "    # Simulate mLSTM behavior using Conv1D + LSTM\n",
    "    conv = Conv1D(filters=embedding_dim, kernel_size=kernel_size, padding='same')(x)\n",
    "    lstm = LSTM(embedding_dim, return_sequences=True)(conv)\n",
    "    \n",
    "    # Multi-head attention mechanism\n",
    "    attention = MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=embedding_dim // num_heads,\n",
    "        value_dim=embedding_dim // num_heads\n",
    "    )(lstm, lstm, lstm)\n",
    "    \n",
    "    # Projection to match dimensions\n",
    "    projection = Dense(embedding_dim * projection_size)(attention)\n",
    "    projection = Dense(embedding_dim)(projection)  # Project back to embedding_dim\n",
    "    \n",
    "    # Residual connection\n",
    "    output = Add()([inputs, projection])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xLSTM_TS_model(input_shape, embedding_dim=64, output_size=7):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Initial linear projection to embedding dimension\n",
    "    x = Dense(embedding_dim)(inputs)\n",
    "    \n",
    "    # xLSTM block stack as per Table IV in the paper\n",
    "    # mLSTM block 1\n",
    "    x = create_mLSTM_block(\n",
    "        x, \n",
    "        embedding_dim=embedding_dim,\n",
    "        kernel_size=4,\n",
    "        projection_size=2,\n",
    "        num_heads=2\n",
    "    )\n",
    "    \n",
    "    # sLSTM block\n",
    "    x = create_sLSTM_block(\n",
    "        x, \n",
    "        embedding_dim=embedding_dim,\n",
    "        kernel_size=2,\n",
    "        num_heads=2,\n",
    "        ff_factor=1.1\n",
    "    )\n",
    "    \n",
    "    # mLSTM block 2\n",
    "    x = create_mLSTM_block(\n",
    "        x, \n",
    "        embedding_dim=embedding_dim,\n",
    "        kernel_size=4,\n",
    "        projection_size=2,\n",
    "        num_heads=2\n",
    "    )\n",
    "    \n",
    "    # mLSTM block 3\n",
    "    x = create_mLSTM_block(\n",
    "        x, \n",
    "        embedding_dim=embedding_dim,\n",
    "        kernel_size=4,\n",
    "        projection_size=2,\n",
    "        num_heads=2\n",
    "    )\n",
    "    \n",
    "    # Final layer normalization\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    # Global average pooling to reduce sequence dimension\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Final linear projection to output size\n",
    "    outputs = Dense(output_size)(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Splitting Explanation\n",
    "Both models use a temporal split approach, but with different considerations:\n",
    "\n",
    "1. Lpez's Approach:\n",
    "   - Training: 70% of data\n",
    "   - Validation: 15% of data\n",
    "   - Testing: 15% of data\n",
    "   \n",
    "   Key Features:\n",
    "   - Maintains temporal order\n",
    "   - Uses sequence creation\n",
    "   - Includes validation set\n",
    "   - Scales data using training statistics only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, target_col='BTC/USD', train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Prepare data according to the paper's specifications\n",
    "    \"\"\"\n",
    "    # Calculate split points\n",
    "    total_rows = len(df)\n",
    "    train_size = int(total_rows * train_ratio)\n",
    "    val_size = int(total_rows * val_ratio)\n",
    "    \n",
    "    # Split maintaining temporal order\n",
    "    train_df = df[:train_size]\n",
    "    val_df = df[train_size:train_size + val_size]\n",
    "    test_df = df[train_size + val_size:]\n",
    "    \n",
    "    # Scale data using MinMaxScaler as per paper\n",
    "    feature_scaler = MinMaxScaler()  # Changed from StandardScaler\n",
    "    target_scaler = MinMaxScaler()   # Changed from StandardScaler\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_columns = [col for col in df.columns if col != target_col]\n",
    "    \n",
    "    # Prepare feature and target data\n",
    "    X_train = pd.DataFrame(\n",
    "        feature_scaler.fit_transform(train_df[feature_columns]),\n",
    "        columns=feature_columns,\n",
    "        index=train_df.index\n",
    "    )\n",
    "    y_train = pd.Series(\n",
    "        target_scaler.fit_transform(train_df[[target_col]]).ravel(),\n",
    "        index=train_df.index\n",
    "    )\n",
    "    \n",
    "    X_val = pd.DataFrame(\n",
    "        feature_scaler.transform(val_df[feature_columns]),\n",
    "        columns=feature_columns,\n",
    "        index=val_df.index\n",
    "    )\n",
    "    y_val = pd.Series(\n",
    "        target_scaler.transform(val_df[[target_col]]).ravel(),\n",
    "        index=val_df.index\n",
    "    )\n",
    "    \n",
    "    X_test = pd.DataFrame(\n",
    "        feature_scaler.transform(test_df[feature_columns]),\n",
    "        columns=feature_columns,\n",
    "        index=test_df.index\n",
    "    )\n",
    "    y_test = pd.Series(\n",
    "        target_scaler.transform(test_df[[target_col]]).ravel(),\n",
    "        index=test_df.index\n",
    "    )\n",
    "    \n",
    "    # Store original data for MASE calculation\n",
    "    y_train_original = train_df[target_col].values\n",
    "    original_test_actuals = test_df[target_col].values\n",
    "    \n",
    "    print(f\"Train size: {len(X_train)}, Validation size: {len(X_val)}, Test size: {len(X_test)}\")\n",
    "    \n",
    "    return {\n",
    "        'train': (X_train, y_train),\n",
    "        'val': (X_val, y_val),\n",
    "        'test': (X_test, y_test),\n",
    "        'scalers': {\n",
    "            'feature': feature_scaler,\n",
    "            'target': target_scaler\n",
    "        },\n",
    "        'original': {\n",
    "            'train': y_train_original,\n",
    "            'test': original_test_actuals\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_tyralis(data, sequence_length=10, forecast_horizon=1, progress_bar=True):\n",
    "    if isinstance(data, pd.Series):\n",
    "        # Convert Series to DataFrame for consistent handling\n",
    "        data = data.to_frame()\n",
    "    \n",
    "    n_samples = len(data) - sequence_length - forecast_horizon + 1\n",
    "    n_features = data.shape[1]\n",
    "    \n",
    "    # Initialize empty arrays\n",
    "    X = np.zeros((n_samples, sequence_length, n_features))\n",
    "    y = np.zeros((n_samples,))\n",
    "    \n",
    "    # Create progress bar if requested\n",
    "    if progress_bar:\n",
    "        iterator = tqdm(range(n_samples), desc=\"Creating sequences\")\n",
    "    else:\n",
    "        iterator = range(n_samples)\n",
    "    \n",
    "    # Create sequences\n",
    "    for i in iterator:\n",
    "        # Input sequence (lookback window)\n",
    "        X[i] = data.iloc[i:i+sequence_length].values\n",
    "        \n",
    "        # Target value (single step ahead as per Tyralis paper)\n",
    "        y[i] = data.iloc[i+sequence_length].values[0]  # Take first column if multiple features\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingProgressCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epochs = self.params['epochs']\n",
    "        self.steps = self.params['steps'] * self.epochs\n",
    "        self.progress_bar = tqdm(\n",
    "            total=self.steps,\n",
    "            desc=\"Training Progress\",\n",
    "            unit=\"batch\",\n",
    "            leave=True  # Ensure progress bar remains after completion\n",
    "        )\n",
    "        self.current_step = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.epochs_without_improvement = 0\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.current_step += 1\n",
    "        self.progress_bar.update(1)\n",
    "        \n",
    "        # Update progress bar with more detailed metrics\n",
    "        self.progress_bar.set_postfix({\n",
    "            'loss': f\"{logs['loss']:.4f}\",\n",
    "            'val_loss': f\"{logs.get('val_loss', 'N/A')}\",\n",
    "            'mae': f\"{logs['mae']:.4f}\",\n",
    "            'epoch': f\"{self.current_step // self.params['steps']}/{self.epochs}\"\n",
    "        })\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Monitor validation loss for early stopping\n",
    "        current_loss = logs.get('val_loss', logs['loss'])\n",
    "        if current_loss < self.best_loss:\n",
    "            self.best_loss = current_loss\n",
    "            self.epochs_without_improvement = 0\n",
    "        else:\n",
    "            self.epochs_without_improvement += 1\n",
    "            \n",
    "        # Update progress bar with epoch information\n",
    "        self.progress_bar.set_postfix({\n",
    "            'loss': f\"{logs['loss']:.4f}\",\n",
    "            'val_loss': f\"{logs.get('val_loss', 'N/A')}\",\n",
    "            'mae': f\"{logs['mae']:.4f}\",\n",
    "            'epoch': f\"{epoch + 1}/{self.epochs}\",\n",
    "            'no_improve': self.epochs_without_improvement\n",
    "        })\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_lopez_model(df, target_col='BTC/USD', sequence_length=30): # sequence length 150\n",
    "    print(\"\\nInitializing Lpez's xLSTM-TS model training...\")\n",
    "    \n",
    "    if df.empty:\n",
    "        raise ValueError(\"Input DataFrame is empty\")\n",
    "    \n",
    "    # Print debug info\n",
    "    print(f\"Input DataFrame shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check if we have enough features\n",
    "    if len(df.columns) <= 1:\n",
    "        # Create a simple feature from the target (e.g., previous value)\n",
    "        df['prev_price'] = df[target_col].shift(1)\n",
    "        df = df.dropna()  # Remove the first row which will have NaN\n",
    "        print(\"Added previous price as a feature\")\n",
    "    \n",
    "    # Calculate split points\n",
    "    total_rows = len(df)\n",
    "    train_size = int(total_rows * 0.7)\n",
    "    val_size = int(total_rows * 0.15)\n",
    "    \n",
    "    # Split maintaining temporal order\n",
    "    train_df = df[:train_size]\n",
    "    val_df = df[train_size:train_size + val_size]\n",
    "    test_df = df[train_size + val_size:]\n",
    "    \n",
    "    print(f\"Train shape: {train_df.shape}\")\n",
    "    \n",
    "    # Store original test values\n",
    "    original_test_actuals = test_df[target_col].values\n",
    "    \n",
    "    # Scale data with proper DataFrame handling\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_columns = [col for col in df.columns if col != target_col]\n",
    "    print(f\"Feature columns: {feature_columns}\")\n",
    "    \n",
    "    # Fit and transform training data\n",
    "    X_train = pd.DataFrame(\n",
    "        feature_scaler.fit_transform(train_df[feature_columns]),\n",
    "        columns=feature_columns,\n",
    "        index=train_df.index\n",
    "    )\n",
    "    y_train = pd.Series(\n",
    "        target_scaler.fit_transform(train_df[[target_col]]).ravel(),\n",
    "        index=train_df.index\n",
    "    )\n",
    "    \n",
    "    # Transform validation data\n",
    "    X_val = pd.DataFrame(\n",
    "        feature_scaler.transform(val_df[feature_columns]),\n",
    "        columns=feature_columns,\n",
    "        index=val_df.index\n",
    "    )\n",
    "    y_val = pd.Series(\n",
    "        target_scaler.transform(val_df[[target_col]]).ravel(),\n",
    "        index=val_df.index\n",
    "    )\n",
    "    \n",
    "    # Transform test data\n",
    "    X_test = pd.DataFrame(\n",
    "        feature_scaler.transform(test_df[feature_columns]),\n",
    "        columns=feature_columns,\n",
    "        index=test_df.index\n",
    "    )\n",
    "    y_test = pd.Series(\n",
    "        target_scaler.transform(test_df[[target_col]]).ravel(),\n",
    "        index=test_df.index\n",
    "    )\n",
    "    \n",
    "    # Store original training data for MASE calculation\n",
    "    y_train_original = train_df[target_col].values\n",
    "    \n",
    "    # Create sequences\n",
    "    print(\"\\nCreating sequences...\")\n",
    "    sequence_progress = tqdm(total=3, desc=\"Sequence preparation\")\n",
    "    \n",
    "    # Training sequences - using sequence_length=150 as per paper\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X_train) - sequence_length - 6):\n",
    "        X_seq.append(X_train.iloc[i:i + sequence_length].values)\n",
    "        y_seq.append(y_train.iloc[i + sequence_length:i + sequence_length + 7].values.reshape(-1))\n",
    "    X_train_seq, y_train_seq = np.array(X_seq), np.array(y_seq)\n",
    "    sequence_progress.update(1)\n",
    "    \n",
    "    # Validation sequences\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X_val) - sequence_length - 6):\n",
    "        X_seq.append(X_val.iloc[i:i + sequence_length].values)\n",
    "        y_seq.append(y_val.iloc[i + sequence_length:i + sequence_length + 7].values.reshape(-1))\n",
    "    X_val_seq, y_val_seq = np.array(X_seq), np.array(y_seq)\n",
    "    sequence_progress.update(1)\n",
    "    \n",
    "    # Test sequences\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X_test) - sequence_length - 6):\n",
    "        X_seq.append(X_test.iloc[i:i + sequence_length].values)\n",
    "        y_seq.append(y_test.iloc[i + sequence_length:i + sequence_length + 7].values.reshape(-1))\n",
    "    X_test_seq, y_test_seq = np.array(X_seq), np.array(y_seq)\n",
    "    sequence_progress.update(1)\n",
    "    sequence_progress.close()\n",
    "    \n",
    "    # Get number of features (excluding target column)\n",
    "    n_features = X_train.shape[1]  # Number of features after dropping target column\n",
    "    \n",
    "    # Initialize model with the paper's xLSTM-TS architecture\n",
    "    print(\"\\nInitializing model...\")\n",
    "    \n",
    "    # Create the model using our new implementation\n",
    "    model = create_xLSTM_TS_model(\n",
    "        input_shape=(sequence_length, n_features),\n",
    "        embedding_dim=64,  # As per paper\n",
    "        output_size=7      # 7-day forecast\n",
    "    )\n",
    "    \n",
    "    # Configure optimizer with paper's parameters\n",
    "    optimizer = Adam(\n",
    "        learning_rate=1e-4,  # As per paper\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-7\n",
    "    )\n",
    "    \n",
    "    # Compile model with directional loss\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=directional_loss,\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=30,  # As per paper\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=10,  # As per paper\n",
    "            min_lr=1e-6\n",
    "        ),\n",
    "        TrainingProgressCallback()\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTraining model...\")\n",
    "    # Train model with paper's parameters\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        epochs=2,         # 200 As per paper\n",
    "        batch_size=16,      # As per paper\n",
    "        callbacks=callbacks,\n",
    "        verbose=0,          # Disable default verbose output to use custom progress bar\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return model, history, (X_test_seq, y_test_seq), target_scaler, y_train_original, original_test_actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_tyralis_model(df, target_col='BTC/USD', sequence_length=10):\n",
    "    \"\"\"\n",
    "    Implementation of the Tyralis model as described in the paper:\n",
    "    \"Variable Selection in Time Series Forecasting Using Random Forests\"\n",
    "    \"\"\"\n",
    "    print(\"\\nInitializing Tyralis model training...\")\n",
    "    \n",
    "    if df.empty:\n",
    "        raise ValueError(\"Input DataFrame is empty\")\n",
    "    \n",
    "    # Print debug info\n",
    "    print(f\"Input DataFrame shape: {df.shape}\")\n",
    "    print(f\"Target column: {target_col}\")\n",
    "    \n",
    "    # Calculate split points\n",
    "    total_rows = len(df)\n",
    "    train_size = int(total_rows * 0.7)\n",
    "    val_size = int(total_rows * 0.15)\n",
    "    \n",
    "    # Split maintaining temporal order\n",
    "    train_df = df[:train_size]\n",
    "    test_df = df[train_size + val_size:]\n",
    "    \n",
    "    print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "    \n",
    "    # Store original test values for evaluation\n",
    "    original_test_actuals = test_df[target_col].values\n",
    "    \n",
    "    # Store original training data for MASE calculation\n",
    "    y_train_original = train_df[target_col].values\n",
    "    \n",
    "    # Scale data with MinMaxScaler as per Tyralis paper\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_columns = [col for col in df.columns if col != target_col]\n",
    "    print(f\"Number of feature columns: {len(feature_columns)}\")\n",
    "    print(f\"Feature columns: {feature_columns[:5]}{'...' if len(feature_columns) > 5 else ''}\")\n",
    "    \n",
    "    # Fit and transform training data\n",
    "    X_train = pd.DataFrame(\n",
    "        feature_scaler.fit_transform(train_df[feature_columns]),\n",
    "        columns=feature_columns,\n",
    "        index=train_df.index\n",
    "    )\n",
    "    y_train = pd.Series(\n",
    "        target_scaler.fit_transform(train_df[[target_col]]).ravel(),\n",
    "        index=train_df.index\n",
    "    )\n",
    "    \n",
    "    # Transform test data\n",
    "    X_test = pd.DataFrame(\n",
    "        feature_scaler.transform(test_df[feature_columns]),\n",
    "        columns=feature_columns,\n",
    "        index=test_df.index\n",
    "    )\n",
    "    y_test = pd.Series(\n",
    "        target_scaler.transform(test_df[[target_col]]).ravel(),\n",
    "        index=test_df.index\n",
    "    )\n",
    "    \n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "    \n",
    "    # Create sequences for Tyralis model\n",
    "    print(\"\\nCreating sequences...\")\n",
    "    \n",
    "    def create_sequences_tyralis(X, y, seq_length=10):\n",
    "        \"\"\"Create sequences for the Tyralis Random Forest model\"\"\"\n",
    "        X_seq, y_seq = [], []\n",
    "        \n",
    "        with tqdm(total=len(X) - seq_length, desc=\"Creating sequences\") as pbar:\n",
    "            for i in range(len(X) - seq_length):\n",
    "                # Input sequence\n",
    "                X_seq.append(X.iloc[i:i + seq_length].values)\n",
    "                \n",
    "                # Target value (single step ahead for Tyralis model)\n",
    "                y_seq.append(y.iloc[i + seq_length])\n",
    "                \n",
    "                pbar.update(1)\n",
    "        \n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train_seq, y_train_seq = create_sequences_tyralis(X_train, y_train, seq_length=sequence_length)\n",
    "    X_test_seq, y_test_seq = create_sequences_tyralis(X_test, y_test, seq_length=sequence_length)\n",
    "    \n",
    "    print(f\"X_train_seq shape: {X_train_seq.shape}, y_train_seq shape: {y_train_seq.shape}\")\n",
    "    print(f\"X_test_seq shape: {X_test_seq.shape}, y_test_seq shape: {y_test_seq.shape}\")\n",
    "    \n",
    "    # Reshape for RF input - RF expects 2D input\n",
    "    n_samples_train = X_train_seq.shape[0]\n",
    "    n_samples_test = X_test_seq.shape[0]\n",
    "    n_features_per_timestep = X_train_seq.shape[2] if len(X_train_seq.shape) > 2 else 1\n",
    "    total_features = sequence_length * n_features_per_timestep\n",
    "    \n",
    "    # Flatten the 3D sequences to 2D for RF\n",
    "    X_train_rf = X_train_seq.reshape(n_samples_train, -1)\n",
    "    X_test_rf = X_test_seq.reshape(n_samples_test, -1)\n",
    "    \n",
    "    print(f\"X_train_rf shape: {X_train_rf.shape}, y_train_seq shape: {y_train_seq.shape}\")\n",
    "    print(f\"X_test_rf shape: {X_test_rf.shape}, y_test_seq shape: {y_test_seq.shape}\")\n",
    "    print(f\"Total features after flattening: {X_train_rf.shape[1]}\")\n",
    "    \n",
    "    # Initialize Random Forest model with more trees and explicit parameters\n",
    "    print(\"\\nInitializing Random Forest model...\")\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=500,  # 500 trees as per paper\n",
    "        max_features='sqrt',  # Default mtry value\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        bootstrap=True,\n",
    "        random_state=42,\n",
    "        verbose=1,  # Add verbosity to see training progress\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    )\n",
    "    \n",
    "    # Train the model with timing\n",
    "    print(\"Training model...\")\n",
    "    start_time = time.time()\n",
    "    rf_model.fit(X_train_rf, y_train_seq)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Model training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Check model properties\n",
    "    print(f\"Number of trees in the forest: {len(rf_model.estimators_)}\")\n",
    "    print(f\"Feature importances: {rf_model.feature_importances_[:5]}...\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"Generating predictions...\")\n",
    "    predictions = rf_model.predict(X_test_rf)\n",
    "    \n",
    "    # Calculate R score on test data\n",
    "    r2_score = rf_model.score(X_test_rf, y_test_seq)\n",
    "    print(f\"R score on test data: {r2_score:.4f}\")\n",
    "    \n",
    "    # Inverse transform predictions to original scale\n",
    "    predictions_reshaped = predictions.reshape(-1, 1)\n",
    "    try:\n",
    "        predictions_original = target_scaler.inverse_transform(predictions_reshaped).flatten()\n",
    "    except:\n",
    "        # Handle case where predictions might need reshaping\n",
    "        predictions_original = np.array([target_scaler.inverse_transform([[p]])[0][0] for p in predictions])\n",
    "    \n",
    "    # Calculate some basic error metrics\n",
    "    mse = np.mean((predictions_original - original_test_actuals[sequence_length:sequence_length+len(predictions_original)])**2)\n",
    "    mae = np.mean(np.abs(predictions_original - original_test_actuals[sequence_length:sequence_length+len(predictions_original)]))\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "    \n",
    "    # Ensure predictions and actuals are the same length\n",
    "    min_length = min(len(predictions_original), len(original_test_actuals[sequence_length:]))\n",
    "    predictions_final = predictions_original[:min_length]\n",
    "    actuals_final = original_test_actuals[sequence_length:sequence_length+min_length]\n",
    "    \n",
    "    # Print sample of predictions vs actuals\n",
    "    print(\"\\nSample predictions vs actuals:\")\n",
    "    for i in range(min(5, len(predictions_final))):\n",
    "        print(f\"Prediction: {predictions_final[i]:.4f}, Actual: {actuals_final[i]:.4f}\")\n",
    "    \n",
    "    return rf_model, predictions_final, actuals_final, y_train_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics, model_choice=None, predictions=None, actuals=None, output_dir=None):\n",
    "    # Create a figure with 2x2 subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(f'Model Performance Metrics: {model_choice}', fontsize=16)\n",
    "    \n",
    "    # Flatten the axes for easier indexing\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    # Plot 1: Classification Metrics (top left)\n",
    "    classification_metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    values = []\n",
    "    \n",
    "    # Extract values from metrics dictionary\n",
    "    for metric in classification_metrics:\n",
    "        if metric in metrics:\n",
    "            values.append(float(metrics[metric]))\n",
    "        else:\n",
    "            values.append(0)\n",
    "    \n",
    "    create_bar_plot(\n",
    "        axs[0], \n",
    "        classification_metrics, \n",
    "        values, \n",
    "        'Classification Metrics', \n",
    "        'Score', \n",
    "        color='#1f77b4', \n",
    "        percentage=True\n",
    "    )\n",
    "    axs[0].set_ylim(0, 1)\n",
    "    \n",
    "    # Plot 2: Regression Metrics (top right)\n",
    "    regression_metrics = ['MAE', 'RMSE']\n",
    "    values = []\n",
    "    \n",
    "    # Extract values from metrics dictionary\n",
    "    for metric in regression_metrics:\n",
    "        if metric in metrics:\n",
    "            values.append(float(metrics[metric]))\n",
    "        else:\n",
    "            values.append(0)\n",
    "    \n",
    "    create_bar_plot(\n",
    "        axs[1], \n",
    "        regression_metrics, \n",
    "        values, \n",
    "        'Regression Metrics', \n",
    "        'Error Rate', \n",
    "        color='#2ca02c', \n",
    "        percentage=True\n",
    "    )\n",
    "    \n",
    "    # Plot 3: Predictions vs Actuals (bottom left)\n",
    "    if predictions is not None and actuals is not None and len(predictions) > 0 and len(actuals) > 0:\n",
    "        # Plot a sample of predictions vs actuals\n",
    "        sample_size = min(100, len(predictions))\n",
    "        indices = np.arange(sample_size)\n",
    "        \n",
    "        # Ensure we have enough data\n",
    "        if len(actuals) >= sample_size and len(predictions) >= sample_size:\n",
    "            plot_time_series_comparison(\n",
    "                indices,\n",
    "                actuals[:sample_size],\n",
    "                predictions[:sample_size],\n",
    "                'Predictions vs Actuals (Sample)',\n",
    "                axs[2],\n",
    "                labels=['Actual', 'Predicted']\n",
    "            )\n",
    "        else:\n",
    "            axs[2].text(0.5, 0.5, 'Insufficient data for plotting', \n",
    "                    ha='center', va='center', transform=axs[2].transAxes)\n",
    "    else:\n",
    "        axs[2].text(0.5, 0.5, 'No prediction data available', \n",
    "                ha='center', va='center', transform=axs[2].transAxes)\n",
    "        axs[2].set_title('Predictions vs Actuals', fontsize=11)\n",
    "    \n",
    "    # Plot 4: RMSSE and MASE Metrics (bottom right)\n",
    "    additional_metrics = []\n",
    "    values = []\n",
    "    \n",
    "    if 'RMSSE' in metrics and metrics['RMSSE'] is not None:\n",
    "        try:\n",
    "            additional_metrics.append('RMSSE')\n",
    "            values.append(float(metrics['RMSSE']))\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "    \n",
    "    if 'MASE' in metrics and metrics['MASE'] is not None:\n",
    "        try:\n",
    "            additional_metrics.append('MASE')\n",
    "            values.append(float(metrics['MASE']))\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "    \n",
    "    if additional_metrics and values:\n",
    "        create_bar_plot(\n",
    "            axs[3], \n",
    "            additional_metrics, \n",
    "            values, \n",
    "            'Time Series Specific Metrics', \n",
    "            'Score', \n",
    "            color='#ff7f0e', \n",
    "            percentage=False\n",
    "        )\n",
    "    else:\n",
    "        axs[3].text(0.5, 0.5, 'Time series metrics not available', \n",
    "                ha='center', va='center', transform=axs[3].transAxes)\n",
    "        axs[3].set_title('Time Series Specific Metrics', fontsize=11)\n",
    "    \n",
    "    # Adjust spacing and layout\n",
    "    plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    if output_dir:\n",
    "        plt.savefig(output_dir / f'{model_choice}_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def print_metrics(metrics, model_name):\n",
    "    \"\"\"\n",
    "    Print formatted metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{model_name} Model Results:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Classification metrics as percentages\n",
    "    print(\"\\nClassification Metrics:\")\n",
    "    for metric in ['Accuracy', 'Recall', 'Precision', 'F1 Score']:\n",
    "        if metric in metrics:\n",
    "            print(f\"{metric:15s}: {metrics[metric]*100:6.2f}%\")\n",
    "    \n",
    "    # Regression metrics\n",
    "    print(\"\\nRegression Metrics:\")\n",
    "    for metric in ['MAE', 'RMSE']:\n",
    "        if metric in metrics:\n",
    "            print(f\"{metric:15s}: {metrics[metric]*100:6.2f}%\")\n",
    "    \n",
    "    # Time series specific metrics\n",
    "    print(\"\\nTime Series Metrics:\")\n",
    "    for metric in ['MASE', 'RMSSE']:\n",
    "        if metric in metrics:\n",
    "            print(f\"{metric:15s}: {metrics[metric]:6.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions, actuals, y_train):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics on the raw predictions without modifications\n",
    "    \"\"\"\n",
    "    # Input validation and reshaping only (no value modifications)\n",
    "    predictions = np.array(predictions).reshape(-1)\n",
    "    actuals = np.array(actuals).reshape(-1)\n",
    "    y_train = np.array(y_train).reshape(-1)\n",
    "    \n",
    "    # Calculate directional changes\n",
    "    binary_pred = np.diff(predictions) > 0\n",
    "    binary_true = np.diff(actuals) > 0\n",
    "    \n",
    "    # Calculate classification metrics\n",
    "    try:\n",
    "        accuracy = accuracy_score(binary_true, binary_pred)\n",
    "        recall = recall_score(binary_true, binary_pred)\n",
    "        precision = precision_score(binary_true, binary_pred)\n",
    "        f1 = f1_score(binary_true, binary_pred)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Classification metrics calculation failed: {e}\")\n",
    "        accuracy = recall = precision = f1 = 0.0\n",
    "    \n",
    "    # Calculate error metrics on the raw values\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "    mase = calculate_mase(actuals, predictions, y_train)\n",
    "    rmsse = calculate_rmsse(actuals, predictions, y_train)\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'F1 Score': f1,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'MASE': mase,\n",
    "        'RMSSE': rmsse\n",
    "    }\n",
    "\n",
    "def calculate_mase(y_true, y_pred, y_train):\n",
    "    \"\"\"\n",
    "    Calculate Mean Absolute Scaled Error with validation\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    # Calculate naive forecast error (t+1 = t)\n",
    "    naive_errors = y_train[1:] - y_train[:-1]\n",
    "    naive_error = np.mean(np.abs(naive_errors))\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if naive_error == 0 or np.isnan(naive_error):\n",
    "        print(\"Warning: Naive error is zero or NaN, using MAE instead\")\n",
    "        return mae\n",
    "    \n",
    "    return mae / naive_error\n",
    "\n",
    "def calculate_rmsse(y_true, y_pred, y_train):\n",
    "    \"\"\"\n",
    "    Calculate Root Mean Squared Scaled Error with validation\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    # Calculate naive forecast error\n",
    "    naive_errors = (y_train[1:] - y_train[:-1])**2\n",
    "    naive_error = np.mean(naive_errors)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if naive_error == 0 or np.isnan(naive_error):\n",
    "        print(\"Warning: Naive error is zero or NaN, using RMSE instead\")\n",
    "        return np.sqrt(mse)\n",
    "    \n",
    "    return np.sqrt(mse / naive_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model_results, model_choice):\n",
    "    print(\"\\nGenerating predictions...\")\n",
    "    \n",
    "    if model_choice == 'lopez':\n",
    "        model, _, test_data, scaler, y_train_original, original_test_actuals = model_results\n",
    "        X_test, y_test = test_data\n",
    "        \n",
    "        # Generate predictions\n",
    "        raw_predictions = model.predict(X_test)\n",
    "        predictions = raw_predictions[:, 0]  # Take first day predictions\n",
    "        \n",
    "        # Validation checks\n",
    "        if np.any(np.isnan(predictions)):\n",
    "            print(\"Warning: NaN values detected in predictions\")\n",
    "            predictions = np.nan_to_num(predictions, 0)\n",
    "        \n",
    "        # Ensure positive values for percentage calculations\n",
    "        predictions = np.abs(predictions)\n",
    "        \n",
    "        # Debug information\n",
    "        print(\"\\nPrediction Scaling Check:\")\n",
    "        print(f\"Raw predictions range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
    "        \n",
    "        # Reshape and inverse transform\n",
    "        predictions = predictions.reshape(-1, 1)\n",
    "        predictions = scaler.inverse_transform(predictions).flatten()\n",
    "        \n",
    "        print(f\"Inverse transformed range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
    "        print(f\"Expected range (actuals): [{original_test_actuals.min():.4f}, {original_test_actuals.max():.4f}]\")\n",
    "        \n",
    "        # Ensure same length\n",
    "        min_length = min(len(predictions), len(original_test_actuals))\n",
    "        predictions = predictions[:min_length]\n",
    "        original_test_actuals = original_test_actuals[:min_length]\n",
    "        \n",
    "        return predictions, original_test_actuals, y_train_original\n",
    "    else:\n",
    "        model, predictions, actuals, y_train = model_results\n",
    "        return predictions, actuals, y_train\n",
    "        \n",
    "def evaluate_and_save_results(predictions, actuals, y_train, model_choice, output_dir):\n",
    "    \"\"\"\n",
    "    Evaluate model performance and save results\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting evaluation...\")\n",
    "    print(f\"Initial shapes - Predictions: {predictions.shape}, Actuals: {actuals.shape}, y_train: {y_train.shape}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = evaluate_model(predictions, actuals, y_train, model_choice)\n",
    "    \n",
    "    # Print metrics\n",
    "    print_metrics(metrics, model_choice)\n",
    "    \n",
    "    # Plot results\n",
    "    fig = plot_metrics(metrics, model_choice, predictions, actuals)\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    metrics_df = pd.DataFrame([metrics])\n",
    "    metrics_df.to_csv(output_dir / f'{model_choice}_model_metrics.csv', index=False)\n",
    "    \n",
    "    # Save plot\n",
    "    fig.savefig(output_dir / f'{model_choice}_metrics_plot.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mase_comparison(all_results, output_dir=None):\n",
    "    # Extract dataset names and model types\n",
    "    datasets = list(all_results.keys())\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    labels = []\n",
    "    mase_values = []\n",
    "    colors = []\n",
    "    \n",
    "    # Define consistent colors for models\n",
    "    model_colors = {\n",
    "        'lopez': '#1f77b4',    # Blue\n",
    "        'tyralis': '#2ca02c',  # Green\n",
    "        'default': '#ff7f0e'   # Orange (for any other models)\n",
    "    }\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        for model in all_results[dataset]:\n",
    "            if 'MASE' in all_results[dataset][model] and all_results[dataset][model]['MASE'] is not None:\n",
    "                try:\n",
    "                    mase_value = float(all_results[dataset][model]['MASE'])\n",
    "                    \n",
    "                    labels.append(f\"{dataset}\\n({model})\")\n",
    "                    mase_values.append(mase_value)\n",
    "                    \n",
    "                    # Assign color based on model type\n",
    "                    if model.lower() in model_colors:\n",
    "                        colors.append(model_colors[model.lower()])\n",
    "                    else:\n",
    "                        colors.append(model_colors['default'])\n",
    "                        \n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "    \n",
    "    # If no valid MASE values were found, display a message\n",
    "    if not mase_values:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.text(0.5, 0.5, 'No MASE metrics available for comparison', \n",
    "                ha='center', va='center', fontsize=14)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if output_dir:\n",
    "            plt.savefig(output_dir / 'mase_comparison.png', dpi=300)\n",
    "        plt.show()\n",
    "        return\n",
    "    \n",
    "    # Define annotations explaining MASE\n",
    "    annotations = [\n",
    "        ('MASE < 1: Model outperforms naive forecast', (0.02, 0.95)),\n",
    "        ('MASE > 1: Naive forecast outperforms model', (0.02, 0.90))\n",
    "    ]\n",
    "    \n",
    "    # Create legend elements\n",
    "    legend_elements = [\n",
    "        plt.Rectangle((0,0), 1, 1, fc=model_colors['lopez'], alpha=0.7, label='Lopez Model'),\n",
    "        plt.Rectangle((0,0), 1, 1, fc=model_colors['tyralis'], alpha=0.7, label='Tyralis Model')\n",
    "    ]\n",
    "    \n",
    "    # Create the comparison plot\n",
    "    fig = create_comparison_bar_plot(\n",
    "        labels=labels,\n",
    "        values=mase_values,\n",
    "        title='MASE Comparison Between Models',\n",
    "        ylabel='MASE Score (lower is better)',\n",
    "        colors=colors,\n",
    "        baseline=1,  # Add a horizontal line at MASE = 1\n",
    "        annotations=annotations,\n",
    "        legend_elements=legend_elements\n",
    "    )\n",
    "    \n",
    "    if output_dir:\n",
    "        plt.savefig(output_dir / 'mase_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluations_on_all_feature_sets(output_dir):\n",
    "    # First load all datasets\n",
    "    datasets = {\n",
    "        'full': pd.read_csv(output_dir / \"2015-2025_dataset_denoised.csv\", index_col=0, parse_dates=True),\n",
    "        'tyralis': pd.read_csv(output_dir / \"2015-2025_dataset_selected_features_tyralis.csv\", index_col=0, parse_dates=True),\n",
    "        'boruta': pd.read_csv(output_dir / \"2015-2025_dataset_selected_features_boruta.csv\", index_col=0, parse_dates=True),\n",
    "        'lasso': pd.read_csv(output_dir / \"2015-2025_dataset_selected_features_lasso.csv\", index_col=0, parse_dates=True)\n",
    "    }\n",
    "    \n",
    "    # Dictionary to store all results\n",
    "    all_results = {}\n",
    "    \n",
    "    # Run each model type\n",
    "    for model_type in ['lopez', 'tyralis']:\n",
    "        print(f\"\\nEVALUATING {model_type.upper()} MODEL\")\n",
    "        print(\"=\" * 50)\n",
    "        model_results = {}\n",
    "        \n",
    "        # Test each feature set\n",
    "        for dataset_name, df in datasets.items():\n",
    "            print(f\"\\nTesting on {dataset_name.upper()} dataset\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            # Run the model evaluation with the current dataset\n",
    "            metrics = run_model_evaluation(output_dir, model_type, df)\n",
    "            model_results[dataset_name] = metrics\n",
    "            \n",
    "            # Save individual results\n",
    "            pd.DataFrame([metrics]).to_csv(output_dir / f'{model_type}_model_metrics_{dataset_name}.csv', index=False)\n",
    "        \n",
    "        # Store results for this model type\n",
    "        all_results[model_type] = model_results\n",
    "        \n",
    "        # Create comparison DataFrame for this model\n",
    "        comparison_df = pd.DataFrame(model_results).T\n",
    "        comparison_df.to_csv(output_dir / f'{model_type}_model_metrics_comparison.csv')\n",
    "        \n",
    "        # Print comparison summary\n",
    "        print(f\"\\n{model_type.upper()} Model Performance Comparison:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for dataset_name, metrics in model_results.items():\n",
    "            print(f\"\\n{dataset_name.upper()} Dataset:\")\n",
    "            print(f\"Accuracy: {metrics['Accuracy']*100:.2f}%\")\n",
    "            print(f\"F1 Score: {metrics['F1 Score']*100:.2f}%\")\n",
    "            print(f\"MAE: {metrics['MAE']*100:.2f}%\")\n",
    "            print(f\"RMSE: {metrics['RMSE']*100:.2f}%\")\n",
    "            print(f\"MASE: {metrics['MASE']:.2f}\")\n",
    "            print(f\"RMSSE: {metrics['RMSSE']:.2f}\")\n",
    "             \n",
    "     # Create and save the scaled error metrics comparison plot\n",
    "    plot_mase_comparison(all_results, output_dir=output_dir)\n",
    "    \n",
    "    # Save overall comparison\n",
    "    overall_comparison = pd.DataFrame({\n",
    "        f\"{model}_{dataset}\": metrics \n",
    "        for model, model_results in all_results.items() \n",
    "        for dataset, metrics in model_results.items()\n",
    "    })\n",
    "    overall_comparison.to_csv(output_dir / 'overall_model_metrics_comparison.csv')\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Modified run_model_evaluation to accept the dataset parameter\n",
    "def run_model_evaluation(output_dir, model_choice, df):   \n",
    "    with tqdm(total=3, desc=f\"Running {model_choice.capitalize()} Model\") as pbar:\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "        \n",
    "        if model_choice == 'lopez':\n",
    "            model_results = use_lopez_model(df)\n",
    "            model, _, (X_test, _), scaler, y_train_original, original_test_actuals = model_results\n",
    "            pbar.update(1)\n",
    "        \n",
    "            predictions = model.predict(X_test)[:, 0]\n",
    "            predictions = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            # Ensure predictions and actuals are the same length\n",
    "            min_length = min(len(predictions), len(original_test_actuals))\n",
    "            predictions = predictions[:min_length]\n",
    "            original_test_actuals = original_test_actuals[:min_length]\n",
    "            \n",
    "            pbar.update(1)\n",
    "            \n",
    "        else:  # tyralis\n",
    "            model_results = use_tyralis_model(df)\n",
    "            model, predictions, original_test_actuals, y_train_original = model_results\n",
    "            pbar.update(2)\n",
    "        \n",
    "        # Print debug info\n",
    "        print(\"\\nDebug - Data Statistics:\")\n",
    "        print(f\"Predictions - min: {predictions.min():.4f}, max: {predictions.max():.4f}\")\n",
    "        print(f\"Actuals - min: {original_test_actuals.min():.4f}, max: {original_test_actuals.max():.4f}\")\n",
    "        \n",
    "        # Calculate and display metrics\n",
    "        metrics = evaluate_model(predictions, original_test_actuals, y_train_original)\n",
    "        print_metrics(metrics, model_choice)\n",
    "        plot_metrics(metrics, model_choice=model_choice, predictions=predictions, actuals=original_test_actuals, output_dir=output_dir)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Save metrics\n",
    "        pd.DataFrame([metrics]).to_csv(output_dir / f'{model_choice}_model_metrics.csv', index=False)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "all_results = run_evaluations_on_all_feature_sets(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
