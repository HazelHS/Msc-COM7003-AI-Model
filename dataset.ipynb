{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if pip install isnt working:\n",
    "# !python --version\n",
    "\n",
    "# !curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
    "# !python get-pip.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add citations to functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# requirements:\n",
    "#!python -m \n",
    "! pip install pandas yfinance requests numpy matplotlib PyWavelets seaborn scikit-learn scipy statsmodels tensorflow tqdm ipywidgets boruta\n",
    "# import the necessary libraries.\n",
    "import requests\n",
    "import pywt\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import deque\n",
    "\n",
    "from statsmodels.robust import mad\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.notebook import tqdm  # For Jupyter Notebooks\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Multiply, LSTM, Activation, LayerNormalization, MultiHeadAttention, Dropout, Bidirectional, Add, BatchNormalization, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need a dataset for this project, I was unable to find a complete dataset from a reputable source that suited my specific use case. So decided to look into collecting my own data and compiling one myself. \n",
    "# I'll be using the yfinance library to get the stock market data (in future I would like to include more data sources such as those from Tiingo).\n",
    "# I'll start by making a directory for the datasets.csv that we will need to generate.\n",
    "\n",
    "def vscode_progress(iterable, length=None, desc=''):\n",
    "    length = length or len(iterable)\n",
    "    for i, item in enumerate(iterable):\n",
    "        sys.stdout.write(f'\\r{desc} {i+1}/{length} ({((i+1)/length)*100:.1f}%)')\n",
    "        sys.stdout.flush()\n",
    "        yield item\n",
    "    print()\n",
    "    \n",
    "project_root = Path(os.getcwd()) #find file path of the working directory for notebook scripts.\n",
    "output_dir = project_root / \"dataset\"\n",
    "#check to see if the directory exists, make it if it doesn't\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_path = output_dir / \"2015-2025_dataset.csv\"\n",
    "\n",
    "# defining the date range for the dataset. \n",
    "# we'll be using start and end dates multiples times so its best to define them here only once, its likely we'll need to generate seperate time frames for the test/training split.\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2025-02-01\"\n",
    "\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D') # Format: YYYY-MM-DD, \"freq\" is the frequency of dates in this case: ='D' means daily.\n",
    "\n",
    "df = pd.DataFrame(index=date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will need to define a few functions that are needed to be run before I can generate the completed dataset require the for the AI model to work. The first block here is Largely AI assisted with Claude sonnet 3.5: Using the yahoo finance API, this block pulls the historical trading data needed for each individual major global stock exhange for volume and closed price, then converted the currencies to USD using forex data (also from the yfinance API) and returns the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stock_data(symbol: str, currency: str, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    df = yf.download(symbol, start=start_date, end=end_date)\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(index=date_range)\n",
    "        \n",
    "    # Create a new DataFrame with just Close and Volume\n",
    "    result = pd.DataFrame(index=df.index)\n",
    "    result['Close'] = df['Close']\n",
    "    result['Volume'] = df['Volume']\n",
    "    \n",
    "    # Get currency conversion rate if needed\n",
    "    if currency:\n",
    "        fx_data = yf.download(currency, start=start_date, end=end_date)\n",
    "        if not fx_data.empty:\n",
    "            fx_rate = fx_data['Close']\n",
    "            \n",
    "            # Ensure both dataframes have datetime index\n",
    "            result.index = pd.to_datetime(result.index)\n",
    "            fx_rate.index = pd.to_datetime(fx_rate.index)\n",
    "            \n",
    "            # Find common dates between stock and forex data\n",
    "            common_dates = result.index.intersection(fx_rate.index)            \n",
    "            # Keep only dates where we have both stock and forex data\n",
    "            result = result.loc[common_dates]\n",
    "            fx_rate = fx_rate.loc[common_dates]\n",
    "            \n",
    "            # Convert only Close prices to USD using element-wise multiplication\n",
    "            result['Close'] = result['Close'].values * fx_rate.values\n",
    "        else:\n",
    "            return pd.DataFrame(index=date_range)\n",
    "    \n",
    "    # Handle volume based on the index\n",
    "    if symbol in ['^N225', '^HSI']:  # Asian markets often have lower nominal volumes\n",
    "        result['Volume'] = result['Volume'] / 1_000  # Convert to thousands\n",
    "    else:\n",
    "        result['Volume'] = result['Volume'] / 1_000_000  # Convert to millions\n",
    "                \n",
    "    # Add sanity checks for extreme values\n",
    "    if result['Close'].max() > 50000 or result['Close'].min() < 1:\n",
    "        return pd.DataFrame(index=date_range)\n",
    "        \n",
    "    if result['Volume'].min() == 0 or result['Volume'].max() / result['Volume'].min() > 1000:\n",
    "        return pd.DataFrame(index=date_range)\n",
    "        \n",
    "    # Rename columns with symbol prefix\n",
    "    result = result.rename(columns={\n",
    "        'Close': f'{symbol}_Close_USD',\n",
    "        'Volume': f'{symbol}_Volume_M'  # M for millions or thousands for Asian markets\n",
    "    })\n",
    "    \n",
    "    # Reindex to full date range without filling\n",
    "    result = result.reindex(date_range)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block calls the previous blocks function iteratively for each of the 7 stock markets I have decided to include in the data collection. After aquiring the entire daily closed USD price and volume data for each exchange they are averaged together into two combined columns for the previously specified time frame.\n",
    "\n",
    "- \"(Tang et al.) demonstrates strong correlations between global market indices and crypto markets\" \n",
    "- \"The inclusion of Asian markets (Nikkei, Hang Seng) is particularly relevant as studies have shown significant Bitcoin trading volume from these regions\" \n",
    "- \"The SKEW index; research shows its effectiveness in predicting \"black swan\" events in crypto markets, OVX (Oil Volatility) \"Enhancing Bitcoin Price Prediction with Deep Learning\" shows volatility indices are key predictors\"\n",
    "\n",
    "- \"\"Cryptocurrency Valuation: An Explainable AI Approach\" validates the use of on-chain metrics as fundamental indicators\" - \"Hash rate and mining difficulty are particularly important as they reflect network security and mining economics\"\n",
    "- \"Transaction metrics provide insight into network usage and adoption\"\n",
    "\n",
    "- \"Deep Learning for Financial Applications: A Survey\" supports the inclusion of traditional safe-haven assets like gold, The DXY (Dollar Index) inclusion is supported by research showing strong inverse correlations with Bitcoin during certain market conditions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_market_stock_data(start_date, end_date):\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    result_df = pd.DataFrame(index=date_range)\n",
    "    \n",
    "    # Define indices with their currencies\n",
    "    indices = {\n",
    "        'GDAXI': {'symbol': '^GDAXI', 'currency': 'EURUSD=X'},    # Germany DAX\n",
    "        'IXIC': {'symbol': '^IXIC', 'currency': None},            # NASDAQ (already in USD)\n",
    "        'DJI': {'symbol': '^DJI', 'currency': None},              # Dow Jones (already in USD)\n",
    "        'N225': {'symbol': '^N225', 'currency': 'JPYUSD=X'},      # Nikkei\n",
    "        'STOXX50E': {'symbol': '^STOXX', 'currency': 'EURUSD=X'}, # Euro STOXX 50\n",
    "        'HSI': {'symbol': '^HSI', 'currency': 'HKDUSD=X'},        # Hang Seng\n",
    "        'FTSE': {'symbol': '^FTSE', 'currency': 'GBPUSD=X'}       # FTSE 100\n",
    "    }\n",
    "    \n",
    "    # Fetch data for all indices\n",
    "    combined_df = pd.DataFrame(index=date_range)\n",
    "    \n",
    "    for name, info in indices.items():\n",
    "        index_data = fetch_stock_data(info['symbol'], info['currency'], start_date, end_date)\n",
    "        if not index_data.empty and len(index_data.columns) > 0:\n",
    "            combined_df = pd.concat([combined_df, index_data], axis=1)\n",
    "    \n",
    "    # Calculate global averages\n",
    "    close_cols = [col for col in combined_df.columns if str(col).endswith('_Close_USD')]\n",
    "    volume_cols = [col for col in combined_df.columns if str(col).endswith('_Volume_M')]\n",
    "    \n",
    "    if close_cols and volume_cols:\n",
    "        result_df = pd.DataFrame(index=date_range)\n",
    "        result_df['Global averaged stocks(USD)'] = combined_df[close_cols].mean(axis=1, skipna=True)\n",
    "        result_df['Global averaged stocks (volume)'] = combined_df[volume_cols].mean(axis=1, skipna=True)\n",
    "        \n",
    "        return result_df\n",
    "    return pd.DataFrame(index=date_range, columns=['Global averaged stocks(USD)', 'Global averaged stocks (volume)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function works similar to the previous, collecting the US Dollar index (DXY) and the gold futures data from Yahoo Finance. Along with the Bitcoin-USD paring with its respective volume data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_currency_metrics(start_date, end_date):   \n",
    "    result_df = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "    # Get DXY (US Dollar Index)\n",
    "    dxy = yf.download(\"DX-Y.NYB\", start=start_date, end=end_date)\n",
    "    result_df['Currency US Dollar Index'] = dxy['Close']\n",
    "    \n",
    "    # Get Gold Futures\n",
    "    gold = yf.download(\"GC=F\", start=start_date, end=end_date)\n",
    "    result_df['Currency Gold Futures'] = gold['Close']\n",
    "    \n",
    "    # Get Bitcoin price data\n",
    "    btc = yf.download(\"BTC-USD\", start=start_date, end=end_date)\n",
    "    result_df['BTC/USD'] = btc['Close']\n",
    "    \n",
    "    # Get Bitcoin price data\n",
    "    btc = yf.download(\"BTC-USD\", start=start_date, end=end_date)\n",
    "    result_df['BTC Volume'] = btc['Volume']\n",
    "    \n",
    "    # Calculate Gold/BTC Ratio where BTC price is not zero or null\n",
    "    result_df['Gold/BTC Ratio'] = result_df['Currency Gold Futures'].div(result_df['BTC/USD'].replace(0, float('nan')))\n",
    "    result_df['Gold/BTC Ratio'] = result_df['Gold/BTC Ratio']\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same again here, with some additional assistence from Clude AI, and using the blockchain.info API, this function collects the individual \"on chain\" metrics that were chosen for inclusion in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blockchain_metric(metric_name, start_date, end_date):\n",
    "    \n",
    "    # Fetch single blockchain metric one by one, from the Blockchain.info API.\n",
    " \n",
    "    # Convert dates to timestamps\n",
    "    start_ts = int(datetime.strptime(start_date, \"%Y-%m-%d\").timestamp())\n",
    "    end_ts = int(datetime.strptime(end_date, \"%Y-%m-%d\").timestamp())\n",
    "    \n",
    "    # Fetch data from API with updated URL structure\n",
    "    url = f\"{\"https://api.blockchain.info\"}/{metric_name}\"\n",
    "    params = {\n",
    "        \"timespan\": \"all\",\n",
    "        \"start\": start_ts,\n",
    "        \"end\": end_ts,\n",
    "        \"format\": \"json\",\n",
    "        \"sampled\": \"true\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        return pd.Series(index=pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "        \n",
    "    data = response.json()\n",
    "    \n",
    "    # Check if the response has the expected structure\n",
    "    if not isinstance(data, dict) or 'values' not in data:\n",
    "        return pd.Series(index=pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "    \n",
    "    # Process the values\n",
    "    values = []\n",
    "    timestamps = []\n",
    "    for entry in data['values']:\n",
    "        if isinstance(entry, (list, tuple)) and len(entry) >= 2:\n",
    "            timestamps.append(entry[0])\n",
    "            values.append(float(entry[1]))\n",
    "        elif isinstance(entry, dict) and 'x' in entry and 'y' in entry:\n",
    "            timestamps.append(entry['x'])\n",
    "            values.append(float(entry['y']))\n",
    "    \n",
    "    if not values:\n",
    "        return pd.Series(index=pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "    \n",
    "    # Create DataFrame and handle data types\n",
    "    df = pd.DataFrame({'timestamp': timestamps, 'value': values})\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s').dt.normalize()\n",
    "    df = df.drop_duplicates('timestamp', keep='last')\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    # Handle potential overflow for large numbers\n",
    "    df['value'] = df['value'].astype('float64')\n",
    "    \n",
    "    # Reindex to ensure consistent date range\n",
    "    return df['value'].reindex(date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calls the previous block iteratively for each metric of \"on chain\" data, it is unclear to me which if any of these metrics have high enough correlation with the BTC-USD price movement to warrent final selection. As a reult I decided to include more than I would expect are required in the interest of thoroughness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onchain_metrics(start_date, end_date):\n",
    "    result_df = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "    # Define metrics and their API endpoints with updated paths\n",
    "    metrics = {\n",
    "        'Onchain Active Addresses': 'charts/n-unique-addresses',\n",
    "        'Onchain Transaction Count': 'charts/n-transactions',\n",
    "        'Onchain Hash Rate (GH/s)': 'charts/hash-rate',\n",
    "        'Onchain Mining Difficulty': 'charts/difficulty',\n",
    "        'Onchain Transaction Fees (BTC)': 'charts/transaction-fees',\n",
    "        'Onchain Median Confirmation Time (min)': 'charts/median-confirmation-time'\n",
    "    }\n",
    "    \n",
    "    # Fetch each metric\n",
    "    for col_name, metric_name in metrics.items():\n",
    "        series = get_blockchain_metric(metric_name, start_date, end_date)\n",
    "        result_df[col_name] = series\n",
    "        \n",
    "        # Handle missing values for each metric appropriately\n",
    "        if col_name in ['Onchain Mining Difficulty', 'Onchain Hash Rate (GH/s)']:\n",
    "            result_df[col_name] = result_df[col_name]\n",
    "        elif col_name in ['Onchain Transaction Count', 'Onchain Active Addresses']:\n",
    "            result_df[col_name] = result_df[col_name]\n",
    "        else:\n",
    "            result_df[col_name] = result_df[col_name]\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These additional metrics track the volatility of the S&P500 stock market and the crude oil volatility index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_volatility_indices(start_date, end_date):    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Get CBOE SKEW Index from Yahoo Finance\n",
    "    skew = yf.download(\"^SKEW\", start=start_date, end=end_date)\n",
    "    df['Volatility_CBOE SKEW Index'] = skew['Close']\n",
    "    \n",
    "    # Get VIX\n",
    "    vix = yf.download(\"^VIX\", start=start_date, end=end_date)\n",
    "    df['Volatility_CBOE Volatility Index (VIX)'] = vix['Close']\n",
    "    \n",
    "    # Get Oil VIX\n",
    "    ovx = yf.download(\"^OVX\", start=start_date, end=end_date)\n",
    "    df['Volatility_Crude Oil Volatility Index (OVX)'] = ovx['Close']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@100trillionUSD/modeling-bitcoins-value-with-scarcity-91fa0fc03e25\n",
    "\n",
    "https://medium.com/@100trillionUSD/bitcoin-stock-to-flow-cross-asset-model-50d260feed12\n",
    "\n",
    "https://newhedge.io/bitcoin/stock-to-flow\n",
    "\n",
    "Here I would like to include and calculate the \"Stock to Flow\" model intially conceptualized by \"PlanB\".\n",
    "\n",
    "Cluade AI helped with the S2F model calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stock_to_flow(start_date, end_date):\n",
    "    \n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    s2f_df = pd.DataFrame(index=date_range)\n",
    "    \n",
    "    # API parameters\n",
    "    params = {\n",
    "        \"timespan\": \"all\",\n",
    "        \"start\": int(pd.Timestamp(start_date).timestamp()),\n",
    "        \"end\": int(pd.Timestamp(end_date).timestamp()),\n",
    "        \"format\": \"json\",\n",
    "        \"sampled\": \"false\"\n",
    "    }\n",
    "    \n",
    "    # Get total supply\n",
    "    response = requests.get(\"https://api.blockchain.info/charts/total-bitcoins\", params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()['values']\n",
    "        df = pd.DataFrame(data, columns=['x', 'y'])\n",
    "        df['timestamp'] = pd.to_datetime(df['x'], unit='s').dt.normalize()\n",
    "        stock = df.groupby('timestamp')['y'].mean()\n",
    "        stock = stock.reindex(date_range).interpolate(method='linear')\n",
    "        \n",
    "        # Calculate flow based on Bitcoin halving schedule\n",
    "        s2f_df['timestamp'] = date_range\n",
    "        s2f_df['block height'] = ((s2f_df['timestamp'] - pd.Timestamp('2009-01-03')) / pd.Timedelta(minutes=10)).astype(int) # \"genesis block\" date (January 3, 2009) the first BTC block to be mined.\n",
    "        \n",
    "        # Calculate daily block rewards based on halving schedule\n",
    "        def get_block_reward(block_height):\n",
    "            halvings = block_height // 210000 # Roughly every 4 years there is a BTC \"halving event\" (when the mining rewards are halved) this is every 210,000 blocks.\n",
    "            return 50 / (2 ** halvings)\n",
    "        \n",
    "        s2f_df['daily production'] = s2f_df['block height'].apply(get_block_reward) * 144  # Timing by 144 gives us the total daily Bitcoin production (24 hours * 60 minutes) / 10 minutes = 144 blocks per day, \".apply(get_block_reward)\" calculates the reward for each block height.\n",
    "        \n",
    "        # Calculate S2F ratio (stock divided by yearly flow)\n",
    "        s2f_df['s2f ratio'] = stock / (s2f_df['daily production'] * 365)\n",
    "        \n",
    "        # Calculate expected price using S2F model\n",
    "        # Using the formula: Price = exp(-1.84) * S2F^3.36\n",
    "        s2f_df['S2F Model'] = np.exp(-1.84) * (s2f_df['s2f ratio'] ** 3.36)\n",
    "        \n",
    "        # Convert to USD and handle any extreme values\n",
    "        s2f_df['S2F Model'] = s2f_df['S2F Model']\n",
    "        \n",
    "        return s2f_df[['S2F Model']]\n",
    "    \n",
    "    return pd.DataFrame(index=date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main fuction for compiling, saving and ordering all the columns required for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all data components\n",
    "components = [\n",
    "    ('Stockmarket', get_market_stock_data(start_date, end_date)),\n",
    "    ('Currency Metrics', get_currency_metrics(start_date, end_date)),\n",
    "    ('On-chain Metrics', get_onchain_metrics(start_date, end_date)),\n",
    "    ('Volatility Indices', get_volatility_indices(start_date, end_date)),\n",
    "    ('S2F Model', calculate_stock_to_flow(start_date, end_date))\n",
    "    ]\n",
    "\n",
    "# Combine all components\n",
    "for name, component_df in components:\n",
    "    if component_df is not None and not component_df.empty:\n",
    "        for column in component_df.columns:\n",
    "            df[column] = component_df[column]\n",
    "\n",
    "# Reorder columns to group related metrics together\n",
    "column_order = [\n",
    "    'Global averaged stocks(USD)',\n",
    "    'Global averaged stocks (volume)',\n",
    "    'Currency US Dollar Index',\n",
    "    'Currency Gold Futures',\n",
    "    'Volatility_CBOE SKEW Index',\n",
    "    'Volatility_CBOE Volatility Index (VIX)',\n",
    "    'Volatility_Crude Oil Volatility Index (OVX)',\n",
    "    'Gold/BTC Ratio',\n",
    "    'BTC/USD',\n",
    "    'BTC Volume',\n",
    "    'S2F Model',\n",
    "    'Onchain Active Addresses',\n",
    "    'Onchain Transaction Count',\n",
    "    'Onchain Hash Rate (GH/s)',\n",
    "    'Onchain Mining Difficulty',\n",
    "    'Onchain Transaction Fees (BTC)',\n",
    "    'Onchain Median Confirmation Time (min)'\n",
    "]\n",
    "\n",
    "# Reorder the columns\n",
    "df = df[column_order]\n",
    "\n",
    "# Save the dataset\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "df.to_csv(output_path)\n",
    "print(f\"Dataset saved to {output_path}\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, provided that the code all runs correctly. We should have a dataset that is largly complete, except for missing entries in the weekends for stockmarket data and every other day in the blckchain metrics for BTC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interpolated dataset saved to e:\\Documents\\skool work\\Msc\\AI module\\Msc-COM7003-AI-Model\\dataset\\2015-2025_dataset_interpolated.csv\n",
      "Shape: (3684, 17)\n"
     ]
    }
   ],
   "source": [
    "# Load the saved dataset\n",
    "df = pd.read_csv(output_path, index_col=0, parse_dates=True)\n",
    "\n",
    "# Interpolate each column based on its data type\n",
    "for column in df.columns:\n",
    "    # For all other metrics (prices, volumes, etc), use linear interpolation\n",
    "    df[column] = df[column].interpolate(method='linear', limit=5)\n",
    "\n",
    "# We will need to remove the first row of data as it contains null entries and there is no way to interpolate it.\n",
    "df = df.iloc[1:]  \n",
    "\n",
    "# Save the interpolated dataset with a new name\n",
    "interpolated_path = output_dir / \"2015-2025_dataset_interpolated.csv\"\n",
    "df.to_csv(interpolated_path)\n",
    "print(f\"\\nInterpolated dataset saved to {interpolated_path}\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset will need to have all of the values normalized here as part of the preprocessing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_dir / \"2015-2025_dataset_interpolated.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Create a copy of the dataframe and normalize all columns\n",
    "df_normalized = pd.DataFrame(\n",
    "    scaler.fit_transform(df),\n",
    "    columns=df.columns,\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "# Save as new\n",
    "df_normalized.to_csv(output_dir / \"2015-2025_dataset_normalized.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this block creates a simple polt of each graph, allowing us to check and compare with online sources such as tradingview for accuracy. If there is anything major missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_dir / \"2015-2025_dataset_normalized.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Create subplots for each column\n",
    "n_cols = len(df.columns)\n",
    "fig, axes = plt.subplots(n_cols, 1, figsize=(15, 7*n_cols))\n",
    "\n",
    "# Plot each column\n",
    "for i, column in enumerate(df.columns):\n",
    "    # Create the plot on the corresponding subplot\n",
    "    axes[i].plot(df.index, df[column])\n",
    "    \n",
    "    # Customize each subplot\n",
    "    axes[i].set_title(f'{column}', fontsize=14)\n",
    "    axes[i].set_xlabel('Date', fontsize=12)\n",
    "    axes[i].set_ylabel('Value', fontsize=12)\n",
    "    axes[i].grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show all plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://dx.doi.org/10.3390/info12100388 - Method used for denoising [decription of how to do \"wavelet decomposition\" and/or \"wavelet\" denoising]\n",
    "\n",
    "Instead of handling outliers in a more traditional approach, since financial data is real world data and I am not versed enough in finance and economics to understand fully what kind of data could or should classify as \"outliers\" with much confidence. I instead would prefer to try \"denoising\" from the literacture I've found on the similar projects.\n",
    "\n",
    "\"Wavelet transforms analyse stock market trends over different periods and often show superior performance. Peng et al. (2021) demonstrated that combining multiresolution wavelet reconstruction with deep learning significantly improves medium-term stock prediction accuracy, achieving a 75% hit rate for US stocks. Another study introduced the Adaptive Multi-Scale Wavelet Neural Network (AMSW-NN), which performs well but depends on dataset quality (Ouyang et al., 2021).\" - https://arxiv.org/html/2408.12408v1#S3.SS3 TL;DR \"multiresolution wavelet reconstruction\"  is very good and preferable over \"adaptive Multi-Scale Wavelet Neural Network (AMSW-NN)\" due to its increased dependance on quality data. - \"multiresolution wavelet\" method explained in greater detail here: Peng et al. (2021) [https://www.mdpi.com/2078-2489/12/10/388] - only had a 0.63% improvement with much greater complexity, Best to keep things simple for both my sanity in programming and the \"computational efficienty\" of Pan Tang, Cheng Tang and Keren Wang's [https://doi.org/10.1002/for.3071] apporach:\n",
    "\n",
    "\n",
    "\"LSTM (long short-term memory), we propose a hybrid model of wavelet transform (WT) and multi-input LSTM\"\n",
    "\n",
    "LSTM + WT = flexible model.\n",
    "\n",
    "\"level 1 decomposition with db4 mother wavelet to eliminate noise. originall used in image processing. it is more widely\n",
    "used in stock price forecasting (Aussem, 1998; Alru-maih & Al-Fawzan, 2002; Caetano & Yoneyama, 2007; Huang, 2011)\"\n",
    "\n",
    "y[n] = Σ x[k]g[2n-k]  # Low-pass filter\n",
    "y[n] = Σ x[k]h[2n-k]  # High-pass filter\n",
    "\n",
    "\"db4\" stands for \"Daubechies-4\" wavelet\n",
    "It's called a \"mother\" wavelet because it's the original pattern that gets scaled and shifted\n",
    "The \"4\" represents the number of vanishing moments (a measure of complexity)\n",
    "\n",
    "Tang's Approach (2024):\n",
    "Simple level 1 decomposition with db4\n",
    "Complete zeroing of high-frequency coefficients\n",
    "Claimed Results:\n",
    "> Test accuracy increased from 51.72% - 57.76% to 64.66% - 72.19% after applying their denoising method\n",
    "> Focused on LSTM model performance improvement\n",
    "López Gil/Peng's Approach (2021, 2024):\n",
    "Multi-level (3-level) decomposition with db4\n",
    "Adaptive thresholding at each level\n",
    "López Gil's Results:\n",
    "> Achieved 72.82% test accuracy and 73.16% F1 score on the EWZ daily dataset\n",
    "Peng's Original Results:\n",
    "> Achieved 75% hit rate for US stocks\n",
    "\n",
    "The following denoising code was block was largely assisted by deepseek-r1, based on Peng's paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_denoising(df, wavelet='db4', level=3):  # Changed to 3 levels\n",
    "    df_denoised = df.copy()\n",
    "    \n",
    "    for column in df.columns:\n",
    "        # 1. Multi-level decomposition\n",
    "        coeffs = pywt.wavedec(df[column].values, wavelet, level=level)\n",
    "        \n",
    "        # 2. Calculate noise threshold (López Gil's method)\n",
    "        sigma = mad(coeffs[-1])\n",
    "        n = len(df[column])\n",
    "        threshold = sigma * np.sqrt(2 * np.log(n)) * 0.8  # Added 0.8 factor for more conservative thresholding\n",
    "        \n",
    "        # 3. Apply soft thresholding\n",
    "        coeffs_modified = [coeffs[0]]\n",
    "        for i in range(1, len(coeffs)):\n",
    "            coeffs_modified.append(pywt.threshold(coeffs[i], threshold, 'soft'))\n",
    "        \n",
    "        # 4. Reconstruct signal\n",
    "        denoised_data = pywt.waverec(coeffs_modified, wavelet)\n",
    "        \n",
    "        # 5. Handle boundary effects (can keep this as it's a good practice)\n",
    "        if len(denoised_data) > len(df):\n",
    "            denoised_data = denoised_data[:len(df)]\n",
    "        elif len(denoised_data) < len(df):\n",
    "            denoised_data = np.pad(denoised_data, (0, len(df)-len(denoised_data)), 'edge')\n",
    "            \n",
    "        df_denoised[column] = denoised_data\n",
    "    \n",
    "    return df_denoised\n",
    "\n",
    "def plot_denoising_results(original_data, denoised_data, column_name):\n",
    "    noise = original_data[column_name] - denoised_data[column_name]\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Original and denoised data\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(original_data.index, original_data[column_name], \n",
    "             label='Original', alpha=0.5)\n",
    "    plt.plot(denoised_data.index, denoised_data[column_name], \n",
    "             label='Denoised', alpha=0.8)\n",
    "    plt.title(f'Denoising Results for {column_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Removed noise\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(original_data.index, noise, label='Removed Noise', \n",
    "             alpha=0.5, color='red')\n",
    "    plt.title('Removed Noise Component')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "df = pd.read_csv(output_dir / \"2015-2025_dataset_normalized.csv\", \n",
    "                    index_col=0, parse_dates=True)\n",
    "\n",
    "df_denoised = wavelet_denoising(df)\n",
    "\n",
    "for column in df.columns:\n",
    "    plot_denoising_results(df, df_denoised, column)\n",
    "\n",
    "df_denoised.to_csv(output_dir / \"2015-2025_dataset_denoised.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for feature selection:\n",
    "\n",
    "[https://arxiv.org/pdf/2303.02223v2] - Pabuccu's - \"Feature Selection for Forecasting\" - Actually used \"FSA\" and \"boruta\" for validation: https://github.com/scikit-learn-contrib/boruta_py\n",
    "\n",
    "[https://doi.org/10.1002/for.3071] - Teng's did not feature select, he manually did it based on domain \"expertize\" - https://doi.org/10.1002/for.3071 - \"Stock movement prediction: A multi-input LSTM approach\"\n",
    "\n",
    "[https://www.mdpi.com/1999-4893/10/4/114] - tyralis's method - \"Variable Selection in Time Series Forecasting Using Random Forests\" - https://doi.org/10.3390/a10040114 - using both his prediction and feature selection method.\n",
    "\n",
    "LASSO feature selection method: https://www.tandfonline.com/doi/epdf/10.1080/09540091.2023.2286188?needAccess=true\n",
    "\n",
    "The original text block overestimated the mathematical parity of the implementations. This updated analysis provides a more accurate assessment of the actual implementation fidelity to the original papers.\n",
    "\n",
    "the code for this section, as well as help with understanding the underlying papers math, methods and processes were completed with the aid of both Clude sonnet 3.5 and deepseek r-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and plotting the feature selection of choice:\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
    "    return df\n",
    "\n",
    "# Use 'Score' column consistently - AI contributed:\n",
    "def plot_feature_importance(scores_df, method_name):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get selected features and their scores\n",
    "    selected_scores = scores_df[scores_df['Selected']]\n",
    "    score_col = 'Score' if 'Score' in selected_scores.columns else 'Importance'\n",
    "    \n",
    "    # Create horizontal bar plot\n",
    "    plt.barh(range(len(selected_scores)), selected_scores[score_col])\n",
    "    plt.yticks(range(len(selected_scores)), selected_scores['Feature'])\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title(f'Feature Importance Scores - {method_name}')\n",
    "    plt.grid(True, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def save_selected_features(df, selected_features, target, method_name):\n",
    "    final_features = selected_features + [target]\n",
    "    df_selected = df[final_features]\n",
    "    output_path = output_dir / f\"2015-2025_dataset_selected_features_{method_name}.csv\"\n",
    "    df_selected.to_csv(output_path)\n",
    "    return df_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forrest Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tyralis_selection(df, target='BTC/USD', window_size=30):\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    \n",
    "    # Paper's RF configuration\n",
    "    rf = RandomForestRegressor(n_estimators=500, #500\n",
    "                             max_features='sqrt',\n",
    "                             random_state=42) \n",
    "    \n",
    "    n_windows = len(X) - window_size\n",
    "    importance_matrix = np.zeros((n_windows, X.shape[1]))\n",
    "    \n",
    "    # Process windows sequentially as per paper\n",
    "    with tqdm(total=n_windows, desc='Processing windows') as pbar:\n",
    "        for i in range(n_windows):\n",
    "            # Paper's window approach\n",
    "            window_data = X.iloc[i:i + window_size]\n",
    "            window_target = y.iloc[i:i + window_size]\n",
    "            \n",
    "            # Fit RF on window\n",
    "            rf.fit(window_data, window_target)\n",
    "            baseline_score = rf.score(window_data, window_target)\n",
    "            \n",
    "            # Sequential permutation importance (paper's method)\n",
    "            for j in range(X.shape[1]):\n",
    "                X_perm = window_data.copy()\n",
    "                X_perm.iloc[:,j] = np.random.permutation(X_perm.iloc[:,j])\n",
    "                perm_score = rf.score(X_perm, window_target)\n",
    "                importance_matrix[i,j] = baseline_score - perm_score\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Paper's quantile threshold (75th percentile)\n",
    "    threshold = np.quantile(importance_matrix, 0.75, axis=0)\n",
    "    selected_features = X.columns[importance_matrix.mean(axis=0) > threshold].tolist()\n",
    "    \n",
    "    return selected_features, pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Score': importance_matrix.mean(axis=0),\n",
    "        'Selected': importance_matrix.mean(axis=0) > threshold\n",
    "    }).sort_values('Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_feature_selection(df, target='BTC/USD', alpha='auto'):\n",
    "    from sklearn.linear_model import LassoCV, Lasso\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Determine optimal alpha if auto\n",
    "    if alpha == 'auto':\n",
    "        lasso_cv = LassoCV(cv=5, random_state=42)\n",
    "        lasso_cv.fit(X_scaled, y)\n",
    "        alpha = lasso_cv.alpha_\n",
    "    \n",
    "    # Fit LASSO\n",
    "    lasso = Lasso(alpha=alpha, random_state=42)\n",
    "    lasso.fit(X_scaled, y)\n",
    "    \n",
    "    # Get selected features\n",
    "    selected_features = X.columns[lasso.coef_ != 0].tolist()\n",
    "    \n",
    "    # Create importance scores\n",
    "    importance_scores = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Score': np.abs(lasso.coef_),\n",
    "        'Selected': lasso.coef_ != 0\n",
    "    }).sort_values('Score', ascending=False)\n",
    "    \n",
    "    print(f\"\\nSelected {len(selected_features)} features\")\n",
    "    print(\"\\nTop features and weights:\")\n",
    "    print(importance_scores[importance_scores['Selected']].head())\n",
    "    \n",
    "    return selected_features, importance_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boruta's method: You can adjust the parameters like max_depth, n_estimators, and alpha based on your specific needs. The verbose=2 parameter will show you the progress of feature selection.\n",
    "\n",
    "Please suggest me code to replace this pabuccu selection function with the one mentioned in: @https://arxiv.org/pdf/2303.02223v2  They helpfully provided a github link to use someone else library, I would like to implement their method if possible (found here:@https://github.com/scikit-learn-contrib/boruta_py ). Thank you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boruta_selection(df, target='BTC/USD', max_iter=100, importance_threshold=0.01):\n",
    "    from boruta import BorutaPy\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from tqdm.notebook import tqdm\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "    \n",
    "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "    \n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    \n",
    "    rf = RandomForestRegressor(\n",
    "        n_jobs=-1,\n",
    "        max_depth=7,\n",
    "        n_estimators=250,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    boruta = BorutaPy(\n",
    "        rf,\n",
    "        n_estimators='auto',\n",
    "        max_iter=max_iter,\n",
    "        perc=98,\n",
    "        alpha=0.001,\n",
    "        two_step=True,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    with tqdm(total=1, desc=\"Running Boruta Selection\") as pbar:\n",
    "        boruta.fit(X.values, y.values)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    if hasattr(boruta, 'importance_history_'):\n",
    "        feature_importance = boruta.importance_history_.mean(axis=0)[:X.shape[1]]\n",
    "        scores = feature_importance / np.max(feature_importance)\n",
    "        \n",
    "        min_nonzero = np.min(feature_importance[feature_importance > 0])\n",
    "        relative_scores = np.zeros_like(feature_importance)\n",
    "        nonzero_mask = feature_importance > 0\n",
    "        relative_scores[nonzero_mask] = 10 * (np.log1p(feature_importance[nonzero_mask]) - np.log1p(min_nonzero)) / (np.log1p(np.max(feature_importance)) - np.log1p(min_nonzero))\n",
    "    else:\n",
    "        scores = np.ones(len(X.columns))\n",
    "        relative_scores = np.ones(len(X.columns))\n",
    "        feature_importance = np.ones(len(X.columns))\n",
    "    \n",
    "    significant_features = scores >= importance_threshold\n",
    "    \n",
    "    importance_scores = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': scores.round(4),\n",
    "        'Relative_Importance': relative_scores.round(2),\n",
    "        'Raw_Score': feature_importance.round(6),\n",
    "        'Selected': significant_features,\n",
    "        'Score': scores.round(4)  # Add Score column for compatibility\n",
    "    })\n",
    "    \n",
    "    importance_scores = importance_scores.sort_values('Importance', ascending=False)\n",
    "    selected_features = importance_scores[importance_scores['Selected']]['Feature'].tolist()\n",
    "    \n",
    "    print(f\"\\nSelected {len(selected_features)} features\")\n",
    "    print(\"\\nSelected features by importance:\")\n",
    "    print(importance_scores[importance_scores['Selected']].to_string(index=False))\n",
    "    \n",
    "    return selected_features, importance_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main execution block, run all three or only one of the feature at a time if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_feature_selection(method_choice):\n",
    "    df = pd.read_csv(output_dir / \"2015-2025_dataset_denoised.csv\", index_col=0, parse_dates=True)\n",
    "    target = 'BTC/USD'\n",
    "    \n",
    "    methods = {\n",
    "        'tyralis': {\n",
    "            'func': tyralis_selection,\n",
    "            'params': {'window_size': 30}\n",
    "        },\n",
    "        'lasso': {\n",
    "            'func': lasso_feature_selection,\n",
    "            'params': {}\n",
    "        },\n",
    "        'boruta': {\n",
    "            'func': boruta_selection,\n",
    "            'params': {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Execute selected method\n",
    "    if method_choice in ['tyralis', 'lasso', 'boruta']:\n",
    "        selected_features, importance_scores = methods[method_choice]['func'](\n",
    "            df, target, **methods[method_choice]['params']\n",
    "        )\n",
    "        \n",
    "        # Ensure consistent column naming\n",
    "        if 'Importance' in importance_scores.columns:\n",
    "            importance_scores['Score'] = importance_scores['Importance']\n",
    "        \n",
    "        # Save results and plot\n",
    "        shape = save_selected_features(df, selected_features, target, method_choice)\n",
    "        plot_feature_importance(importance_scores, method_choice)\n",
    "        \n",
    "        print(f\"\\nSelected {len(selected_features)} features using {method_choice.upper()} method\")\n",
    "        print(\"Features:\", ', '.join(selected_features))\n",
    "        print(f\"Output shape: {shape}\")\n",
    "\n",
    "#run_feature_selection('tyralis')\n",
    "run_feature_selection('lasso')\n",
    "run_feature_selection('boruta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is the model, 3 AI algorithms were chosem for this: \n",
    "\n",
    "1. Deep direct reinforcement learning:\n",
    "2. Random forrest:\n",
    "3. xLSTM-TS (Supervised Learning - Regression):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuzzyLayer(Layer):\n",
    "    def __init__(self, input_dim, fuzzy_sets=3, **kwargs):\n",
    "        super(FuzzyLayer, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.fuzzy_sets = fuzzy_sets\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.centers = self.add_weight(\n",
    "            name='centers',\n",
    "            shape=(self.input_dim, self.fuzzy_sets),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.widths = self.add_weight(\n",
    "            name='widths',\n",
    "            shape=(self.input_dim, self.fuzzy_sets),\n",
    "            initializer='ones',\n",
    "            trainable=True\n",
    "        )\n",
    "        super(FuzzyLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        expanded_inputs = tf.expand_dims(inputs, axis=2)\n",
    "        diff = expanded_inputs - self.centers\n",
    "        return tf.exp(-tf.square(diff) / (2 * tf.square(self.widths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_ddr_model(df):\n",
    "    # Prepare data\n",
    "    target = 'BTC/USD'\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "    \n",
    "    # Train/test split using same approach as other models\n",
    "    train_size = int(len(df) * 0.8)\n",
    "    X_train = X[:train_size]\n",
    "    X_test = X[train_size:]\n",
    "    y_train = y[:train_size]\n",
    "    y_test = y[train_size:]\n",
    "    \n",
    "    # Save original values for metrics calculation\n",
    "    y_train_original = y_train.copy()\n",
    "    original_test_actuals = y_test.copy()\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "    \n",
    "    # Build model\n",
    "    model = tf.keras.Sequential([\n",
    "        FuzzyLayer(X_train.shape[1]),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(20, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(\n",
    "        X_train_scaled, \n",
    "        y_train_scaled,\n",
    "        epochs=100,\n",
    "        verbose=0,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = model.predict(X_test_scaled)\n",
    "    predictions = scaler.inverse_transform(predictions).flatten()\n",
    "    \n",
    "    return model, predictions, original_test_actuals, y_train_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tyralis's model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tyralis_prediction_model(X_train, y_train, X_test=None, window_size=5):\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=10,  # Reduced for testing, 50 is needed for full run, with a window size of 30.\n",
    "        max_features='sqrt',\n",
    "        criterion='squared_error',  \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # If X_test is provided, we're doing out-of-sample prediction\n",
    "    if X_test is not None:\n",
    "        rf.fit(X_train, y_train)\n",
    "        predictions = rf.predict(X_test)\n",
    "        model = {'rf': rf}\n",
    "        return model, predictions\n",
    "    \n",
    "    # Otherwise, do rolling window prediction\n",
    "    predictions = []\n",
    "    feature_importance_history = []\n",
    "    \n",
    "    # Rolling window prediction\n",
    "    for i in range(len(X_train) - window_size):\n",
    "        # Get window data\n",
    "        X_window = X_train[i:i+window_size]\n",
    "        y_window = y_train[i:i+window_size]\n",
    "        \n",
    "        # Fit RF on current window\n",
    "        rf.fit(X_window, y_window)\n",
    "        \n",
    "        # Store feature importance for this window\n",
    "        feature_importance_history.append(rf.feature_importances_)\n",
    "        \n",
    "        # Predict next point\n",
    "        if i + window_size < len(X_train):\n",
    "            next_X = X_train[i+window_size].reshape(1, -1)\n",
    "            pred = rf.predict(next_X)[0]\n",
    "            predictions.append(pred)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    model = {\n",
    "        'rf': rf,\n",
    "        'feature_importance_history': np.array(feature_importance_history)\n",
    "    }\n",
    "    \n",
    "    return model, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lopez's model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function combining MSE and directional accuracy\n",
    "    \"\"\"\n",
    "    # MSE component\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    \n",
    "    # Directional component\n",
    "    y_true_direction = tf.cast(y_true[1:] > y_true[:-1], tf.float32)\n",
    "    y_pred_direction = tf.cast(y_pred[1:] > y_pred[:-1], tf.float32)\n",
    "    directional = tf.reduce_mean(tf.keras.losses.binary_crossentropy(\n",
    "        y_true_direction, \n",
    "        y_pred_direction,\n",
    "        from_logits=False\n",
    "    ))\n",
    "    \n",
    "    # Combine losses with weighting (α=0.7 as per paper)\n",
    "    alpha = 0.7\n",
    "    return alpha * mse + (1 - alpha) * directional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xLSTM_block(inputs, units):\n",
    "    # xLSTM block with simplified attention\n",
    "    lstm_out = LSTM(units, return_sequences=True)(inputs)\n",
    "    norm = LayerNormalization(epsilon=1e-6)(lstm_out)\n",
    "    \n",
    "    # Simplified multi-head attention (4 heads as per optimization)\n",
    "    attn = MultiHeadAttention(\n",
    "        num_heads=4, \n",
    "        key_dim=units//4\n",
    "    )(norm, norm, norm)\n",
    "    \n",
    "    add1 = tf.keras.layers.Add()([norm, attn])\n",
    "    norm2 = LayerNormalization(epsilon=1e-6)(add1)\n",
    "    \n",
    "    return norm2\n",
    "\n",
    "def lopez_xLSTM_TS_model(input_shape, forecast_horizon=7):\n",
    "    \"\"\"\n",
    "    Proper xLSTM-TS implementation following López's paper architecture\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # First xLSTM block\n",
    "    x = Bidirectional(LSTM(512, return_sequences=True))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    skip1 = x\n",
    "    \n",
    "    # Self-attention mechanism\n",
    "    attention = Dense(1024, use_bias=False)(x)  # 1024 = 2*512 (bidirectional)\n",
    "    attention = BatchNormalization()(attention)\n",
    "    attention = Activation('tanh')(attention)\n",
    "    attention_weights = Dense(1, activation='sigmoid')(attention)\n",
    "    x = Multiply()([x, attention_weights])\n",
    "    \n",
    "    # Second xLSTM block with skip connection\n",
    "    x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    # Project skip connection to match dimensions\n",
    "    skip1 = Dense(512)(skip1)  # Match dimensions for skip connection\n",
    "    x = Add()([x, skip1])\n",
    "    \n",
    "    # Third xLSTM block\n",
    "    x = Bidirectional(LSTM(128))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(forecast_horizon)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Splitting Explanation\n",
    "Both models use a temporal split approach, but with different considerations:\n",
    "\n",
    "1. López's Approach:\n",
    "   - Training: 70% of data\n",
    "   - Validation: 15% of data\n",
    "   - Testing: 15% of data\n",
    "   \n",
    "   Key Features:\n",
    "   - Maintains temporal order\n",
    "   - Uses sequence creation\n",
    "   - Includes validation set\n",
    "   - Scales data using training statistics only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, target_col='BTC/USD', train_ratio=0.7, val_ratio=0.15):\n",
    "    # Calculate split points\n",
    "    total_rows = len(df)\n",
    "    train_size = int(total_rows * train_ratio)\n",
    "    val_size = int(total_rows * val_ratio)\n",
    "    \n",
    "    # Split maintaining temporal order\n",
    "    train_df = df[:train_size]\n",
    "    val_df = df[train_size:train_size + val_size]\n",
    "    test_df = df[train_size + val_size:]\n",
    "    \n",
    "    # Scale data using only training data statistics\n",
    "    feature_scaler = StandardScaler()\n",
    "    target_scaler = StandardScaler()\n",
    "    \n",
    "    # Prepare feature and target data\n",
    "    X_train = feature_scaler.fit_transform(train_df.drop(columns=[target_col]))\n",
    "    y_train = target_scaler.fit_transform(train_df[[target_col]])\n",
    "    \n",
    "    X_val = feature_scaler.transform(val_df.drop(columns=[target_col]))\n",
    "    y_val = target_scaler.transform(val_df[[target_col]])\n",
    "    \n",
    "    X_test = feature_scaler.transform(test_df.drop(columns=[target_col]))\n",
    "    y_test = target_scaler.transform(test_df[[target_col]])\n",
    "    \n",
    "    print(f\"Train size: {len(X_train)}, Validation size: {len(X_val)}, Test size: {len(X_test)}\")\n",
    "    \n",
    "    return {\n",
    "        'train': (X_train, y_train),\n",
    "        'val': (X_val, y_val),\n",
    "        'test': (X_test, y_test),\n",
    "        'scalers': {\n",
    "            'feature': feature_scaler,\n",
    "            'target': target_scaler\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    with tqdm(total=len(X) - seq_length, desc=\"Creating sequences\") as pbar:\n",
    "        for i in range(len(X) - seq_length):\n",
    "            X_seq.append(X[i:i + seq_length])\n",
    "            y_seq.append(y[i + seq_length])\n",
    "            pbar.update(1)\n",
    "    return np.array(X_seq), np.array(y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingProgressCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epochs = self.params['epochs']\n",
    "        self.steps = self.params['steps'] * self.epochs\n",
    "        self.progress_bar = tqdm(\n",
    "            total=self.steps,\n",
    "            desc=\"Training Progress\",\n",
    "            unit=\"batch\",\n",
    "            leave=True  # Ensure progress bar remains after completion\n",
    "        )\n",
    "        self.current_step = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.epochs_without_improvement = 0\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.current_step += 1\n",
    "        self.progress_bar.update(1)\n",
    "        \n",
    "        # Update progress bar with more detailed metrics\n",
    "        self.progress_bar.set_postfix({\n",
    "            'loss': f\"{logs['loss']:.4f}\",\n",
    "            'val_loss': f\"{logs.get('val_loss', 'N/A')}\",\n",
    "            'mae': f\"{logs['mae']:.4f}\",\n",
    "            'epoch': f\"{self.current_step // self.params['steps']}/{self.epochs}\"\n",
    "        })\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Monitor validation loss for early stopping\n",
    "        current_loss = logs.get('val_loss', logs['loss'])\n",
    "        if current_loss < self.best_loss:\n",
    "            self.best_loss = current_loss\n",
    "            self.epochs_without_improvement = 0\n",
    "        else:\n",
    "            self.epochs_without_improvement += 1\n",
    "            \n",
    "        # Update progress bar with epoch information\n",
    "        self.progress_bar.set_postfix({\n",
    "            'loss': f\"{logs['loss']:.4f}\",\n",
    "            'val_loss': f\"{logs.get('val_loss', 'N/A')}\",\n",
    "            'mae': f\"{logs['mae']:.4f}\",\n",
    "            'epoch': f\"{epoch + 1}/{self.epochs}\",\n",
    "            'no_improve': self.epochs_without_improvement\n",
    "        })\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_lopez_model(df, target_col='BTC/USD', sequence_length=30):\n",
    "    print(\"\\nInitializing López's xLSTM-TS model training...\")\n",
    "    \n",
    "    # Calculate split points\n",
    "    total_rows = len(df)\n",
    "    train_size = int(total_rows * 0.7)\n",
    "    val_size = int(total_rows * 0.15)\n",
    "    \n",
    "    # Split maintaining temporal order\n",
    "    train_df = df[:train_size]\n",
    "    val_df = df[train_size:train_size + val_size]\n",
    "    test_df = df[train_size + val_size:]\n",
    "    \n",
    "    # Store original test values\n",
    "    original_test_actuals = test_df[target_col].values\n",
    "    \n",
    "    # Scale data with proper DataFrame handling\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "    \n",
    "    # Fit and transform training data\n",
    "    X_train = pd.DataFrame(\n",
    "        feature_scaler.fit_transform(train_df.drop(columns=[target_col])),\n",
    "        columns=train_df.drop(columns=[target_col]).columns,\n",
    "        index=train_df.index\n",
    "    )\n",
    "    y_train = pd.DataFrame(\n",
    "        target_scaler.fit_transform(train_df[[target_col]]),\n",
    "        columns=[target_col],\n",
    "        index=train_df.index\n",
    "    )\n",
    "    \n",
    "    # Transform validation data\n",
    "    X_val = pd.DataFrame(\n",
    "        feature_scaler.transform(val_df.drop(columns=[target_col])),\n",
    "        columns=train_df.drop(columns=[target_col]).columns,\n",
    "        index=val_df.index\n",
    "    )\n",
    "    y_val = pd.DataFrame(\n",
    "        target_scaler.transform(val_df[[target_col]]),\n",
    "        columns=[target_col],\n",
    "        index=val_df.index\n",
    "    )\n",
    "    \n",
    "    # Transform test data\n",
    "    X_test = pd.DataFrame(\n",
    "        feature_scaler.transform(test_df.drop(columns=[target_col])),\n",
    "        columns=train_df.drop(columns=[target_col]).columns,\n",
    "        index=test_df.index\n",
    "    )\n",
    "    y_test = pd.DataFrame(\n",
    "        target_scaler.transform(test_df[[target_col]]),\n",
    "        columns=[target_col],\n",
    "        index=test_df.index\n",
    "    )\n",
    "    \n",
    "    # Store original training data for MASE calculation\n",
    "    y_train_original = train_df[target_col].values\n",
    "    \n",
    "    # Create sequences\n",
    "    print(\"\\nCreating sequences...\")\n",
    "    sequence_progress = tqdm(total=3, desc=\"Sequence preparation\")\n",
    "    \n",
    "    # Training sequences\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X_train) - sequence_length - 6):\n",
    "        X_seq.append(X_train.iloc[i:i + sequence_length].values)\n",
    "        y_seq.append(y_train.iloc[i + sequence_length:i + sequence_length + 7].values.reshape(-1))\n",
    "    X_train_seq, y_train_seq = np.array(X_seq), np.array(y_seq)\n",
    "    sequence_progress.update(1)\n",
    "    \n",
    "    # Validation sequences\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X_val) - sequence_length - 6):\n",
    "        X_seq.append(X_val.iloc[i:i + sequence_length].values)\n",
    "        y_seq.append(y_val.iloc[i + sequence_length:i + sequence_length + 7].values.reshape(-1))\n",
    "    X_val_seq, y_val_seq = np.array(X_seq), np.array(y_seq)\n",
    "    sequence_progress.update(1)\n",
    "    \n",
    "    # Test sequences\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X_test) - sequence_length - 6):\n",
    "        X_seq.append(X_test.iloc[i:i + sequence_length].values)\n",
    "        y_seq.append(y_test.iloc[i + sequence_length:i + sequence_length + 7].values.reshape(-1))\n",
    "    X_test_seq, y_test_seq = np.array(X_seq), np.array(y_seq)\n",
    "    sequence_progress.update(1)\n",
    "    sequence_progress.close()\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"\\nInitializing model...\")\n",
    "    inputs = Input(shape=(sequence_length, X_train.shape[1]))\n",
    "    \n",
    "    # First xLSTM block with enhanced attention and normalization\n",
    "    x = Bidirectional(LSTM(256, return_sequences=True))(inputs)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    skip1 = x\n",
    "\n",
    "    # Enhanced self-attention mechanism matching paper specs\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)  # Pre-attention norm\n",
    "    attention = MultiHeadAttention(\n",
    "        num_heads=8,\n",
    "        key_dim=64,\n",
    "        value_dim=64,\n",
    "        dropout=0.1\n",
    "    )(x, x, x)\n",
    "    \n",
    "    attention = Dense(512)(attention)  # Project to match dimensions\n",
    "    attention = LayerNormalization(epsilon=1e-6)(attention)  # Post-attention norm\n",
    "    x = Add()([x, attention])\n",
    "    \n",
    "    # Second xLSTM block with enhanced skip connections\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)  # Changed from 256 to 128\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Enhanced skip connection handling\n",
    "    skip1_projection = Dense(256, kernel_regularizer=l1(1e-5))(skip1)  # Changed from 512 to 256\n",
    "    skip1_projection = LayerNormalization(epsilon=1e-6)(skip1_projection)\n",
    "    x = Add()([x, skip1_projection])\n",
    "    \n",
    "    # Third xLSTM block with additional normalization\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = Bidirectional(LSTM(64))(x)  # Changed from 128 to 64\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer with additional regularization\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    outputs = Dense(7, kernel_regularizer=l1(1e-5))(x)\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = Adam(\n",
    "        learning_rate=1e-4,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-7\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=directional_loss,\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    # Train model with paper's configuration\n",
    "    print(\"\\nTraining model...\")\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        ),\n",
    "        TrainingProgressCallback()\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        epochs=100,\n",
    "        batch_size=128,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return model, history, (X_test_seq, y_test_seq), target_scaler, y_train_original, original_test_actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_tyralis_model(df, target_col='BTC/USD', window_size=5):\n",
    "\n",
    "    # Prepare data (features are already selected)\n",
    "    data = prepare_data(df, target_col)\n",
    "    X_train, y_train = data['train']\n",
    "    X_test, y_test = data['test']\n",
    "    \n",
    "    # Train model and get predictions\n",
    "    model, predictions = tyralis_prediction_model(\n",
    "        X_train, \n",
    "        y_train.reshape(-1),\n",
    "        X_test,\n",
    "        window_size=window_size\n",
    "    )\n",
    "    \n",
    "    # Transform predictions back to original scale\n",
    "    predictions_original = data['scalers']['target'].inverse_transform(\n",
    "        predictions.reshape(-1, 1)\n",
    "    )\n",
    "    y_test_original = data['scalers']['target'].inverse_transform(y_test)\n",
    "    y_train_original = data['scalers']['target'].inverse_transform(y_train)\n",
    "    \n",
    "    return model, predictions_original, y_test_original, y_train_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics, model_name, predictions, actuals):\n",
    "    # Ensure arrays are 1D\n",
    "    predictions = predictions.reshape(-1)\n",
    "    actuals = actuals.reshape(-1)\n",
    "    \n",
    "    # Trim arrays to same length\n",
    "    min_length = min(len(predictions), len(actuals))\n",
    "    predictions = predictions[:min_length]\n",
    "    actuals = actuals[:min_length]\n",
    "    time_idx = np.arange(min_length)\n",
    "    \n",
    "    # Create plot\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(20, 15))\n",
    "    \n",
    "    # 1. Classification Metrics Bar Plot\n",
    "    class_metrics = ['Accuracy', 'Recall', 'Precision', 'F1 Score']\n",
    "    sns.barplot(x=class_metrics, \n",
    "                y=[metrics[m]*100 for m in class_metrics], \n",
    "                ax=axes[0,0])\n",
    "    axes[0,0].set_title(f'{model_name} Classification Metrics (%)')\n",
    "    axes[0,0].set_ylim(0, 100)\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    axes[0,0].set_ylabel('Percentage (%)')\n",
    "    \n",
    "    # 2. Error Metrics Bar Plot\n",
    "    reg_metrics = ['MAE', 'RMSE']\n",
    "    sns.barplot(x=reg_metrics, \n",
    "                y=[metrics[m]*100 for m in reg_metrics], \n",
    "                ax=axes[0,1])\n",
    "    axes[0,1].set_title(f'{model_name} Error Metrics (%)')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    axes[0,1].set_ylabel('Percentage (%)')\n",
    "    \n",
    "    # 3. Actual vs Predicted Plot (Normalized)\n",
    "    axes[1,0].plot(time_idx, actuals, label='Actual', alpha=0.7, linewidth=1)\n",
    "    axes[1,0].plot(time_idx, predictions, label='Predicted', alpha=0.7, linewidth=1)\n",
    "    axes[1,0].set_title(f'{model_name} Predictions vs Actuals (Normalized)\\n' +\n",
    "                       f'Pred range: [{predictions.min():.4f}, {predictions.max():.4f}]\\n' +\n",
    "                       f'Actual range: [{actuals.min():.4f}, {actuals.max():.4f}]')\n",
    "    axes[1,0].set_xlabel('Time Steps')\n",
    "    axes[1,0].set_ylabel('Normalized Value')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True)\n",
    "    \n",
    "    # Add moving averages for clearer trend visualization\n",
    "    window = 7\n",
    "    ma_actual = pd.Series(actuals).rolling(window=window).mean()\n",
    "    ma_pred = pd.Series(predictions).rolling(window=window).mean()\n",
    "    axes[1,0].plot(time_idx, ma_actual, '--', label='Actual (MA)', alpha=0.5)\n",
    "    axes[1,0].plot(time_idx, ma_pred, '--', label='Predicted (MA)', alpha=0.5)\n",
    "    \n",
    "    # 4. Scaled Error Metrics\n",
    "    scale_metrics = ['MASE', 'RMSSE']\n",
    "    sns.barplot(x=scale_metrics, \n",
    "                y=[metrics[m] for m in scale_metrics], \n",
    "                ax=axes[1,1])\n",
    "    axes[1,1].set_title(f'{model_name} Scaled Error Metrics')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    axes[1,1].set_ylabel('Ratio')\n",
    "    \n",
    "    # New trading metrics subplot (for all models)\n",
    "    trading_metrics = ['Sharpe_Ratio', 'Max_Drawdown', 'Trading_Frequency']\n",
    "    trading_values = [metrics[m] for m in trading_metrics]\n",
    "    sns.barplot(x=trading_metrics,\n",
    "                y=trading_values,\n",
    "                ax=axes[2,0])\n",
    "    axes[2,0].set_title(f'{model_name} Trading Performance Metrics')\n",
    "    axes[2,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Only add Fuzzy Uncertainty for DDR model\n",
    "    if model_name == 'ddr' and not np.isnan(metrics['Fuzzy_Uncertainty']):\n",
    "        axes[2,1].bar(['Fuzzy Uncertainty'], [metrics['Fuzzy_Uncertainty']])\n",
    "        axes[2,1].set_title('DDR Model Uncertainty')\n",
    "    else:\n",
    "        axes[2,1].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def print_metrics(metrics, model_name):\n",
    "    print(f\"\\n{model_name} Model Results:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Existing metrics\n",
    "    print(\"\\nClassification Metrics:\")\n",
    "    for metric in ['Accuracy', 'Recall', 'Precision', 'F1 Score']:\n",
    "        print(f\"{metric:15s}: {metrics[metric]*100:6.2f}%\")\n",
    "    \n",
    "    print(\"\\nRegression Metrics:\")\n",
    "    print(f\"{'MAE':15s}: {metrics['MAE']*100:6.2f}%\")\n",
    "    print(f\"{'RMSE':15s}: {metrics['RMSE']*100:6.2f}%\")\n",
    "    print(f\"{'MASE':15s}: {metrics['MASE']:6.2f}\")\n",
    "    print(f\"{'RMSSE':15s}: {metrics['RMSSE']:6.2f}\")\n",
    "    \n",
    "    # New trading metrics section\n",
    "    print(\"\\nTrading Performance Metrics:\")\n",
    "    print(f\"{'Sharpe Ratio':15s}: {metrics['Sharpe_Ratio']:6.2f}\")\n",
    "    print(f\"{'Max Drawdown':15s}: {metrics['Max_Drawdown']*100:6.2f}%\")\n",
    "    print(f\"{'Trading Freq':15s}: {metrics['Trading_Frequency']*100:6.2f}%\")\n",
    "    \n",
    "    # Model-specific metrics\n",
    "    if model_name == 'ddr' and not np.isnan(metrics['Fuzzy_Uncertainty']):\n",
    "        print(f\"\\nDDR-Specific Metrics:\")\n",
    "        print(f\"{'Fuzzy Uncert':15s}: {metrics['Fuzzy_Uncertainty']:6.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions, actuals, y_train):\n",
    "    # Ensure arrays are 1D and same length\n",
    "    predictions = predictions.reshape(-1)\n",
    "    actuals = actuals.reshape(-1)\n",
    "    y_train = y_train.reshape(-1)\n",
    "    \n",
    "    # Trim arrays to same length if needed\n",
    "    min_length = min(len(predictions), len(actuals))\n",
    "    predictions = predictions[:min_length]\n",
    "    actuals = actuals[:min_length]\n",
    "    \n",
    "    # Calculate directional changes\n",
    "    binary_pred = (predictions[1:] > predictions[:-1]).astype(int)\n",
    "    binary_true = (actuals[1:] > actuals[:-1]).astype(int)\n",
    "    \n",
    "    # Ensure binary arrays are same length\n",
    "    min_binary_length = min(len(binary_pred), len(binary_true))\n",
    "    binary_pred = binary_pred[:min_binary_length]\n",
    "    binary_true = binary_true[:min_binary_length]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(binary_true, binary_pred),\n",
    "        'Recall': recall_score(binary_true, binary_pred),\n",
    "        'Precision': precision_score(binary_true, binary_pred),\n",
    "        'F1 Score': f1_score(binary_true, binary_pred),\n",
    "        'MAE': mean_absolute_error(actuals, predictions),\n",
    "        'RMSE': np.sqrt(mean_squared_error(actuals, predictions)),\n",
    "        'MASE': calculate_mase(actuals, predictions, y_train),\n",
    "        'RMSSE': calculate_rmsse(actuals, predictions, y_train)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_mase(y_true, y_pred, y_train):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    naive_error = np.mean(np.abs(y_train[1:] - y_train[:-1]))\n",
    "    return mae / naive_error if naive_error != 0 else mae\n",
    "\n",
    "def calculate_rmsse(y_true, y_pred, y_train):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    naive_error = np.mean((y_train[1:] - y_train[:-1])**2)\n",
    "    return np.sqrt(mse / naive_error) if naive_error != 0 else np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify get_predictions function to include verification\n",
    "def get_predictions(model_results, model_choice):\n",
    "    print(\"\\nGenerating predictions...\")\n",
    "    \n",
    "    if model_choice == 'lopez':\n",
    "        model, _, test_data, scaler, y_train_original, original_test_actuals = model_results\n",
    "        X_test, y_test = test_data\n",
    "        \n",
    "        # Generate predictions\n",
    "        raw_predictions = model.predict(X_test)\n",
    "        predictions = raw_predictions[:, 0]  # Take first day predictions\n",
    "        \n",
    "        print(\"\\nPrediction Scaling Check:\")\n",
    "        print(f\"Raw predictions range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
    "        \n",
    "        # Reshape and inverse transform\n",
    "        predictions = predictions.reshape(-1, 1)\n",
    "        predictions = scaler.inverse_transform(predictions).flatten()\n",
    "        \n",
    "        print(f\"Inverse transformed range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
    "        print(f\"Expected range (actuals): [{original_test_actuals.min():.4f}, {original_test_actuals.max():.4f}]\")\n",
    "        \n",
    "        return predictions, original_test_actuals, y_train_original\n",
    "    else:\n",
    "        model, predictions, actuals, y_train = model_results\n",
    "        return predictions, actuals, y_train\n",
    "        \n",
    "def evaluate_and_save_results(predictions, actuals, y_train, model_choice, output_dir):\n",
    "    \"\"\"\n",
    "    Evaluate model performance and save results\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting evaluation...\")\n",
    "    print(f\"Initial shapes - Predictions: {predictions.shape}, Actuals: {actuals.shape}, y_train: {y_train.shape}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = evaluate_model(predictions, actuals, y_train, model_choice)\n",
    "    \n",
    "    # Print metrics\n",
    "    print_metrics(metrics, model_choice)\n",
    "    \n",
    "    # Plot results\n",
    "    fig = plot_metrics(metrics, model_choice, predictions, actuals)\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    metrics_df = pd.DataFrame([metrics])\n",
    "    metrics_df.to_csv(output_dir / f'{model_choice}_model_metrics.csv', index=False)\n",
    "    \n",
    "    # Save plot\n",
    "    fig.savefig(output_dir / f'{model_choice}_metrics_plot.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluations_on_all_feature_sets(output_dir):\n",
    "    datasets = {\n",
    "        'full': \"2015-2025_dataset_denoised.csv\",\n",
    "        'tyralis': \"2015-2025_dataset_selected_features_tyralis.csv\",\n",
    "        'tang': \"2015-2025_dataset_selected_features_tang.csv\",\n",
    "        'pabuccu': \"2015-2025_dataset_selected_features_pabuccu.csv\"\n",
    "    }\n",
    "    \n",
    "    # Dictionary to store all results\n",
    "    all_results = {}\n",
    "    \n",
    "    # Run each model type\n",
    "    for model_type in ['lopez', 'tyralis']:\n",
    "        print(f\"EVALUATING {model_type.upper()} MODEL\")\n",
    "        model_results = {}\n",
    "        \n",
    "        # Test each feature set\n",
    "        for dataset_name, df in datasets.items():\n",
    "            # Run the model evaluation with the current dataset\n",
    "            metrics = run_model_evaluation(output_dir, model_type, df)\n",
    "            model_results[dataset_name] = metrics\n",
    "            \n",
    "            # Save individual results\n",
    "            pd.DataFrame([metrics]).to_csv(output_dir / f'{model_type}_model_metrics_{dataset_name}.csv', index=False)\n",
    "        \n",
    "        # Store results for this model type\n",
    "        all_results[model_type] = model_results\n",
    "        \n",
    "        # Create comparison DataFrame for this model\n",
    "        comparison_df = pd.DataFrame(model_results).T\n",
    "        comparison_df.to_csv(output_dir / f'{model_type}_model_metrics_comparison.csv')\n",
    "        \n",
    "        # Print comparison summary\n",
    "        print(f\"\\n{model_type.upper()} Model Performance Comparison:\")\n",
    "        print(\"=\"*50)\n",
    "        for dataset_name, metrics in model_results.items():\n",
    "            print(f\"\\n{dataset_name.upper()} Dataset:\")\n",
    "            print(f\"Accuracy: {metrics['Accuracy']*100:.2f}%\")\n",
    "            print(f\"F1 Score: {metrics['F1 Score']*100:.2f}%\")\n",
    "            print(f\"MAE: {metrics['MAE']*100:.2f}%\")\n",
    "            print(f\"RMSE: {metrics['RMSE']*100:.2f}%\")\n",
    "    \n",
    "    # Save overall comparison\n",
    "    overall_comparison = pd.DataFrame({\n",
    "        f\"{model}_{dataset}\": metrics \n",
    "        for model, model_results in all_results.items() \n",
    "        for dataset, metrics in model_results.items()\n",
    "    })\n",
    "    overall_comparison.to_csv(output_dir / 'overall_model_metrics_comparison.csv')\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Modified run_model_evaluation to accept the dataset parameter\n",
    "def run_model_evaluation(output_dir, model_choice, df):   \n",
    "    with tqdm(total=3, desc=f\"Running {model_choice.capitalize()} Model\") as pbar:\n",
    "        if model_choice == 'lopez':\n",
    "            model_results = use_lopez_model(df)\n",
    "            model, _, (X_test, _), scaler, y_train_original, original_test_actuals = model_results\n",
    "            pbar.update(1)\n",
    "        \n",
    "            predictions = model.predict(X_test)[:, 0]\n",
    "            predictions = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "            pbar.update(1)\n",
    "            \n",
    "        elif model_choice == 'ddr':\n",
    "            model_results = use_ddr_model(df)\n",
    "            model, predictions, original_test_actuals, y_train_original = model_results\n",
    "            pbar.update(2)\n",
    "            \n",
    "        else:  # tyralis\n",
    "            model_results = use_tyralis_model(df)\n",
    "            model, predictions, original_test_actuals, y_train_original = model_results\n",
    "            pbar.update(2)\n",
    "        \n",
    "        # Calculate and display metrics\n",
    "        metrics = evaluate_model(predictions, original_test_actuals, y_train_original)\n",
    "        print_metrics(metrics, model_choice)\n",
    "        plot_metrics(metrics, model_choice, predictions, original_test_actuals)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Save metrics\n",
    "        pd.DataFrame([metrics]).to_csv(output_dir / f'{model_choice}_model_metrics.csv', index=False)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "#all_results = run_evaluations_on_all_feature_sets(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6854fa5c7f554dcaa534e9c9f91f541b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Lopez Model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing López's xLSTM-TS model training...\n",
      "\n",
      "Creating sequences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f619a6bdf4c45baa331a6e6020d02b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sequence preparation:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model...\n",
      "\n",
      "Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c002c073534702ab24a7ee2ad969d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/2000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_model_evaluation(output_dir, model_choice='lopez'):\n",
    "   \n",
    "    df_denoised_full = pd.read_csv(output_dir / \"2015-2025_dataset_denoised.csv\", index_col=0, parse_dates=True)\n",
    "    df_features_tyralis = pd.read_csv(output_dir / \"2015-2025_dataset_selected_features_tyralis.csv\", index_col=0, parse_dates=True)\n",
    "    df_features_bourta = pd.read_csv(output_dir / \"2015-2025_dataset_selected_features_boruta.csv\", index_col=0, parse_dates=True)\n",
    "    df_features_lasso = pd.read_csv(output_dir / \"2015-2025_dataset_selected_features_lasso.csv\", index_col=0, parse_dates=True)\n",
    "    \n",
    "    \n",
    "    if model_choice in ['lopez', 'tyralis', 'ddr']:    \n",
    "        with tqdm(total=3, desc=f\"Running {model_choice.capitalize()} Model\") as pbar:\n",
    "            if model_choice == 'lopez':\n",
    "                model_results = use_lopez_model(df_denoised_full)\n",
    "                model, _, (X_test, _), scaler, y_train_original, original_test_actuals = model_results\n",
    "                pbar.update(1)\n",
    "                \n",
    "                predictions = model.predict(X_test)[:, 0]\n",
    "                predictions = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "                pbar.update(1)\n",
    "                \n",
    "            elif model_choice == 'ddr':\n",
    "                model_results = use_ddr_model(df_denoised_full)\n",
    "                model, predictions, original_test_actuals, y_train_original = model_results\n",
    "                pbar.update(2)\n",
    "                \n",
    "            else:  # tyralis\n",
    "                model_results = use_tyralis_model(df_denoised_full)\n",
    "                model, predictions, original_test_actuals, y_train_original = model_results\n",
    "                pbar.update(2)\n",
    "        \n",
    "        metrics = evaluate_model(predictions, original_test_actuals, y_train_original)\n",
    "        print_metrics(metrics, model_choice)\n",
    "        plot_metrics(metrics, model_choice, predictions, original_test_actuals)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        pd.DataFrame([metrics]).to_csv(output_dir / f'{model_choice}_model_metrics.csv', index=False)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "metrics = run_model_evaluation(output_dir, 'lopez')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
