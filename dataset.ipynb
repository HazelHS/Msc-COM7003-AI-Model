{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if pip install isnt working:\n",
    "# !python --version\n",
    "\n",
    "# !curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
    "# !python get-pip.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add citations to functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# requirements:\n",
    "#!python -m \n",
    "! pip install pandas yfinance requests numpy matplotlib PyWavelets seaborn scikit-learn scipy statsmodels tensorflow tqdm ipywidgets boruta\n",
    "# import the necessary libraries.\n",
    "import requests\n",
    "import pywt\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import deque\n",
    "\n",
    "from statsmodels.robust import mad\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.notebook import tqdm  # For Jupyter Notebooks\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Multiply, LSTM, Activation, LayerNormalization, MultiHeadAttention, Dropout, Bidirectional, Add, BatchNormalization, Layer, Conv1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need a dataset for this project, I was unable to find a complete dataset from a reputable source that suited my specific use case. So decided to look into collecting my own data and compiling one myself. \n",
    "# I'll be using the yfinance library to get the stock market data (in future I would like to include more data sources such as those from Tiingo).\n",
    "# I'll start by making a directory for the datasets.csv that we will need to generate.\n",
    "\n",
    "def vscode_progress(iterable, length=None, desc=''):\n",
    "    length = length or len(iterable)\n",
    "    for i, item in enumerate(iterable):\n",
    "        sys.stdout.write(f'\\r{desc} {i+1}/{length} ({((i+1)/length)*100:.1f}%)')\n",
    "        sys.stdout.flush()\n",
    "        yield item\n",
    "    print()\n",
    "    \n",
    "project_root = Path(os.getcwd()) #find file path of the working directory for notebook scripts.\n",
    "output_dir = project_root / \"dataset\"\n",
    "#check to see if the directory exists, make it if it doesn't\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_path = output_dir / \"2015-2025_dataset.csv\"\n",
    "\n",
    "# defining the date range for the dataset. \n",
    "# we'll be using start and end dates multiples times so its best to define them here only once, its likely we'll need to generate seperate time frames for the test/training split.\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2025-02-01\"\n",
    "\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D') # Format: YYYY-MM-DD, \"freq\" is the frequency of dates in this case: ='D' means daily.\n",
    "\n",
    "df = pd.DataFrame(index=date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will need to define a few functions that are needed to be run before I can generate the completed dataset require the for the AI model to work. The first block here is Largely AI assisted with Claude sonnet 3.5: Using the yahoo finance API, this block pulls the historical trading data needed for each individual major global stock exhange for volume and closed price, then converted the currencies to USD using forex data (also from the yfinance API) and returns the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stock_data(symbol: str, currency: str, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    df = yf.download(symbol, start=start_date, end=end_date)\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(index=date_range)\n",
    "        \n",
    "    # Create a new DataFrame with just Close and Volume\n",
    "    result = pd.DataFrame(index=df.index)\n",
    "    result['Close'] = df['Close']\n",
    "    result['Volume'] = df['Volume']\n",
    "    \n",
    "    # Get currency conversion rate if needed\n",
    "    if currency:\n",
    "        fx_data = yf.download(currency, start=start_date, end=end_date)\n",
    "        if not fx_data.empty:\n",
    "            fx_rate = fx_data['Close']\n",
    "            \n",
    "            # Ensure both dataframes have datetime index\n",
    "            result.index = pd.to_datetime(result.index)\n",
    "            fx_rate.index = pd.to_datetime(fx_rate.index)\n",
    "            \n",
    "            # Find common dates between stock and forex data\n",
    "            common_dates = result.index.intersection(fx_rate.index)            \n",
    "            # Keep only dates where we have both stock and forex data\n",
    "            result = result.loc[common_dates]\n",
    "            fx_rate = fx_rate.loc[common_dates]\n",
    "            \n",
    "            # Convert only Close prices to USD using element-wise multiplication\n",
    "            result['Close'] = result['Close'].values * fx_rate.values\n",
    "        else:\n",
    "            return pd.DataFrame(index=date_range)\n",
    "    \n",
    "    # Handle volume based on the index\n",
    "    if symbol in ['^N225', '^HSI']:  # Asian markets often have lower nominal volumes\n",
    "        result['Volume'] = result['Volume'] / 1_000  # Convert to thousands\n",
    "    else:\n",
    "        result['Volume'] = result['Volume'] / 1_000_000  # Convert to millions\n",
    "                \n",
    "    # Add sanity checks for extreme values\n",
    "    if result['Close'].max() > 50000 or result['Close'].min() < 1:\n",
    "        return pd.DataFrame(index=date_range)\n",
    "        \n",
    "    if result['Volume'].min() == 0 or result['Volume'].max() / result['Volume'].min() > 1000:\n",
    "        return pd.DataFrame(index=date_range)\n",
    "        \n",
    "    # Rename columns with symbol prefix\n",
    "    result = result.rename(columns={\n",
    "        'Close': f'{symbol}_Close_USD',\n",
    "        'Volume': f'{symbol}_Volume_M'  # M for millions or thousands for Asian markets\n",
    "    })\n",
    "    \n",
    "    # Reindex to full date range without filling\n",
    "    result = result.reindex(date_range)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block calls the previous blocks function iteratively for each of the 7 stock markets I have decided to include in the data collection. After aquiring the entire daily closed USD price and volume data for each exchange they are averaged together into two combined columns for the previously specified time frame.\n",
    "\n",
    "- \"(Tang et al.) demonstrates strong correlations between global market indices and crypto markets\" \n",
    "- \"The inclusion of Asian markets (Nikkei, Hang Seng) is particularly relevant as studies have shown significant Bitcoin trading volume from these regions\" \n",
    "- \"The SKEW index; research shows its effectiveness in predicting \"black swan\" events in crypto markets, OVX (Oil Volatility) \"Enhancing Bitcoin Price Prediction with Deep Learning\" shows volatility indices are key predictors\"\n",
    "\n",
    "- \"\"Cryptocurrency Valuation: An Explainable AI Approach\" validates the use of on-chain metrics as fundamental indicators\" - \"Hash rate and mining difficulty are particularly important as they reflect network security and mining economics\"\n",
    "- \"Transaction metrics provide insight into network usage and adoption\"\n",
    "\n",
    "- \"Deep Learning for Financial Applications: A Survey\" supports the inclusion of traditional safe-haven assets like gold, The DXY (Dollar Index) inclusion is supported by research showing strong inverse correlations with Bitcoin during certain market conditions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_market_stock_data(start_date, end_date):\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    result_df = pd.DataFrame(index=date_range)\n",
    "    \n",
    "    # Define indices with their currencies\n",
    "    indices = {\n",
    "        'GDAXI': {'symbol': '^GDAXI', 'currency': 'EURUSD=X'},    # Germany DAX\n",
    "        'IXIC': {'symbol': '^IXIC', 'currency': None},            # NASDAQ (already in USD)\n",
    "        'DJI': {'symbol': '^DJI', 'currency': None},              # Dow Jones (already in USD)\n",
    "        'N225': {'symbol': '^N225', 'currency': 'JPYUSD=X'},      # Nikkei\n",
    "        'STOXX50E': {'symbol': '^STOXX', 'currency': 'EURUSD=X'}, # Euro STOXX 50\n",
    "        'HSI': {'symbol': '^HSI', 'currency': 'HKDUSD=X'},        # Hang Seng\n",
    "        'FTSE': {'symbol': '^FTSE', 'currency': 'GBPUSD=X'}       # FTSE 100\n",
    "    }\n",
    "    \n",
    "    # Fetch data for all indices\n",
    "    combined_df = pd.DataFrame(index=date_range)\n",
    "    \n",
    "    for name, info in indices.items():\n",
    "        index_data = fetch_stock_data(info['symbol'], info['currency'], start_date, end_date)\n",
    "        if not index_data.empty and len(index_data.columns) > 0:\n",
    "            combined_df = pd.concat([combined_df, index_data], axis=1)\n",
    "    \n",
    "    # Calculate global averages\n",
    "    close_cols = [col for col in combined_df.columns if str(col).endswith('_Close_USD')]\n",
    "    volume_cols = [col for col in combined_df.columns if str(col).endswith('_Volume_M')]\n",
    "    \n",
    "    if close_cols and volume_cols:\n",
    "        result_df = pd.DataFrame(index=date_range)\n",
    "        result_df['Global averaged stocks(USD)'] = combined_df[close_cols].mean(axis=1, skipna=True)\n",
    "        result_df['Global averaged stocks (volume)'] = combined_df[volume_cols].mean(axis=1, skipna=True)\n",
    "        \n",
    "        return result_df\n",
    "    return pd.DataFrame(index=date_range, columns=['Global averaged stocks(USD)', 'Global averaged stocks (volume)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function works similar to the previous, collecting the US Dollar index (DXY) and the gold futures data from Yahoo Finance. Along with the Bitcoin-USD paring with its respective volume data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_currency_metrics(start_date, end_date):   \n",
    "    result_df = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "    # Get DXY (US Dollar Index)\n",
    "    dxy = yf.download(\"DX-Y.NYB\", start=start_date, end=end_date)\n",
    "    result_df['Currency US Dollar Index'] = dxy['Close']\n",
    "    \n",
    "    # Get Gold Futures\n",
    "    gold = yf.download(\"GC=F\", start=start_date, end=end_date)\n",
    "    result_df['Currency Gold Futures'] = gold['Close']\n",
    "    \n",
    "    # Get Bitcoin price data\n",
    "    btc = yf.download(\"BTC-USD\", start=start_date, end=end_date)\n",
    "    result_df['BTC/USD'] = btc['Close']\n",
    "    \n",
    "    # Get Bitcoin price data\n",
    "    btc = yf.download(\"BTC-USD\", start=start_date, end=end_date)\n",
    "    result_df['BTC Volume'] = btc['Volume']\n",
    "    \n",
    "    # Calculate Gold/BTC Ratio where BTC price is not zero or null\n",
    "    result_df['Gold/BTC Ratio'] = result_df['Currency Gold Futures'].div(result_df['BTC/USD'].replace(0, float('nan')))\n",
    "    result_df['Gold/BTC Ratio'] = result_df['Gold/BTC Ratio']\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same again here, with some additional assistence from Clude AI, and using the blockchain.info API, this function collects the individual \"on chain\" metrics that were chosen for inclusion in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blockchain_metric(metric_name, start_date, end_date):\n",
    "    \n",
    "    # Fetch single blockchain metric one by one, from the Blockchain.info API.\n",
    " \n",
    "    # Convert dates to timestamps\n",
    "    start_ts = int(datetime.strptime(start_date, \"%Y-%m-%d\").timestamp())\n",
    "    end_ts = int(datetime.strptime(end_date, \"%Y-%m-%d\").timestamp())\n",
    "    \n",
    "    # Fetch data from API with updated URL structure\n",
    "    url = f\"{\"https://api.blockchain.info\"}/{metric_name}\"\n",
    "    params = {\n",
    "        \"timespan\": \"all\",\n",
    "        \"start\": start_ts,\n",
    "        \"end\": end_ts,\n",
    "        \"format\": \"json\",\n",
    "        \"sampled\": \"true\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        return pd.Series(index=pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "        \n",
    "    data = response.json()\n",
    "    \n",
    "    # Check if the response has the expected structure\n",
    "    if not isinstance(data, dict) or 'values' not in data:\n",
    "        return pd.Series(index=pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "    \n",
    "    # Process the values\n",
    "    values = []\n",
    "    timestamps = []\n",
    "    for entry in data['values']:\n",
    "        if isinstance(entry, (list, tuple)) and len(entry) >= 2:\n",
    "            timestamps.append(entry[0])\n",
    "            values.append(float(entry[1]))\n",
    "        elif isinstance(entry, dict) and 'x' in entry and 'y' in entry:\n",
    "            timestamps.append(entry['x'])\n",
    "            values.append(float(entry['y']))\n",
    "    \n",
    "    if not values:\n",
    "        return pd.Series(index=pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "    \n",
    "    # Create DataFrame and handle data types\n",
    "    df = pd.DataFrame({'timestamp': timestamps, 'value': values})\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s').dt.normalize()\n",
    "    df = df.drop_duplicates('timestamp', keep='last')\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    # Handle potential overflow for large numbers\n",
    "    df['value'] = df['value'].astype('float64')\n",
    "    \n",
    "    # Reindex to ensure consistent date range\n",
    "    return df['value'].reindex(date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calls the previous block iteratively for each metric of \"on chain\" data, it is unclear to me which if any of these metrics have high enough correlation with the BTC-USD price movement to warrent final selection. As a reult I decided to include more than I would expect are required in the interest of thoroughness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onchain_metrics(start_date, end_date):\n",
    "    result_df = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "    # Define metrics and their API endpoints with updated paths\n",
    "    metrics = {\n",
    "        'Onchain Active Addresses': 'charts/n-unique-addresses',\n",
    "        'Onchain Transaction Count': 'charts/n-transactions',\n",
    "        'Onchain Hash Rate (GH/s)': 'charts/hash-rate',\n",
    "        'Onchain Mining Difficulty': 'charts/difficulty',\n",
    "        'Onchain Transaction Fees (BTC)': 'charts/transaction-fees',\n",
    "        'Onchain Median Confirmation Time (min)': 'charts/median-confirmation-time'\n",
    "    }\n",
    "    \n",
    "    # Fetch each metric\n",
    "    for col_name, metric_name in metrics.items():\n",
    "        series = get_blockchain_metric(metric_name, start_date, end_date)\n",
    "        result_df[col_name] = series\n",
    "        \n",
    "        # Handle missing values for each metric appropriately\n",
    "        if col_name in ['Onchain Mining Difficulty', 'Onchain Hash Rate (GH/s)']:\n",
    "            result_df[col_name] = result_df[col_name]\n",
    "        elif col_name in ['Onchain Transaction Count', 'Onchain Active Addresses']:\n",
    "            result_df[col_name] = result_df[col_name]\n",
    "        else:\n",
    "            result_df[col_name] = result_df[col_name]\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These additional metrics track the volatility of the S&P500 stock market and the crude oil volatility index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_volatility_indices(start_date, end_date):    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Get CBOE SKEW Index from Yahoo Finance\n",
    "    skew = yf.download(\"^SKEW\", start=start_date, end=end_date)\n",
    "    df['Volatility_CBOE SKEW Index'] = skew['Close']\n",
    "    \n",
    "    # Get VIX\n",
    "    vix = yf.download(\"^VIX\", start=start_date, end=end_date)\n",
    "    df['Volatility_CBOE Volatility Index (VIX)'] = vix['Close']\n",
    "    \n",
    "    # Get Oil VIX\n",
    "    ovx = yf.download(\"^OVX\", start=start_date, end=end_date)\n",
    "    df['Volatility_Crude Oil Volatility Index (OVX)'] = ovx['Close']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@100trillionUSD/modeling-bitcoins-value-with-scarcity-91fa0fc03e25\n",
    "\n",
    "https://medium.com/@100trillionUSD/bitcoin-stock-to-flow-cross-asset-model-50d260feed12\n",
    "\n",
    "https://newhedge.io/bitcoin/stock-to-flow\n",
    "\n",
    "Here I would like to include and calculate the \"Stock to Flow\" model intially conceptualized by \"PlanB\".\n",
    "\n",
    "Cluade AI helped with the S2F model calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stock_to_flow(start_date, end_date):\n",
    "    \n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    s2f_df = pd.DataFrame(index=date_range)\n",
    "    \n",
    "    # API parameters\n",
    "    params = {\n",
    "        \"timespan\": \"all\",\n",
    "        \"start\": int(pd.Timestamp(start_date).timestamp()),\n",
    "        \"end\": int(pd.Timestamp(end_date).timestamp()),\n",
    "        \"format\": \"json\",\n",
    "        \"sampled\": \"false\"\n",
    "    }\n",
    "    \n",
    "    # Get total supply\n",
    "    response = requests.get(\"https://api.blockchain.info/charts/total-bitcoins\", params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()['values']\n",
    "        df = pd.DataFrame(data, columns=['x', 'y'])\n",
    "        df['timestamp'] = pd.to_datetime(df['x'], unit='s').dt.normalize()\n",
    "        stock = df.groupby('timestamp')['y'].mean()\n",
    "        stock = stock.reindex(date_range).interpolate(method='linear')\n",
    "        \n",
    "        # Calculate flow based on Bitcoin halving schedule\n",
    "        s2f_df['timestamp'] = date_range\n",
    "        s2f_df['block height'] = ((s2f_df['timestamp'] - pd.Timestamp('2009-01-03')) / pd.Timedelta(minutes=10)).astype(int) # \"genesis block\" date (January 3, 2009) the first BTC block to be mined.\n",
    "        \n",
    "        # Calculate daily block rewards based on halving schedule\n",
    "        def get_block_reward(block_height):\n",
    "            halvings = block_height // 210000 # Roughly every 4 years there is a BTC \"halving event\" (when the mining rewards are halved) this is every 210,000 blocks.\n",
    "            return 50 / (2 ** halvings)\n",
    "        \n",
    "        s2f_df['daily production'] = s2f_df['block height'].apply(get_block_reward) * 144  # Timing by 144 gives us the total daily Bitcoin production (24 hours * 60 minutes) / 10 minutes = 144 blocks per day, \".apply(get_block_reward)\" calculates the reward for each block height.\n",
    "        \n",
    "        # Calculate S2F ratio (stock divided by yearly flow)\n",
    "        s2f_df['s2f ratio'] = stock / (s2f_df['daily production'] * 365)\n",
    "        \n",
    "        # Calculate expected price using S2F model\n",
    "        # Using the formula: Price = exp(-1.84) * S2F^3.36\n",
    "        s2f_df['S2F Model'] = np.exp(-1.84) * (s2f_df['s2f ratio'] ** 3.36)\n",
    "        \n",
    "        # Convert to USD and handle any extreme values\n",
    "        s2f_df['S2F Model'] = s2f_df['S2F Model']\n",
    "        \n",
    "        return s2f_df[['S2F Model']]\n",
    "    \n",
    "    return pd.DataFrame(index=date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main fuction for compiling, saving and ordering all the columns required for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all data components\n",
    "components = [\n",
    "    ('Stockmarket', get_market_stock_data(start_date, end_date)),\n",
    "    ('Currency Metrics', get_currency_metrics(start_date, end_date)),\n",
    "    ('On-chain Metrics', get_onchain_metrics(start_date, end_date)),\n",
    "    ('Volatility Indices', get_volatility_indices(start_date, end_date)),\n",
    "    ('S2F Model', calculate_stock_to_flow(start_date, end_date))\n",
    "    ]\n",
    "\n",
    "# Combine all components\n",
    "for name, component_df in components:\n",
    "    if component_df is not None and not component_df.empty:\n",
    "        for column in component_df.columns:\n",
    "            df[column] = component_df[column]\n",
    "\n",
    "# Reorder columns to group related metrics together\n",
    "column_order = [\n",
    "    'Global averaged stocks(USD)',\n",
    "    'Global averaged stocks (volume)',\n",
    "    'Currency US Dollar Index',\n",
    "    'Currency Gold Futures',\n",
    "    'Volatility_CBOE SKEW Index',\n",
    "    'Volatility_CBOE Volatility Index (VIX)',\n",
    "    'Volatility_Crude Oil Volatility Index (OVX)',\n",
    "    'Gold/BTC Ratio',\n",
    "    'BTC/USD',\n",
    "    'BTC Volume',\n",
    "    'S2F Model',\n",
    "    'Onchain Active Addresses',\n",
    "    'Onchain Transaction Count',\n",
    "    'Onchain Hash Rate (GH/s)',\n",
    "    'Onchain Mining Difficulty',\n",
    "    'Onchain Transaction Fees (BTC)',\n",
    "    'Onchain Median Confirmation Time (min)'\n",
    "]\n",
    "\n",
    "# Reorder the columns\n",
    "df = df[column_order]\n",
    "\n",
    "# Save the dataset\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "df.to_csv(output_path)\n",
    "print(f\"Dataset saved to {output_path}\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, provided that the code all runs correctly. We should have a dataset that is largly complete, except for missing entries in the weekends for stockmarket data and every other day in the blckchain metrics for BTC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved dataset\n",
    "df = pd.read_csv(output_path, index_col=0, parse_dates=True)\n",
    "\n",
    "# Interpolate each column based on its data type\n",
    "for column in df.columns:\n",
    "    # For all other metrics (prices, volumes, etc), use linear interpolation\n",
    "    df[column] = df[column].interpolate(method='linear', limit=5)\n",
    "\n",
    "# We will need to remove the first row of data as it contains null entries and there is no way to interpolate it.\n",
    "df = df.iloc[1:]  \n",
    "\n",
    "# Save the interpolated dataset with a new name\n",
    "interpolated_path = output_dir / \"2015-2025_dataset_interpolated.csv\"\n",
    "df.to_csv(interpolated_path)\n",
    "print(f\"\\nInterpolated dataset saved to {interpolated_path}\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset will need to have all of the values normalized here as part of the preprocessing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_dir / \"2015-2025_dataset_interpolated.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Create a copy of the dataframe and normalize all columns\n",
    "df_normalized = pd.DataFrame(\n",
    "    scaler.fit_transform(df),\n",
    "    columns=df.columns,\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "# Save as new\n",
    "df_normalized.to_csv(output_dir / \"2015-2025_dataset_normalized.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this block creates a simple polt of each graph, allowing us to check and compare with online sources such as tradingview for accuracy. If there is anything major missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_dir / \"2015-2025_dataset_normalized.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Create subplots for each column\n",
    "n_cols = len(df.columns)\n",
    "fig, axes = plt.subplots(n_cols, 1, figsize=(15, 7*n_cols))\n",
    "\n",
    "# Plot each column\n",
    "for i, column in enumerate(df.columns):\n",
    "    # Create the plot on the corresponding subplot\n",
    "    axes[i].plot(df.index, df[column])\n",
    "    \n",
    "    # Customize each subplot\n",
    "    axes[i].set_title(f'{column}', fontsize=14)\n",
    "    axes[i].set_xlabel('Date', fontsize=12)\n",
    "    axes[i].set_ylabel('Value', fontsize=12)\n",
    "    axes[i].grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show all plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://dx.doi.org/10.3390/info12100388 - Method used for denoising [decription of how to do \"wavelet decomposition\" and/or \"wavelet\" denoising]\n",
    "\n",
    "Instead of handling outliers in a more traditional approach, since financial data is real world data and I am not versed enough in finance and economics to understand fully what kind of data could or should classify as \"outliers\" with much confidence. I instead would prefer to try \"denoising\" from the literacture I've found on the similar projects.\n",
    "\n",
    "\"Wavelet transforms analyse stock market trends over different periods and often show superior performance. Peng et al. (2021) demonstrated that combining multiresolution wavelet reconstruction with deep learning significantly improves medium-term stock prediction accuracy, achieving a 75% hit rate for US stocks. Another study introduced the Adaptive Multi-Scale Wavelet Neural Network (AMSW-NN), which performs well but depends on dataset quality (Ouyang et al., 2021).\" - https://arxiv.org/html/2408.12408v1#S3.SS3 TL;DR \"multiresolution wavelet reconstruction\"  is very good and preferable over \"adaptive Multi-Scale Wavelet Neural Network (AMSW-NN)\" due to its increased dependance on quality data. - \"multiresolution wavelet\" method explained in greater detail here: Peng et al. (2021) [https://www.mdpi.com/2078-2489/12/10/388] - only had a 0.63% improvement with much greater complexity, Best to keep things simple for both my sanity in programming and the \"computational efficienty\" of Pan Tang, Cheng Tang and Keren Wang's [https://doi.org/10.1002/for.3071] apporach:\n",
    "\n",
    "\n",
    "\"LSTM (long short-term memory), we propose a hybrid model of wavelet transform (WT) and multi-input LSTM\"\n",
    "\n",
    "LSTM + WT = flexible model.\n",
    "\n",
    "\"level 1 decomposition with db4 mother wavelet to eliminate noise. originall used in image processing. it is more widely\n",
    "used in stock price forecasting (Aussem, 1998; Alru-maih & Al-Fawzan, 2002; Caetano & Yoneyama, 2007; Huang, 2011)\"\n",
    "\n",
    "y[n] = Σ x[k]g[2n-k]  # Low-pass filter\n",
    "y[n] = Σ x[k]h[2n-k]  # High-pass filter\n",
    "\n",
    "\"db4\" stands for \"Daubechies-4\" wavelet\n",
    "It's called a \"mother\" wavelet because it's the original pattern that gets scaled and shifted\n",
    "The \"4\" represents the number of vanishing moments (a measure of complexity)\n",
    "\n",
    "Tang's Approach (2024):\n",
    "Simple level 1 decomposition with db4\n",
    "Complete zeroing of high-frequency coefficients\n",
    "Claimed Results:\n",
    "> Test accuracy increased from 51.72% - 57.76% to 64.66% - 72.19% after applying their denoising method\n",
    "> Focused on LSTM model performance improvement\n",
    "López Gil/Peng's Approach (2021, 2024):\n",
    "Multi-level (3-level) decomposition with db4\n",
    "Adaptive thresholding at each level\n",
    "López Gil's Results:\n",
    "> Achieved 72.82% test accuracy and 73.16% F1 score on the EWZ daily dataset\n",
    "Peng's Original Results:\n",
    "> Achieved 75% hit rate for US stocks\n",
    "\n",
    "The following denoising code was block was largely assisted by deepseek-r1, based on Peng's paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_denoising(df, wavelet='db4', level=3):  # Changed to 3 levels\n",
    "    df_denoised = df.copy()\n",
    "    \n",
    "    for column in df.columns:\n",
    "        # 1. Multi-level decomposition\n",
    "        coeffs = pywt.wavedec(df[column].values, wavelet, level=level)\n",
    "        \n",
    "        # 2. Calculate noise threshold (López Gil's method)\n",
    "        sigma = mad(coeffs[-1])\n",
    "        n = len(df[column])\n",
    "        threshold = sigma * np.sqrt(2 * np.log(n)) * 0.8  # Added 0.8 factor for more conservative thresholding\n",
    "        \n",
    "        # 3. Apply soft thresholding\n",
    "        coeffs_modified = [coeffs[0]]\n",
    "        for i in range(1, len(coeffs)):\n",
    "            coeffs_modified.append(pywt.threshold(coeffs[i], threshold, 'soft'))\n",
    "        \n",
    "        # 4. Reconstruct signal\n",
    "        denoised_data = pywt.waverec(coeffs_modified, wavelet)\n",
    "        \n",
    "        # 5. Handle boundary effects (can keep this as it's a good practice)\n",
    "        if len(denoised_data) > len(df):\n",
    "            denoised_data = denoised_data[:len(df)]\n",
    "        elif len(denoised_data) < len(df):\n",
    "            denoised_data = np.pad(denoised_data, (0, len(df)-len(denoised_data)), 'edge')\n",
    "            \n",
    "        df_denoised[column] = denoised_data\n",
    "    \n",
    "    return df_denoised\n",
    "\n",
    "def plot_denoising_results(original_data, denoised_data, column_name):\n",
    "    noise = original_data[column_name] - denoised_data[column_name]\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Original and denoised data\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(original_data.index, original_data[column_name], \n",
    "             label='Original', alpha=0.5)\n",
    "    plt.plot(denoised_data.index, denoised_data[column_name], \n",
    "             label='Denoised', alpha=0.8)\n",
    "    plt.title(f'Denoising Results for {column_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Removed noise\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(original_data.index, noise, label='Removed Noise', \n",
    "             alpha=0.5, color='red')\n",
    "    plt.title('Removed Noise Component')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "df = pd.read_csv(output_dir / \"2015-2025_dataset_normalized.csv\", \n",
    "                    index_col=0, parse_dates=True)\n",
    "\n",
    "df_denoised = wavelet_denoising(df)\n",
    "\n",
    "for column in df.columns:\n",
    "    plot_denoising_results(df, df_denoised, column)\n",
    "\n",
    "df_denoised.to_csv(output_dir / \"2015-2025_dataset_denoised.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for feature selection:\n",
    "\n",
    "[https://arxiv.org/pdf/2303.02223v2] - Pabuccu's - \"Feature Selection for Forecasting\" - Actually used \"FSA\" and \"boruta\" for validation: https://github.com/scikit-learn-contrib/boruta_py\n",
    "\n",
    "[https://doi.org/10.1002/for.3071] - Teng's did not feature select, he manually did it based on domain \"expertize\" - https://doi.org/10.1002/for.3071 - \"Stock movement prediction: A multi-input LSTM approach\"\n",
    "\n",
    "[https://www.mdpi.com/1999-4893/10/4/114] - tyralis's method - \"Variable Selection in Time Series Forecasting Using Random Forests\" - https://doi.org/10.3390/a10040114 - using both his prediction and feature selection method.\n",
    "\n",
    "LASSO feature selection method: https://www.tandfonline.com/doi/epdf/10.1080/09540091.2023.2286188?needAccess=true\n",
    "\n",
    "The original text block overestimated the mathematical parity of the implementations. This updated analysis provides a more accurate assessment of the actual implementation fidelity to the original papers.\n",
    "\n",
    "the code for this section, as well as help with understanding the underlying papers math, methods and processes were completed with the aid of both Clude sonnet 3.5 and deepseek r-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and plotting the feature selection of choice:\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
    "    return df\n",
    "\n",
    "# Use 'Score' column consistently - AI contributed:\n",
    "def plot_feature_importance(scores_df, method_name):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get selected features and their scores\n",
    "    selected_scores = scores_df[scores_df['Selected']]\n",
    "    score_col = 'Score' if 'Score' in selected_scores.columns else 'Importance'\n",
    "    \n",
    "    # Create horizontal bar plot\n",
    "    plt.barh(range(len(selected_scores)), selected_scores[score_col])\n",
    "    plt.yticks(range(len(selected_scores)), selected_scores['Feature'])\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title(f'Feature Importance Scores - {method_name}')\n",
    "    plt.grid(True, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def save_selected_features(df, selected_features, target, method_name):\n",
    "    final_features = selected_features + [target]\n",
    "    df_selected = df[final_features]\n",
    "    output_path = output_dir / f\"2015-2025_dataset_selected_features_{method_name}.csv\"\n",
    "    df_selected.to_csv(output_path)\n",
    "    return df_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forrest Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tyralis_selection(df, target='BTC/USD', window_size=30):\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    \n",
    "    # Paper's RF configuration\n",
    "    rf = RandomForestRegressor(n_estimators=50, #500\n",
    "                             max_features='sqrt',\n",
    "                             random_state=42) \n",
    "    \n",
    "    n_windows = len(X) - window_size\n",
    "    importance_matrix = np.zeros((n_windows, X.shape[1]))\n",
    "    \n",
    "    # Process windows sequentially as per paper\n",
    "    with tqdm(total=n_windows, desc='Processing windows') as pbar:\n",
    "        for i in range(n_windows):\n",
    "            # Paper's window approach\n",
    "            window_data = X.iloc[i:i + window_size]\n",
    "            window_target = y.iloc[i:i + window_size]\n",
    "            \n",
    "            # Fit RF on window\n",
    "            rf.fit(window_data, window_target)\n",
    "            baseline_score = rf.score(window_data, window_target)\n",
    "            \n",
    "            # Sequential permutation importance (paper's method)\n",
    "            for j in range(X.shape[1]):\n",
    "                X_perm = window_data.copy()\n",
    "                X_perm.iloc[:,j] = np.random.permutation(X_perm.iloc[:,j])\n",
    "                perm_score = rf.score(X_perm, window_target)\n",
    "                importance_matrix[i,j] = baseline_score - perm_score\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Paper's quantile threshold (75th percentile)\n",
    "    threshold = np.quantile(importance_matrix, 0.75, axis=0)\n",
    "    selected_features = X.columns[importance_matrix.mean(axis=0) > threshold].tolist()\n",
    "    \n",
    "    return selected_features, pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Score': importance_matrix.mean(axis=0),\n",
    "        'Selected': importance_matrix.mean(axis=0) > threshold\n",
    "    }).sort_values('Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_feature_selection(df, target='BTC/USD', alpha='auto'):\n",
    "    from sklearn.linear_model import LassoCV, Lasso\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Determine optimal alpha if auto\n",
    "    if alpha == 'auto':\n",
    "        lasso_cv = LassoCV(cv=5, random_state=42)\n",
    "        lasso_cv.fit(X_scaled, y)\n",
    "        alpha = lasso_cv.alpha_\n",
    "    \n",
    "    # Fit LASSO\n",
    "    lasso = Lasso(alpha=alpha, random_state=42)\n",
    "    lasso.fit(X_scaled, y)\n",
    "    \n",
    "    # Get selected features\n",
    "    selected_features = X.columns[lasso.coef_ != 0].tolist()\n",
    "    \n",
    "    # Create importance scores\n",
    "    importance_scores = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Score': np.abs(lasso.coef_),\n",
    "        'Selected': lasso.coef_ != 0\n",
    "    }).sort_values('Score', ascending=False)\n",
    "    \n",
    "    print(f\"\\nSelected {len(selected_features)} features\")\n",
    "    print(\"\\nTop features and weights:\")\n",
    "    print(importance_scores[importance_scores['Selected']].head())\n",
    "    \n",
    "    return selected_features, importance_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boruta's method: You can adjust the parameters like max_depth, n_estimators, and alpha based on your specific needs. The verbose=2 parameter will show you the progress of feature selection.\n",
    "\n",
    "Please suggest me code to replace this pabuccu selection function with the one mentioned in: @https://arxiv.org/pdf/2303.02223v2  They helpfully provided a github link to use someone else library, I would like to implement their method if possible (found here:@https://github.com/scikit-learn-contrib/boruta_py ). Thank you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boruta_selection(df, target='BTC/USD', max_iter=100, importance_threshold=0.01):\n",
    "    from boruta import BorutaPy\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from tqdm.notebook import tqdm\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "    \n",
    "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "    \n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    \n",
    "    rf = RandomForestRegressor(\n",
    "        n_jobs=-1,\n",
    "        max_depth=7,\n",
    "        n_estimators=250,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    boruta = BorutaPy(\n",
    "        rf,\n",
    "        n_estimators='auto',\n",
    "        max_iter=max_iter,\n",
    "        perc=98,\n",
    "        alpha=0.001,\n",
    "        two_step=True,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    with tqdm(total=1, desc=\"Running Boruta Selection\") as pbar:\n",
    "        boruta.fit(X.values, y.values)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    if hasattr(boruta, 'importance_history_'):\n",
    "        feature_importance = boruta.importance_history_.mean(axis=0)[:X.shape[1]]\n",
    "        scores = feature_importance / np.max(feature_importance)\n",
    "        \n",
    "        min_nonzero = np.min(feature_importance[feature_importance > 0])\n",
    "        relative_scores = np.zeros_like(feature_importance)\n",
    "        nonzero_mask = feature_importance > 0\n",
    "        relative_scores[nonzero_mask] = 10 * (np.log1p(feature_importance[nonzero_mask]) - np.log1p(min_nonzero)) / (np.log1p(np.max(feature_importance)) - np.log1p(min_nonzero))\n",
    "    else:\n",
    "        scores = np.ones(len(X.columns))\n",
    "        relative_scores = np.ones(len(X.columns))\n",
    "        feature_importance = np.ones(len(X.columns))\n",
    "    \n",
    "    significant_features = scores >= importance_threshold\n",
    "    \n",
    "    importance_scores = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': scores.round(4),\n",
    "        'Relative_Importance': relative_scores.round(2),\n",
    "        'Raw_Score': feature_importance.round(6),\n",
    "        'Selected': significant_features,\n",
    "        'Score': scores.round(4)  # Add Score column for compatibility\n",
    "    })\n",
    "    \n",
    "    importance_scores = importance_scores.sort_values('Importance', ascending=False)\n",
    "    selected_features = importance_scores[importance_scores['Selected']]['Feature'].tolist()\n",
    "    \n",
    "    print(f\"\\nSelected {len(selected_features)} features\")\n",
    "    print(\"\\nSelected features by importance:\")\n",
    "    print(importance_scores[importance_scores['Selected']].to_string(index=False))\n",
    "    \n",
    "    return selected_features, importance_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main execution block, run all three or only one of the feature at a time if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_feature_selection(method_choice):\n",
    "    df = pd.read_csv(output_dir / \"2015-2025_dataset_denoised.csv\", index_col=0, parse_dates=True)\n",
    "    target = 'BTC/USD'\n",
    "    \n",
    "    methods = {\n",
    "        'tyralis': {\n",
    "            'func': tyralis_selection,\n",
    "            'params': {'window_size': 30}\n",
    "        },\n",
    "        'lasso': {\n",
    "            'func': lasso_feature_selection,\n",
    "            'params': {}\n",
    "        },\n",
    "        'boruta': {\n",
    "            'func': boruta_selection,\n",
    "            'params': {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Execute selected method\n",
    "    if method_choice in ['tyralis', 'lasso', 'boruta']:\n",
    "        selected_features, importance_scores = methods[method_choice]['func'](\n",
    "            df, target, **methods[method_choice]['params']\n",
    "        )\n",
    "        \n",
    "        # Ensure consistent column naming\n",
    "        if 'Importance' in importance_scores.columns:\n",
    "            importance_scores['Score'] = importance_scores['Importance']\n",
    "        \n",
    "        # Save results and plot\n",
    "        shape = save_selected_features(df, selected_features, target, method_choice)\n",
    "        plot_feature_importance(importance_scores, method_choice)\n",
    "        \n",
    "        print(f\"\\nSelected {len(selected_features)} features using {method_choice.upper()} method\")\n",
    "        print(\"Features:\", ', '.join(selected_features))\n",
    "        print(f\"Output shape: {shape}\")\n",
    "\n",
    "#run_feature_selection('tyralis')\n",
    "#run_feature_selection('lasso')\n",
    "#run_feature_selection('boruta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is the model, 3 AI algorithms were chosem for this: \n",
    "\n",
    "1. Deep direct reinforcement learning:\n",
    "2. Random forrest:\n",
    "3. xLSTM-TS (Supervised Learning - Regression):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tyralis's model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tyralis_prediction_model(X_train, y_train, X_test=None, window_size=1):\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=1,  # Reduced for testing, 50 is needed for full run, with a window size of 30.\n",
    "        max_features='sqrt',\n",
    "        criterion='squared_error',  \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # If X_test is provided, we're doing out-of-sample prediction\n",
    "    if X_test is not None:\n",
    "        rf.fit(X_train, y_train)\n",
    "        predictions = rf.predict(X_test)\n",
    "        model = {'rf': rf}\n",
    "        return model, predictions\n",
    "    \n",
    "    # Otherwise, do rolling window prediction\n",
    "    predictions = []\n",
    "    feature_importance_history = []\n",
    "    \n",
    "    # Rolling window prediction\n",
    "    for i in range(len(X_train) - window_size):\n",
    "        # Get window data\n",
    "        X_window = X_train[i:i+window_size]\n",
    "        y_window = y_train[i:i+window_size]\n",
    "        \n",
    "        # Fit RF on current window\n",
    "        rf.fit(X_window, y_window)\n",
    "        \n",
    "        # Store feature importance for this window\n",
    "        feature_importance_history.append(rf.feature_importances_)\n",
    "        \n",
    "        # Predict next point\n",
    "        if i + window_size < len(X_train):\n",
    "            next_X = X_train[i+window_size].reshape(1, -1)\n",
    "            pred = rf.predict(next_X)[0]\n",
    "            predictions.append(pred)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    model = {\n",
    "        'rf': rf,\n",
    "        'feature_importance_history': np.array(feature_importance_history)\n",
    "    }\n",
    "    \n",
    "    return model, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lopez's model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientClippingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, max_norm=1.0):\n",
    "        super().__init__()\n",
    "        self.max_norm = max_norm\n",
    "        \n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        # Get the optimizer\n",
    "        optimizer = self.model.optimizer\n",
    "        \n",
    "        # Add gradient clipping to the optimizer\n",
    "        if not hasattr(optimizer, '_was_clipping_added'):\n",
    "            optimizer.clipnorm = self.max_norm\n",
    "            optimizer._was_clipping_added = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function combining MSE and directional accuracy as per paper\n",
    "    \"\"\"\n",
    "    # MSE component\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    \n",
    "    # Directional component\n",
    "    # Get directional movement (up/down) for true and predicted values\n",
    "    y_true_direction = tf.cast(y_true[:, 1:] > y_true[:, :-1], tf.float32)\n",
    "    y_pred_direction = tf.cast(y_pred[:, 1:] > y_pred[:, :-1], tf.float32)\n",
    "    \n",
    "    # Binary cross-entropy for directional accuracy\n",
    "    directional = tf.reduce_mean(tf.keras.losses.binary_crossentropy(\n",
    "        y_true_direction, \n",
    "        y_pred_direction,\n",
    "        from_logits=False\n",
    "    ))\n",
    "    \n",
    "    # Combine losses with weighting (α=0.7 as per paper)\n",
    "    alpha = 0.7\n",
    "    return alpha * mse + (1 - alpha) * directional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sLSTM_block(inputs, embedding_dim=64, kernel_size=2, num_heads=2, ff_factor=1.1):\n",
    "    # Layer normalization\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    \n",
    "    # Scalar memory LSTM with convolutional processing\n",
    "    # Simulate sLSTM behavior using Conv1D + LSTM with different configuration\n",
    "    conv = Conv1D(filters=embedding_dim, kernel_size=kernel_size, padding='same')(x)\n",
    "    lstm = LSTM(embedding_dim, return_sequences=True, recurrent_activation='sigmoid')(conv)\n",
    "    \n",
    "    # Multi-head attention mechanism\n",
    "    attention = MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=embedding_dim // num_heads,\n",
    "        value_dim=embedding_dim // num_heads\n",
    "    )(lstm, lstm, lstm)\n",
    "    \n",
    "    # Feedforward network with projection factor\n",
    "    ff_dim = int(embedding_dim * ff_factor)\n",
    "    ff = Dense(ff_dim, activation='relu')(attention)\n",
    "    ff = Dense(embedding_dim)(ff)\n",
    "    \n",
    "    # Residual connection\n",
    "    output = Add()([inputs, ff])\n",
    "    \n",
    "    return output\n",
    "\n",
    "def create_mLSTM_block(inputs, embedding_dim=64, kernel_size=4, projection_size=2, num_heads=2):\n",
    "    # Layer normalization\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    \n",
    "    # Matrix memory LSTM with convolutional processing\n",
    "    # Simulate mLSTM behavior using Conv1D + LSTM\n",
    "    conv = Conv1D(filters=embedding_dim, kernel_size=kernel_size, padding='same')(x)\n",
    "    lstm = LSTM(embedding_dim, return_sequences=True)(conv)\n",
    "    \n",
    "    # Multi-head attention mechanism\n",
    "    attention = MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=embedding_dim // num_heads,\n",
    "        value_dim=embedding_dim // num_heads\n",
    "    )(lstm, lstm, lstm)\n",
    "    \n",
    "    # Projection to match dimensions\n",
    "    projection = Dense(embedding_dim * projection_size)(attention)\n",
    "    projection = Dense(embedding_dim)(projection)  # Project back to embedding_dim\n",
    "    \n",
    "    # Residual connection\n",
    "    output = Add()([inputs, projection])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xLSTM_TS_model(input_shape, embedding_dim=64, output_size=7):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Initial linear projection to embedding dimension\n",
    "    x = Dense(embedding_dim)(inputs)\n",
    "    \n",
    "    # xLSTM block stack as per Table IV in the paper\n",
    "    # mLSTM block 1\n",
    "    x = create_mLSTM_block(\n",
    "        x, \n",
    "        embedding_dim=embedding_dim,\n",
    "        kernel_size=4,\n",
    "        projection_size=2,\n",
    "        num_heads=2\n",
    "    )\n",
    "    \n",
    "    # sLSTM block\n",
    "    x = create_sLSTM_block(\n",
    "        x, \n",
    "        embedding_dim=embedding_dim,\n",
    "        kernel_size=2,\n",
    "        num_heads=2,\n",
    "        ff_factor=1.1\n",
    "    )\n",
    "    \n",
    "    # mLSTM block 2\n",
    "    x = create_mLSTM_block(\n",
    "        x, \n",
    "        embedding_dim=embedding_dim,\n",
    "        kernel_size=4,\n",
    "        projection_size=2,\n",
    "        num_heads=2\n",
    "    )\n",
    "    \n",
    "    # mLSTM block 3\n",
    "    x = create_mLSTM_block(\n",
    "        x, \n",
    "        embedding_dim=embedding_dim,\n",
    "        kernel_size=4,\n",
    "        projection_size=2,\n",
    "        num_heads=2\n",
    "    )\n",
    "    \n",
    "    # Final layer normalization\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    # Global average pooling to reduce sequence dimension\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Final linear projection to output size\n",
    "    outputs = Dense(output_size)(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Splitting Explanation\n",
    "Both models use a temporal split approach, but with different considerations:\n",
    "\n",
    "1. López's Approach:\n",
    "   - Training: 70% of data\n",
    "   - Validation: 15% of data\n",
    "   - Testing: 15% of data\n",
    "   \n",
    "   Key Features:\n",
    "   - Maintains temporal order\n",
    "   - Uses sequence creation\n",
    "   - Includes validation set\n",
    "   - Scales data using training statistics only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, target_col='BTC/USD', train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Prepare data according to the paper's specifications\n",
    "    \"\"\"\n",
    "    # Calculate split points\n",
    "    total_rows = len(df)\n",
    "    train_size = int(total_rows * train_ratio)\n",
    "    val_size = int(total_rows * val_ratio)\n",
    "    \n",
    "    # Split maintaining temporal order\n",
    "    train_df = df[:train_size]\n",
    "    val_df = df[train_size:train_size + val_size]\n",
    "    test_df = df[train_size + val_size:]\n",
    "    \n",
    "    # Scale data using MinMaxScaler as per paper\n",
    "    feature_scaler = MinMaxScaler()  # Changed from StandardScaler\n",
    "    target_scaler = MinMaxScaler()   # Changed from StandardScaler\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_columns = [col for col in df.columns if col != target_col]\n",
    "    \n",
    "    # Prepare feature and target data\n",
    "    X_train = pd.DataFrame(\n",
    "        feature_scaler.fit_transform(train_df[feature_columns]),\n",
    "        columns=feature_columns,\n",
    "        index=train_df.index\n",
    "    )\n",
    "    y_train = pd.Series(\n",
    "        target_scaler.fit_transform(train_df[[target_col]]).ravel(),\n",
    "        index=train_df.index\n",
    "    )\n",
    "    \n",
    "    X_val = pd.DataFrame(\n",
    "        feature_scaler.transform(val_df[feature_columns]),\n",
    "        columns=feature_columns,\n",
    "        index=val_df.index\n",
    "    )\n",
    "    y_val = pd.Series(\n",
    "        target_scaler.transform(val_df[[target_col]]).ravel(),\n",
    "        index=val_df.index\n",
    "    )\n",
    "    \n",
    "    X_test = pd.DataFrame(\n",
    "        feature_scaler.transform(test_df[feature_columns]),\n",
    "        columns=feature_columns,\n",
    "        index=test_df.index\n",
    "    )\n",
    "    y_test = pd.Series(\n",
    "        target_scaler.transform(test_df[[target_col]]).ravel(),\n",
    "        index=test_df.index\n",
    "    )\n",
    "    \n",
    "    # Store original data for MASE calculation\n",
    "    y_train_original = train_df[target_col].values\n",
    "    original_test_actuals = test_df[target_col].values\n",
    "    \n",
    "    print(f\"Train size: {len(X_train)}, Validation size: {len(X_val)}, Test size: {len(X_test)}\")\n",
    "    \n",
    "    return {\n",
    "        'train': (X_train, y_train),\n",
    "        'val': (X_val, y_val),\n",
    "        'test': (X_test, y_test),\n",
    "        'scalers': {\n",
    "            'feature': feature_scaler,\n",
    "            'target': target_scaler\n",
    "        },\n",
    "        'original': {\n",
    "            'train': y_train_original,\n",
    "            'test': original_test_actuals\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_tyralis(data, sequence_length=10, forecast_horizon=1, progress_bar=True):\n",
    "    if isinstance(data, pd.Series):\n",
    "        # Convert Series to DataFrame for consistent handling\n",
    "        data = data.to_frame()\n",
    "    \n",
    "    n_samples = len(data) - sequence_length - forecast_horizon + 1\n",
    "    n_features = data.shape[1]\n",
    "    \n",
    "    # Initialize empty arrays\n",
    "    X = np.zeros((n_samples, sequence_length, n_features))\n",
    "    y = np.zeros((n_samples,))\n",
    "    \n",
    "    # Create progress bar if requested\n",
    "    if progress_bar:\n",
    "        iterator = tqdm(range(n_samples), desc=\"Creating sequences\")\n",
    "    else:\n",
    "        iterator = range(n_samples)\n",
    "    \n",
    "    # Create sequences\n",
    "    for i in iterator:\n",
    "        # Input sequence (lookback window)\n",
    "        X[i] = data.iloc[i:i+sequence_length].values\n",
    "        \n",
    "        # Target value (single step ahead as per Tyralis paper)\n",
    "        y[i] = data.iloc[i+sequence_length].values[0]  # Take first column if multiple features\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingProgressCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epochs = self.params['epochs']\n",
    "        self.steps = self.params['steps'] * self.epochs\n",
    "        self.progress_bar = tqdm(\n",
    "            total=self.steps,\n",
    "            desc=\"Training Progress\",\n",
    "            unit=\"batch\",\n",
    "            leave=True  # Ensure progress bar remains after completion\n",
    "        )\n",
    "        self.current_step = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.epochs_without_improvement = 0\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.current_step += 1\n",
    "        self.progress_bar.update(1)\n",
    "        \n",
    "        # Update progress bar with more detailed metrics\n",
    "        self.progress_bar.set_postfix({\n",
    "            'loss': f\"{logs['loss']:.4f}\",\n",
    "            'val_loss': f\"{logs.get('val_loss', 'N/A')}\",\n",
    "            'mae': f\"{logs['mae']:.4f}\",\n",
    "            'epoch': f\"{self.current_step // self.params['steps']}/{self.epochs}\"\n",
    "        })\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Monitor validation loss for early stopping\n",
    "        current_loss = logs.get('val_loss', logs['loss'])\n",
    "        if current_loss < self.best_loss:\n",
    "            self.best_loss = current_loss\n",
    "            self.epochs_without_improvement = 0\n",
    "        else:\n",
    "            self.epochs_without_improvement += 1\n",
    "            \n",
    "        # Update progress bar with epoch information\n",
    "        self.progress_bar.set_postfix({\n",
    "            'loss': f\"{logs['loss']:.4f}\",\n",
    "            'val_loss': f\"{logs.get('val_loss', 'N/A')}\",\n",
    "            'mae': f\"{logs['mae']:.4f}\",\n",
    "            'epoch': f\"{epoch + 1}/{self.epochs}\",\n",
    "            'no_improve': self.epochs_without_improvement\n",
    "        })\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_lopez_model(df, target_col='BTC/USD', sequence_length=30): # sequence length 150\n",
    "    print(\"\\nInitializing López's xLSTM-TS model training...\")\n",
    "    \n",
    "    if df.empty:\n",
    "        raise ValueError(\"Input DataFrame is empty\")\n",
    "    \n",
    "    # Print debug info\n",
    "    print(f\"Input DataFrame shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check if we have enough features\n",
    "    if len(df.columns) <= 1:\n",
    "        # Create a simple feature from the target (e.g., previous value)\n",
    "        df['prev_price'] = df[target_col].shift(1)\n",
    "        df = df.dropna()  # Remove the first row which will have NaN\n",
    "        print(\"Added previous price as a feature\")\n",
    "    \n",
    "    # Calculate split points\n",
    "    total_rows = len(df)\n",
    "    train_size = int(total_rows * 0.7)\n",
    "    val_size = int(total_rows * 0.15)\n",
    "    \n",
    "    # Split maintaining temporal order\n",
    "    train_df = df[:train_size]\n",
    "    val_df = df[train_size:train_size + val_size]\n",
    "    test_df = df[train_size + val_size:]\n",
    "    \n",
    "    print(f\"Train shape: {train_df.shape}\")\n",
    "    \n",
    "    # Store original test values\n",
    "    original_test_actuals = test_df[target_col].values\n",
    "    \n",
    "    # Scale data with proper DataFrame handling\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_columns = [col for col in df.columns if col != target_col]\n",
    "    print(f\"Feature columns: {feature_columns}\")\n",
    "    \n",
    "    # Fit and transform training data\n",
    "    X_train = pd.DataFrame(\n",
    "        feature_scaler.fit_transform(train_df[feature_columns]),\n",
    "        columns=feature_columns,\n",
    "        index=train_df.index\n",
    "    )\n",
    "    y_train = pd.Series(\n",
    "        target_scaler.fit_transform(train_df[[target_col]]).ravel(),\n",
    "        index=train_df.index\n",
    "    )\n",
    "    \n",
    "    # Transform validation data\n",
    "    X_val = pd.DataFrame(\n",
    "        feature_scaler.transform(val_df[feature_columns]),\n",
    "        columns=feature_columns,\n",
    "        index=val_df.index\n",
    "    )\n",
    "    y_val = pd.Series(\n",
    "        target_scaler.transform(val_df[[target_col]]).ravel(),\n",
    "        index=val_df.index\n",
    "    )\n",
    "    \n",
    "    # Transform test data\n",
    "    X_test = pd.DataFrame(\n",
    "        feature_scaler.transform(test_df[feature_columns]),\n",
    "        columns=feature_columns,\n",
    "        index=test_df.index\n",
    "    )\n",
    "    y_test = pd.Series(\n",
    "        target_scaler.transform(test_df[[target_col]]).ravel(),\n",
    "        index=test_df.index\n",
    "    )\n",
    "    \n",
    "    # Store original training data for MASE calculation\n",
    "    y_train_original = train_df[target_col].values\n",
    "    \n",
    "    # Create sequences\n",
    "    print(\"\\nCreating sequences...\")\n",
    "    sequence_progress = tqdm(total=3, desc=\"Sequence preparation\")\n",
    "    \n",
    "    # Training sequences - using sequence_length=150 as per paper\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X_train) - sequence_length - 6):\n",
    "        X_seq.append(X_train.iloc[i:i + sequence_length].values)\n",
    "        y_seq.append(y_train.iloc[i + sequence_length:i + sequence_length + 7].values.reshape(-1))\n",
    "    X_train_seq, y_train_seq = np.array(X_seq), np.array(y_seq)\n",
    "    sequence_progress.update(1)\n",
    "    \n",
    "    # Validation sequences\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X_val) - sequence_length - 6):\n",
    "        X_seq.append(X_val.iloc[i:i + sequence_length].values)\n",
    "        y_seq.append(y_val.iloc[i + sequence_length:i + sequence_length + 7].values.reshape(-1))\n",
    "    X_val_seq, y_val_seq = np.array(X_seq), np.array(y_seq)\n",
    "    sequence_progress.update(1)\n",
    "    \n",
    "    # Test sequences\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X_test) - sequence_length - 6):\n",
    "        X_seq.append(X_test.iloc[i:i + sequence_length].values)\n",
    "        y_seq.append(y_test.iloc[i + sequence_length:i + sequence_length + 7].values.reshape(-1))\n",
    "    X_test_seq, y_test_seq = np.array(X_seq), np.array(y_seq)\n",
    "    sequence_progress.update(1)\n",
    "    sequence_progress.close()\n",
    "    \n",
    "    # Get number of features (excluding target column)\n",
    "    n_features = X_train.shape[1]  # Number of features after dropping target column\n",
    "    \n",
    "    # Initialize model with the paper's xLSTM-TS architecture\n",
    "    print(\"\\nInitializing model...\")\n",
    "    \n",
    "    # Create the model using our new implementation\n",
    "    model = create_xLSTM_TS_model(\n",
    "        input_shape=(sequence_length, n_features),\n",
    "        embedding_dim=64,  # As per paper\n",
    "        output_size=7      # 7-day forecast\n",
    "    )\n",
    "    \n",
    "    # Configure optimizer with paper's parameters\n",
    "    optimizer = Adam(\n",
    "        learning_rate=1e-4,  # As per paper\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-7\n",
    "    )\n",
    "    \n",
    "    # Compile model with directional loss\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=directional_loss,\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=30,  # As per paper\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=10,  # As per paper\n",
    "            min_lr=1e-6\n",
    "        ),\n",
    "        TrainingProgressCallback()\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTraining model...\")\n",
    "    # Train model with paper's parameters\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        epochs=2,         # 200 As per paper\n",
    "        batch_size=16,      # As per paper\n",
    "        callbacks=callbacks,\n",
    "        verbose=0,          # Disable default verbose output to use custom progress bar\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return model, history, (X_test_seq, y_test_seq), target_scaler, y_train_original, original_test_actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_tyralis_model(df, target_col='BTC/USD', sequence_length=10):\n",
    "    \"\"\"\n",
    "    Implementation of the Tyralis model as described in the paper:\n",
    "    \"Variable Selection in Time Series Forecasting Using Random Forests\"\n",
    "    \"\"\"\n",
    "    print(\"\\nInitializing Tyralis model training...\")\n",
    "    \n",
    "    if df.empty:\n",
    "        raise ValueError(\"Input DataFrame is empty\")\n",
    "    \n",
    "    # Print debug info\n",
    "    print(f\"Input DataFrame shape: {df.shape}\")\n",
    "    print(f\"Target column: {target_col}\")\n",
    "    \n",
    "    # Calculate split points\n",
    "    total_rows = len(df)\n",
    "    train_size = int(total_rows * 0.7)\n",
    "    val_size = int(total_rows * 0.15)\n",
    "    \n",
    "    # Split maintaining temporal order\n",
    "    train_df = df[:train_size]\n",
    "    test_df = df[train_size + val_size:]\n",
    "    \n",
    "    print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "    \n",
    "    # Store original test values for evaluation\n",
    "    original_test_actuals = test_df[target_col].values\n",
    "    \n",
    "    # Store original training data for MASE calculation\n",
    "    y_train_original = train_df[target_col].values\n",
    "    \n",
    "    # Scale data with MinMaxScaler as per Tyralis paper\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_columns = [col for col in df.columns if col != target_col]\n",
    "    print(f\"Number of feature columns: {len(feature_columns)}\")\n",
    "    print(f\"Feature columns: {feature_columns[:5]}{'...' if len(feature_columns) > 5 else ''}\")\n",
    "    \n",
    "    # Fit and transform training data\n",
    "    X_train = pd.DataFrame(\n",
    "        feature_scaler.fit_transform(train_df[feature_columns]),\n",
    "        columns=feature_columns,\n",
    "        index=train_df.index\n",
    "    )\n",
    "    y_train = pd.Series(\n",
    "        target_scaler.fit_transform(train_df[[target_col]]).ravel(),\n",
    "        index=train_df.index\n",
    "    )\n",
    "    \n",
    "    # Transform test data\n",
    "    X_test = pd.DataFrame(\n",
    "        feature_scaler.transform(test_df[feature_columns]),\n",
    "        columns=feature_columns,\n",
    "        index=test_df.index\n",
    "    )\n",
    "    y_test = pd.Series(\n",
    "        target_scaler.transform(test_df[[target_col]]).ravel(),\n",
    "        index=test_df.index\n",
    "    )\n",
    "    \n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "    \n",
    "    # Create sequences for Tyralis model\n",
    "    print(\"\\nCreating sequences...\")\n",
    "    \n",
    "    def create_sequences_tyralis(X, y, seq_length=10):\n",
    "        \"\"\"Create sequences for the Tyralis Random Forest model\"\"\"\n",
    "        X_seq, y_seq = [], []\n",
    "        \n",
    "        with tqdm(total=len(X) - seq_length, desc=\"Creating sequences\") as pbar:\n",
    "            for i in range(len(X) - seq_length):\n",
    "                # Input sequence\n",
    "                X_seq.append(X.iloc[i:i + seq_length].values)\n",
    "                \n",
    "                # Target value (single step ahead for Tyralis model)\n",
    "                y_seq.append(y.iloc[i + seq_length])\n",
    "                \n",
    "                pbar.update(1)\n",
    "        \n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train_seq, y_train_seq = create_sequences_tyralis(X_train, y_train, seq_length=sequence_length)\n",
    "    X_test_seq, y_test_seq = create_sequences_tyralis(X_test, y_test, seq_length=sequence_length)\n",
    "    \n",
    "    print(f\"X_train_seq shape: {X_train_seq.shape}, y_train_seq shape: {y_train_seq.shape}\")\n",
    "    print(f\"X_test_seq shape: {X_test_seq.shape}, y_test_seq shape: {y_test_seq.shape}\")\n",
    "    \n",
    "    # Reshape for RF input - RF expects 2D input\n",
    "    n_samples_train = X_train_seq.shape[0]\n",
    "    n_samples_test = X_test_seq.shape[0]\n",
    "    n_features_per_timestep = X_train_seq.shape[2] if len(X_train_seq.shape) > 2 else 1\n",
    "    total_features = sequence_length * n_features_per_timestep\n",
    "    \n",
    "    # Flatten the 3D sequences to 2D for RF\n",
    "    X_train_rf = X_train_seq.reshape(n_samples_train, -1)\n",
    "    X_test_rf = X_test_seq.reshape(n_samples_test, -1)\n",
    "    \n",
    "    print(f\"X_train_rf shape: {X_train_rf.shape}, y_train_seq shape: {y_train_seq.shape}\")\n",
    "    print(f\"X_test_rf shape: {X_test_rf.shape}, y_test_seq shape: {y_test_seq.shape}\")\n",
    "    print(f\"Total features after flattening: {X_train_rf.shape[1]}\")\n",
    "    \n",
    "    # Initialize Random Forest model with more trees and explicit parameters\n",
    "    print(\"\\nInitializing Random Forest model...\")\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=500,  # 500 trees as per paper\n",
    "        max_features='sqrt',  # Default mtry value\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        bootstrap=True,\n",
    "        random_state=42,\n",
    "        verbose=1,  # Add verbosity to see training progress\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    )\n",
    "    \n",
    "    # Train the model with timing\n",
    "    print(\"Training model...\")\n",
    "    start_time = time.time()\n",
    "    rf_model.fit(X_train_rf, y_train_seq)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Model training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Check model properties\n",
    "    print(f\"Number of trees in the forest: {len(rf_model.estimators_)}\")\n",
    "    print(f\"Feature importances: {rf_model.feature_importances_[:5]}...\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"Generating predictions...\")\n",
    "    predictions = rf_model.predict(X_test_rf)\n",
    "    \n",
    "    # Calculate R² score on test data\n",
    "    r2_score = rf_model.score(X_test_rf, y_test_seq)\n",
    "    print(f\"R² score on test data: {r2_score:.4f}\")\n",
    "    \n",
    "    # Inverse transform predictions to original scale\n",
    "    predictions_reshaped = predictions.reshape(-1, 1)\n",
    "    try:\n",
    "        predictions_original = target_scaler.inverse_transform(predictions_reshaped).flatten()\n",
    "    except:\n",
    "        # Handle case where predictions might need reshaping\n",
    "        predictions_original = np.array([target_scaler.inverse_transform([[p]])[0][0] for p in predictions])\n",
    "    \n",
    "    # Calculate some basic error metrics\n",
    "    mse = np.mean((predictions_original - original_test_actuals[sequence_length:sequence_length+len(predictions_original)])**2)\n",
    "    mae = np.mean(np.abs(predictions_original - original_test_actuals[sequence_length:sequence_length+len(predictions_original)]))\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "    \n",
    "    # Ensure predictions and actuals are the same length\n",
    "    min_length = min(len(predictions_original), len(original_test_actuals[sequence_length:]))\n",
    "    predictions_final = predictions_original[:min_length]\n",
    "    actuals_final = original_test_actuals[sequence_length:sequence_length+min_length]\n",
    "    \n",
    "    # Print sample of predictions vs actuals\n",
    "    print(\"\\nSample predictions vs actuals:\")\n",
    "    for i in range(min(5, len(predictions_final))):\n",
    "        print(f\"Prediction: {predictions_final[i]:.4f}, Actual: {actuals_final[i]:.4f}\")\n",
    "    \n",
    "    return rf_model, predictions_final, actuals_final, y_train_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics, model_choice=None, predictions=None, actuals=None, output_dir=None):\n",
    "    \"\"\"\n",
    "    Plot metrics from model evaluations\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    metrics : dict\n",
    "        Dictionary containing evaluation metrics\n",
    "    model_choice : str, optional\n",
    "        Name of the model being evaluated\n",
    "    predictions : array-like, optional\n",
    "        Model predictions\n",
    "    actuals : array-like, optional\n",
    "        Actual values\n",
    "    output_dir : Path, optional\n",
    "        Directory to save the plot\n",
    "    \"\"\"\n",
    "    # Create a figure with 2x2 subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(f'Model Performance Metrics: {model_choice}', fontsize=16)\n",
    "    \n",
    "    # Flatten the axes for easier indexing\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    # Plot 1: Classification Metrics (top left)\n",
    "    ax = axs[0]\n",
    "    classification_metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    values = []\n",
    "    \n",
    "    # Extract values from metrics dictionary\n",
    "    for metric in classification_metrics:\n",
    "        if metric in metrics:\n",
    "            values.append(float(metrics[metric]))\n",
    "        else:\n",
    "            values.append(0)\n",
    "    \n",
    "    bars = ax.bar(classification_metrics, values, color='skyblue')\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.2%}', ha='center', va='bottom')\n",
    "    \n",
    "    ax.set_title('Classification Metrics')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot 2: Regression Metrics (top right)\n",
    "    ax = axs[1]\n",
    "    regression_metrics = ['MAE', 'RMSE']\n",
    "    values = []\n",
    "    \n",
    "    # Extract values from metrics dictionary\n",
    "    for metric in regression_metrics:\n",
    "        if metric in metrics:\n",
    "            values.append(float(metrics[metric]))\n",
    "        else:\n",
    "            values.append(0)\n",
    "    \n",
    "    bars = ax.bar(regression_metrics, values, color='lightgreen')\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.2%}', ha='center', va='bottom')\n",
    "    \n",
    "    ax.set_title('Regression Metrics')\n",
    "    ax.set_ylabel('Error Rate')\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot 3: Predictions vs Actuals (bottom left)\n",
    "    ax = axs[2]\n",
    "    if predictions is not None and actuals is not None and len(predictions) > 0 and len(actuals) > 0:\n",
    "        # Plot a sample of predictions vs actuals\n",
    "        sample_size = min(100, len(predictions))\n",
    "        indices = np.arange(sample_size)\n",
    "        \n",
    "        # Ensure we have enough data\n",
    "        if len(actuals) >= sample_size and len(predictions) >= sample_size:\n",
    "            ax.plot(indices, actuals[:sample_size], 'b-', label='Actual')\n",
    "            ax.plot(indices, predictions[:sample_size], 'r--', label='Predicted')\n",
    "            ax.set_title('Predictions vs Actuals (Sample)')\n",
    "            ax.set_xlabel('Sample Index')\n",
    "            ax.set_ylabel('Value')\n",
    "            ax.legend()\n",
    "            ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'Insufficient data for plotting', \n",
    "                    ha='center', va='center', transform=ax.transAxes)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No prediction data available', \n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title('Predictions vs Actuals')\n",
    "    \n",
    "    # Plot 4: RMSSE Metrics (bottom right)\n",
    "    ax = axs[3]\n",
    "    if 'RMSSE' in metrics and metrics['RMSSE'] is not None:\n",
    "        try:\n",
    "            value = float(metrics['RMSSE'])\n",
    "            \n",
    "            bars = ax.bar(['RMSSE'], [value], color='orange')\n",
    "            \n",
    "            # Add value label on top of bar\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                        f'{height:.4f}', ha='center', va='bottom')\n",
    "            \n",
    "            ax.set_title('RMSSE Metric')\n",
    "            ax.set_ylabel('RMSSE Score')\n",
    "            ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        except (ValueError, TypeError):\n",
    "            ax.text(0.5, 0.5, 'Error processing RMSSE metric', \n",
    "                    ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title('RMSSE Metric')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'RMSSE metric not available', \n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title('RMSSE Metric')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    if output_dir:\n",
    "        plt.savefig(output_dir / f'{model_choice}_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def print_metrics(metrics, model_name):\n",
    "    \"\"\"\n",
    "    Print formatted metrics - removed RMSSE\n",
    "    \"\"\"\n",
    "    print(f\"\\n{model_name} Model Results:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Classification metrics as percentages\n",
    "    print(\"\\nClassification Metrics:\")\n",
    "    for metric in ['Accuracy', 'Recall', 'Precision', 'F1 Score']:\n",
    "        print(f\"{metric:15s}: {metrics[metric]*100:6.2f}%\")\n",
    "    \n",
    "    # Regression metrics\n",
    "    print(\"\\nRegression Metrics:\")\n",
    "    print(f\"{'MAE':15s}: {metrics['MAE']*100:6.2f}%\")\n",
    "    print(f\"{'RMSE':15s}: {metrics['RMSE']*100:6.2f}%\")\n",
    "    print(f\"{'MASE':15s}: {metrics['MASE']:6.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions, actuals, y_train):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics on the raw predictions without modifications\n",
    "    \"\"\"\n",
    "    # Input validation and reshaping only (no value modifications)\n",
    "    predictions = np.array(predictions).reshape(-1)\n",
    "    actuals = np.array(actuals).reshape(-1)\n",
    "    y_train = np.array(y_train).reshape(-1)\n",
    "    \n",
    "    # Calculate directional changes\n",
    "    binary_pred = np.diff(predictions) > 0\n",
    "    binary_true = np.diff(actuals) > 0\n",
    "    \n",
    "    # Calculate classification metrics\n",
    "    try:\n",
    "        accuracy = accuracy_score(binary_true, binary_pred)\n",
    "        recall = recall_score(binary_true, binary_pred)\n",
    "        precision = precision_score(binary_true, binary_pred)\n",
    "        f1 = f1_score(binary_true, binary_pred)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Classification metrics calculation failed: {e}\")\n",
    "        accuracy = recall = precision = f1 = 0.0\n",
    "    \n",
    "    # Calculate error metrics on the raw values\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "    mase = calculate_mase(actuals, predictions, y_train)\n",
    "    rmsse = calculate_rmsse(actuals, predictions, y_train)\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'F1 Score': f1,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'MASE': mase,\n",
    "        'RMSSE': rmsse\n",
    "    }\n",
    "\n",
    "def calculate_mase(y_true, y_pred, y_train):\n",
    "    \"\"\"\n",
    "    Calculate Mean Absolute Scaled Error with validation\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    # Calculate naive forecast error (t+1 = t)\n",
    "    naive_errors = y_train[1:] - y_train[:-1]\n",
    "    naive_error = np.mean(np.abs(naive_errors))\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if naive_error == 0 or np.isnan(naive_error):\n",
    "        print(\"Warning: Naive error is zero or NaN, using MAE instead\")\n",
    "        return mae\n",
    "    \n",
    "    return mae / naive_error\n",
    "\n",
    "def calculate_rmsse(y_true, y_pred, y_train):\n",
    "    \"\"\"\n",
    "    Calculate Root Mean Squared Scaled Error with validation\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    # Calculate naive forecast error\n",
    "    naive_errors = (y_train[1:] - y_train[:-1])**2\n",
    "    naive_error = np.mean(naive_errors)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if naive_error == 0 or np.isnan(naive_error):\n",
    "        print(\"Warning: Naive error is zero or NaN, using RMSE instead\")\n",
    "        return np.sqrt(mse)\n",
    "    \n",
    "    return np.sqrt(mse / naive_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model_results, model_choice):\n",
    "    print(\"\\nGenerating predictions...\")\n",
    "    \n",
    "    if model_choice == 'lopez':\n",
    "        model, _, test_data, scaler, y_train_original, original_test_actuals = model_results\n",
    "        X_test, y_test = test_data\n",
    "        \n",
    "        # Generate predictions\n",
    "        raw_predictions = model.predict(X_test)\n",
    "        predictions = raw_predictions[:, 0]  # Take first day predictions\n",
    "        \n",
    "        # Validation checks\n",
    "        if np.any(np.isnan(predictions)):\n",
    "            print(\"Warning: NaN values detected in predictions\")\n",
    "            predictions = np.nan_to_num(predictions, 0)\n",
    "        \n",
    "        # Ensure positive values for percentage calculations\n",
    "        predictions = np.abs(predictions)\n",
    "        \n",
    "        # Debug information\n",
    "        print(\"\\nPrediction Scaling Check:\")\n",
    "        print(f\"Raw predictions range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
    "        \n",
    "        # Reshape and inverse transform\n",
    "        predictions = predictions.reshape(-1, 1)\n",
    "        predictions = scaler.inverse_transform(predictions).flatten()\n",
    "        \n",
    "        print(f\"Inverse transformed range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
    "        print(f\"Expected range (actuals): [{original_test_actuals.min():.4f}, {original_test_actuals.max():.4f}]\")\n",
    "        \n",
    "        # Ensure same length\n",
    "        min_length = min(len(predictions), len(original_test_actuals))\n",
    "        predictions = predictions[:min_length]\n",
    "        original_test_actuals = original_test_actuals[:min_length]\n",
    "        \n",
    "        return predictions, original_test_actuals, y_train_original\n",
    "    else:\n",
    "        model, predictions, actuals, y_train = model_results\n",
    "        return predictions, actuals, y_train\n",
    "        \n",
    "def evaluate_and_save_results(predictions, actuals, y_train, model_choice, output_dir):\n",
    "    \"\"\"\n",
    "    Evaluate model performance and save results\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting evaluation...\")\n",
    "    print(f\"Initial shapes - Predictions: {predictions.shape}, Actuals: {actuals.shape}, y_train: {y_train.shape}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = evaluate_model(predictions, actuals, y_train, model_choice)\n",
    "    \n",
    "    # Print metrics\n",
    "    print_metrics(metrics, model_choice)\n",
    "    \n",
    "    # Plot results\n",
    "    fig = plot_metrics(metrics, model_choice, predictions, actuals)\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    metrics_df = pd.DataFrame([metrics])\n",
    "    metrics_df.to_csv(output_dir / f'{model_choice}_model_metrics.csv', index=False)\n",
    "    \n",
    "    # Save plot\n",
    "    fig.savefig(output_dir / f'{model_choice}_metrics_plot.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mase_comparison(all_results, output_dir=None):\n",
    "    \"\"\"\n",
    "    Plot MASE metrics comparison between models\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_results : dict\n",
    "        Dictionary containing results for all models and datasets\n",
    "    output_dir : Path, optional\n",
    "        Directory to save the plot\n",
    "    \"\"\"\n",
    "    # Extract dataset names and model types\n",
    "    datasets = list(all_results.keys())\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    labels = []\n",
    "    mase_values = []\n",
    "    colors = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        for model in all_results[dataset]:\n",
    "            if 'MASE' in all_results[dataset][model] and all_results[dataset][model]['MASE'] is not None:\n",
    "                try:\n",
    "                    mase_value = float(all_results[dataset][model]['MASE'])\n",
    "                    \n",
    "                    labels.append(f\"{dataset}\\n({model})\")\n",
    "                    mase_values.append(mase_value)\n",
    "                    colors.append('blue' if model == 'lopez' else 'green')\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "    \n",
    "    # If no valid MASE values were found, display a message\n",
    "    if not mase_values:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.text(0.5, 0.5, 'No MASE metrics available for comparison', \n",
    "                ha='center', va='center', fontsize=14)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if output_dir:\n",
    "            plt.savefig(output_dir / 'mase_comparison.png', dpi=300)\n",
    "        plt.show()\n",
    "        return\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create bars\n",
    "    bars = plt.bar(labels, mase_values, color=colors, alpha=0.7)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{height:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.title('MASE Comparison Between Models', fontsize=16)\n",
    "    plt.ylabel('MASE Score (lower is better)', fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add a horizontal line at MASE = 1 (baseline)\n",
    "    plt.axhline(y=1, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add annotations explaining MASE\n",
    "    plt.annotate('MASE < 1: Model outperforms naive forecast', \n",
    "                 xy=(0.02, 0.95), xycoords='axes fraction',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    plt.annotate('MASE > 1: Naive forecast outperforms model', \n",
    "                 xy=(0.02, 0.90), xycoords='axes fraction',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    # Create legend\n",
    "    legend_elements = [\n",
    "        plt.Rectangle((0,0), 1, 1, fc='blue', alpha=0.7, label='Lopez Model'),\n",
    "        plt.Rectangle((0,0), 1, 1, fc='green', alpha=0.7, label='Tyralis Model')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_dir:\n",
    "        plt.savefig(output_dir / 'mase_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluations_on_all_feature_sets(output_dir):\n",
    "    # First load all datasets\n",
    "    datasets = {\n",
    "        'full': pd.read_csv(output_dir / \"2015-2025_dataset_denoised.csv\", index_col=0, parse_dates=True),\n",
    "        'tyralis': pd.read_csv(output_dir / \"2015-2025_dataset_selected_features_tyralis.csv\", index_col=0, parse_dates=True),\n",
    "        'boruta': pd.read_csv(output_dir / \"2015-2025_dataset_selected_features_boruta.csv\", index_col=0, parse_dates=True),\n",
    "        'lasso': pd.read_csv(output_dir / \"2015-2025_dataset_selected_features_lasso.csv\", index_col=0, parse_dates=True)\n",
    "    }\n",
    "    \n",
    "    # Dictionary to store all results\n",
    "    all_results = {}\n",
    "    \n",
    "    # Run each model type\n",
    "    for model_type in ['lopez', 'tyralis']:\n",
    "        print(f\"\\nEVALUATING {model_type.upper()} MODEL\")\n",
    "        print(\"=\" * 50)\n",
    "        model_results = {}\n",
    "        \n",
    "        # Test each feature set\n",
    "        for dataset_name, df in datasets.items():\n",
    "            print(f\"\\nTesting on {dataset_name.upper()} dataset\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            # Run the model evaluation with the current dataset\n",
    "            metrics = run_model_evaluation(output_dir, model_type, df)\n",
    "            model_results[dataset_name] = metrics\n",
    "            \n",
    "            # Save individual results\n",
    "            pd.DataFrame([metrics]).to_csv(output_dir / f'{model_type}_model_metrics_{dataset_name}.csv', index=False)\n",
    "        \n",
    "        # Store results for this model type\n",
    "        all_results[model_type] = model_results\n",
    "        \n",
    "        # Create comparison DataFrame for this model\n",
    "        comparison_df = pd.DataFrame(model_results).T\n",
    "        comparison_df.to_csv(output_dir / f'{model_type}_model_metrics_comparison.csv')\n",
    "        \n",
    "        # Print comparison summary\n",
    "        print(f\"\\n{model_type.upper()} Model Performance Comparison:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for dataset_name, metrics in model_results.items():\n",
    "            print(f\"\\n{dataset_name.upper()} Dataset:\")\n",
    "            print(f\"Accuracy: {metrics['Accuracy']*100:.2f}%\")\n",
    "            print(f\"F1 Score: {metrics['F1 Score']*100:.2f}%\")\n",
    "            print(f\"MAE: {metrics['MAE']*100:.2f}%\")\n",
    "            print(f\"RMSE: {metrics['RMSE']*100:.2f}%\")\n",
    "            print(f\"MASE: {metrics['MASE']:.2f}\")\n",
    "            print(f\"RMSSE: {metrics['RMSSE']:.2f}\")\n",
    "             \n",
    "     # Create and save the scaled error metrics comparison plot\n",
    "    plot_mase_comparison(all_results, output_dir=output_dir)\n",
    "    \n",
    "    # Save overall comparison\n",
    "    overall_comparison = pd.DataFrame({\n",
    "        f\"{model}_{dataset}\": metrics \n",
    "        for model, model_results in all_results.items() \n",
    "        for dataset, metrics in model_results.items()\n",
    "    })\n",
    "    overall_comparison.to_csv(output_dir / 'overall_model_metrics_comparison.csv')\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Modified run_model_evaluation to accept the dataset parameter\n",
    "def run_model_evaluation(output_dir, model_choice, df):   \n",
    "    with tqdm(total=3, desc=f\"Running {model_choice.capitalize()} Model\") as pbar:\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "        \n",
    "        if model_choice == 'lopez':\n",
    "            model_results = use_lopez_model(df)\n",
    "            model, _, (X_test, _), scaler, y_train_original, original_test_actuals = model_results\n",
    "            pbar.update(1)\n",
    "        \n",
    "            predictions = model.predict(X_test)[:, 0]\n",
    "            predictions = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            # Ensure predictions and actuals are the same length\n",
    "            min_length = min(len(predictions), len(original_test_actuals))\n",
    "            predictions = predictions[:min_length]\n",
    "            original_test_actuals = original_test_actuals[:min_length]\n",
    "            \n",
    "            pbar.update(1)\n",
    "            \n",
    "        else:  # tyralis\n",
    "            model_results = use_tyralis_model(df)\n",
    "            model, predictions, original_test_actuals, y_train_original = model_results\n",
    "            pbar.update(2)\n",
    "        \n",
    "        # Print debug info\n",
    "        print(\"\\nDebug - Data Statistics:\")\n",
    "        print(f\"Predictions - min: {predictions.min():.4f}, max: {predictions.max():.4f}\")\n",
    "        print(f\"Actuals - min: {original_test_actuals.min():.4f}, max: {original_test_actuals.max():.4f}\")\n",
    "        \n",
    "        # Calculate and display metrics\n",
    "        metrics = evaluate_model(predictions, original_test_actuals, y_train_original)\n",
    "        print_metrics(metrics, model_choice)\n",
    "        plot_metrics(metrics, model_choice=model_choice, predictions=predictions, actuals=original_test_actuals, output_dir=output_dir)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Save metrics\n",
    "        pd.DataFrame([metrics]).to_csv(output_dir / f'{model_choice}_model_metrics.csv', index=False)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "all_results = run_evaluations_on_all_feature_sets(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
